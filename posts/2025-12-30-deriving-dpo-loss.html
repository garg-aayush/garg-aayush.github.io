<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-12-30">
<meta name="description" content="A step-by-step derivation of the Direct Preference Optimization (DPO) loss, showing how it reformulates the RLHF objective into a simple classification loss without requiring explicit reward modeling or reinforcement learning.">

<title>Deriving the DPO Loss from First Principles – Aayush Garg</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../static/img/aayush_cropped.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-2d3f84a5f37526cd3979248aedf2fbfb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-16JWTLXJNG"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-16JWTLXJNG', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog/"> 
<span class="menu-text">Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/garg-aayush"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aayush-garg-8b26a734"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Aayush_ander"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#deriving-the-dpo-loss-from-first-principles" id="toc-deriving-the-dpo-loss-from-first-principles" class="nav-link active" data-scroll-target="#deriving-the-dpo-loss-from-first-principles">Deriving the DPO Loss from First Principles</a>
  <ul class="collapse">
  <li><a href="#i-the-rlhf-objective" id="toc-i-the-rlhf-objective" class="nav-link" data-scroll-target="#i-the-rlhf-objective">I: The RLHF Objective</a></li>
  <li><a href="#ii-the-bradley-terry-model-for-preference-learning" id="toc-ii-the-bradley-terry-model-for-preference-learning" class="nav-link" data-scroll-target="#ii-the-bradley-terry-model-for-preference-learning">II: The Bradley-Terry Model for Preference Learning</a>
  <ul class="collapse">
  <li><a href="#the-bradley-terry-probability-model" id="toc-the-bradley-terry-probability-model" class="nav-link" data-scroll-target="#the-bradley-terry-probability-model">The Bradley-Terry Probability Model</a></li>
  <li><a href="#reward-model-loss" id="toc-reward-model-loss" class="nav-link" data-scroll-target="#reward-model-loss">Reward Model Loss</a></li>
  </ul></li>
  <li><a href="#iii-optimal-policy-in-closed-form" id="toc-iii-optimal-policy-in-closed-form" class="nav-link" data-scroll-target="#iii-optimal-policy-in-closed-form">III: Optimal Policy in Closed Form</a></li>
  <li><a href="#iv-the-reparameterization-trick" id="toc-iv-the-reparameterization-trick" class="nav-link" data-scroll-target="#iv-the-reparameterization-trick">IV: The Reparameterization Trick</a></li>
  <li><a href="#v-deriving-the-dpo-loss" id="toc-v-deriving-the-dpo-loss" class="nav-link" data-scroll-target="#v-deriving-the-dpo-loss">V: Deriving the DPO Loss</a></li>
  <li><a href="#vi-building-intuition-for-dpo" id="toc-vi-building-intuition-for-dpo" class="nav-link" data-scroll-target="#vi-building-intuition-for-dpo">VI: Building Intuition for DPO</a>
  <ul class="collapse">
  <li><a href="#implicit-reward-model" id="toc-implicit-reward-model" class="nav-link" data-scroll-target="#implicit-reward-model">Implicit Reward Model</a></li>
  <li><a href="#analyzing-the-gradient-update" id="toc-analyzing-the-gradient-update" class="nav-link" data-scroll-target="#analyzing-the-gradient-update">Analyzing the Gradient Update</a></li>
  </ul></li>
  <li><a href="#vii-computing-log-probabilities-in-practice" id="toc-vii-computing-log-probabilities-in-practice" class="nav-link" data-scroll-target="#vii-computing-log-probabilities-in-practice">VII: Computing Log Probabilities in Practice</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Deriving the DPO Loss from First Principles</h1>
</div>

<div>
  <div class="description">
    A step-by-step derivation of the Direct Preference Optimization (DPO) loss, showing how it reformulates the RLHF objective into a simple classification loss without requiring explicit reward modeling or reinforcement learning.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 30, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="deriving-the-dpo-loss-from-first-principles" class="level1">
<h1>Deriving the DPO Loss from First Principles</h1>
<p>In my <a href="https://aayushgarg.dev/posts/2025-12-25-deriving-ppo-loss.html">previous post</a>, I worked through the derivation of the PPO loss used in RLHF for LLMs. By the end, we arrived at a fairly daunting objective function with multiple components: clipped surrogate, value function loss, entropy bonus and KL penalty. It is not just that the final objective is intimidating but the entire RLHF pipeline is complex and multi-step. You first train a separate reward model to reflect human preferences then fine-tune the LLM using RL with PPO.</p>
<p>That brings us to <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization</a> (DPO). DPO is a computationally lightweight alternative that directly optimizes LLMs to adhere to human preferences <strong>without explicit reward modeling or reinforcement learning</strong>. The key insight is that DPO implicitly optimizes the <strong>same</strong> objective as PPO-based RLHF (reward maximization with a KL-divergence constraint) but it replaces the entire reward model + PPO loop with a single supervised objective on preference pairs. There is no sampling during training, no value function, no clipping, just a classification loss!</p>
<p>Here I derive the DPO loss showing exactly how this simplification is possible. I will assume familiarity with concepts from the PPO post, particularly the reward model and the KL-constrained RLHF objective.</p>
<blockquote class="blockquote">
<p>Again, a huge shoutout to Umar Jamil’s <a href="https://www.youtube.com/watch?v=hvGa5Mba4c8">video on DPO</a> for an excellent walkthrough that helped me understand the derivation.</p>
</blockquote>
<section id="i-the-rlhf-objective" class="level2">
<h2 class="anchored" data-anchor-id="i-the-rlhf-objective">I: The RLHF Objective</h2>
<p>Let’s recall the RLHF objective from the PPO blog. The goal of RLHF is to find a policy <span class="math inline">\(\pi_\theta\)</span> that maximizes expected reward while staying close to a reference model <span class="math inline">\(\pi_{\text{ref}}\)</span>:</p>
<p><span class="math display">\[
J_{\text{RLHF}}(\theta) = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)}\left[r_\phi(x, y)\right] - \beta \cdot D_{\text{KL}}\left(\pi_\theta(y|x) \| \pi_{\text{ref}}(y|x)\right)
\]</span></p>
<p>The first term encourages the model to generate high-reward responses. The second term (KL penalty) prevents the model from drifting too far from the reference which helps avoid reward hacking and maintains language quality.</p>
<p>As we saw in the PPO blog, we can’t optimize this objective directly with gradient descent because the expectation <span class="math inline">\(\mathbb{E}_{y \sim \pi_\theta(y|x)}[\cdot]\)</span> requires sampling from the policy and sampling is non-differentiable. This is why we needed reinforcement learning algorithms like REINFORCE and PPO. They provide ways to estimate policy gradients without differentiating through the sampling process.</p>
<blockquote class="blockquote">
<p>What if we could reformulate the problem so that we don’t need to sample from the policy during training? This is exactly what DPO will achieve.</p>
</blockquote>
</section>
<section id="ii-the-bradley-terry-model-for-preference-learning" class="level2">
<h2 class="anchored" data-anchor-id="ii-the-bradley-terry-model-for-preference-learning">II: The Bradley-Terry Model for Preference Learning</h2>
<p>We also need to understand the <a href="https://www.jstor.org/stable/2334029">Bradley-Terry model</a> for reward model training in a bit more detail with focus on <em>why</em> it works the way it does.</p>
<p>Training a reward model requires human-labeled preference data that compares pairs of responses. <span class="math display">\[
\mathcal{D} = \left\{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\right\}_{i=1}^{N}
\]</span></p>
<p>where: - <span class="math inline">\(x\)</span> is the prompt - <span class="math inline">\(y_w\)</span> is the <strong>preferred</strong> (winning) response - <span class="math inline">\(y_l\)</span> is the <strong>dispreferred</strong> (losing) response</p>
<section id="the-bradley-terry-probability-model" class="level3">
<h3 class="anchored" data-anchor-id="the-bradley-terry-probability-model">The Bradley-Terry Probability Model</h3>
<p>The Bradley-Terry model provides a principled way to convert comparison data (defined above) into a probabilistic model. It assumes there exists some latent reward function <span class="math inline">\(r^*(x, y)\)</span> that captures true response quality and models the probability that response <span class="math inline">\(y_w\)</span> is preferred over <span class="math inline">\(y_l\)</span> as:</p>
<p><span class="math display">\[
P(y_w \succ y_l | x) = \frac{e^{r^*(x, y_w)}}{e^{r^*(x, y_w)} + e^{r^*(x, y_l)}} \tag{II.I}
\]</span></p>
<p>The intuition is straightforward that the responses with higher reward are exponentially more likely to be preferred.</p>
<p>A key step is recognizing that this <strong>ratio of exponentials can be written as a sigmoid function</strong>. This is important because it connects Bradley-Terry to standard binary classification.</p>
<p>Let <span class="math inline">\(A = r(x, y_w)\)</span> and <span class="math inline">\(B = r(x, y_l)\)</span>. We want to show:</p>
<p><span class="math display">\[
\frac{e^A}{e^A + e^B} = \sigma(A - B)
\]</span></p>
<p>where <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is the sigmoid function.</p>
<p>Starting with the left-hand side:</p>
<p><span class="math display">\[
\frac{e^A}{e^A + e^B}
\]</span></p>
<p>we can rewrite it as: <span class="math display">\[
= \frac{e^A / e^A}{(e^A + e^B) / e^A} = \frac{1}{1 + e^B / e^A} = \frac{1}{1 + e^{B-A}}
\]</span></p>
<p>which is the sigmoid function of <span class="math inline">\((A - B)\)</span>: <span class="math display">\[
= \frac{1}{1 + e^{-(A-B)}} = \sigma(A - B)
\]</span></p>
<p>Therefore, the Bradley-Terry model can be written as:</p>
<p><span class="math display">\[
\boxed{P(y_w \succ y_l | x) = \sigma\left(r(x, y_w) - r(x, y_l)\right)} \tag{II.II}
\]</span></p>
</section>
<section id="reward-model-loss" class="level3">
<h3 class="anchored" data-anchor-id="reward-model-loss">Reward Model Loss</h3>
<p>Given a dataset of preferences <span class="math inline">\(\mathcal{D}\)</span>, we can train a parameterized reward model <span class="math inline">\(r_\phi(x, y)\)</span> using maximum likelihood estimation. We want to maximize the probability of observing the preferences in our dataset:</p>
<p><span class="math display">\[
\max_\phi \prod_{(x, y_w, y_l) \in \mathcal{D}} P(y_w \succ y_l | x)
\]</span></p>
<p>Taking the log and negating (to turn maximization into minimization), we get the negative log-likelihood loss:</p>
<p><span class="math display">\[
\boxed{\mathcal{L}_{\text{RM}}(\phi) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma\left(r_\phi(x, y_w) - r_\phi(x, y_l)\right)\right]} \tag{II.III}
\]</span></p>
<p>This is just binary cross-entropy objective which helps the reward model learn to assign higher rewards to preferred responses. Notice that the Bradley-Terry model depends <strong>only on the difference of rewards</strong>: <span class="math inline">\(r(x, y_w) - r(x, y_l)\)</span>. The absolute values dont matter, it is only their relative ordering. This means:</p>
<blockquote class="blockquote">
<p>If we add any constant <span class="math inline">\(c\)</span> or any function <span class="math inline">\(f(x)\)</span> that depends only on the prompt (not the response), the <strong>preference probabilities dont change</strong>. This invariance property will be the key to deriving DPO.</p>
</blockquote>
</section>
</section>
<section id="iii-optimal-policy-in-closed-form" class="level2">
<h2 class="anchored" data-anchor-id="iii-optimal-policy-in-closed-form">III: Optimal Policy in Closed Form</h2>
<p>Here, we will find the <strong>exact</strong> analytical optimal policy solution to the optimization problem. We want to find the policy that maximizes expected reward while keeping the KL divergence from the reference policy bounded:</p>
<p><span class="math display">\[
\max_\pi \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(y|x)}\left[r(x, y)\right] - \beta \cdot D_{\text{KL}}\left(\pi(y|x) \| \pi_{\text{ref}}(y|x)\right) \tag{III.I}
\]</span></p>
<p>Note, I am writing <span class="math inline">\(\pi\)</span> instead of <span class="math inline">\(\pi_\theta\)</span> to emphasize that we are looking for the optimal policy in general not just the parameterized version.</p>
<p>Expanding the KL divergence:</p>
<p><span class="math display">\[
D_{\text{KL}}\left(\pi(y|x) \| \pi_{\text{ref}}(y|x)\right) = \mathbb{E}_{y \sim \pi(y|x)}\left[\log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]
\]</span></p>
<p>So our objective becomes:</p>
<p><span class="math display">\[
\max_\pi \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y|x)}\left[r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]
\]</span></p>
<p>For a fixed prompt <span class="math inline">\(x\)</span>, we want to find the distribution <span class="math inline">\(\pi(\cdot|x)\)</span> that maximizes:</p>
<p><span class="math display">\[
\mathbb{E}_{y \sim \pi(y|x)}\left[r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]
\]</span></p>
<p>This is a constrained optimization problem over probability distributions. We can solve it using the method of <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>, enforcing that <span class="math inline">\(\pi(y|x)\)</span> sums to 1. For discrete <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}(\pi, \lambda) = \sum_y \pi(y|x) \left[r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right] + \lambda \left(1 - \sum_y \pi(y|x)\right)
\]</span></p>
<p>Taking the derivative with respect to <span class="math inline">\(\pi(y|x)\)</span> and setting it to zero (stationary point):</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \pi(y|x)} = r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \beta - \lambda = 0
\]</span></p>
<p>Now, solving for <span class="math inline">\(\pi(y|x)\)</span>:</p>
<p><span class="math display">\[
\log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} = \frac{1}{\beta}\left(r(x, y) - \beta - \lambda\right)
\]</span></p>
<p><span class="math display">\[
\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} = \exp\left(\frac{1}{\beta}r(x, y)\right) \cdot \exp\left(-1 - \frac{\lambda}{\beta}\right)
\]</span></p>
<p><span class="math display">\[
\pi(y|x) = \pi_{\text{ref}}(y|x) \cdot \exp\left(\frac{1}{\beta}r(x, y)\right) \cdot \exp\left(-1 - \frac{\lambda}{\beta}\right)
\]</span></p>
<p>The term <span class="math inline">\(\exp\left(-1 - \frac{\lambda}{\beta}\right)\)</span> is a constant (with respect to <span class="math inline">\(y\)</span>) that ensures normalization. To find its value, we enforce that <span class="math inline">\(\pi(y|x)\)</span> must be a valid probability distribution and sum to 1:</p>
<p><span class="math display">\[
\sum_y \pi(y|x) = 1
\]</span></p>
<p>Substituting our expression for <span class="math inline">\(\pi(y|x)\)</span>:</p>
<p><span class="math display">\[
\sum_y \pi_{\text{ref}}(y|x) \cdot \exp\left(\frac{1}{\beta}r(x, y)\right) \cdot \exp\left(-1 - \frac{\lambda}{\beta}\right) = 1
\]</span></p>
<p>Since <span class="math inline">\(\exp\left(-1 - \frac{\lambda}{\beta}\right)\)</span> doesn’t depend on <span class="math inline">\(y\)</span>, we can factor it out of the sum:</p>
<p><span class="math display">\[
\exp\left(-1 - \frac{\lambda}{\beta}\right) \cdot \sum_y \pi_{\text{ref}}(y|x) \cdot \exp\left(\frac{1}{\beta}r(x, y)\right) = 1
\]</span></p>
<p>Solving for the constant:</p>
<p><span class="math display">\[
\exp\left(-1 - \frac{\lambda}{\beta}\right) = \frac{1}{\sum_y \pi_{\text{ref}}(y|x) \cdot \exp\left(\frac{1}{\beta}r(x, y)\right)}
\]</span></p>
<p>We define this normalizing sum as the <strong>partition function</strong> <span class="math inline">\(Z(x)\)</span>:</p>
<p><span class="math display">\[
Z(x) = \sum_y \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta}r(x, y)\right) \tag{III.II}
\]</span></p>
<p>Substituting back, we get the optimal policy:</p>
<p><span class="math display">\[
\boxed{\pi_r(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta}r(x, y)\right)} \tag{III.III}
\]</span></p>
<p>We have an exact closed-form expression for the optimal policy. However, we cannot compute it directly because <span class="math inline">\(Z(x)\)</span> is intractable. To compute it, we need to sum over <strong>all possible responses</strong> <span class="math inline">\(y\)</span> which not possible.</p>
</section>
<section id="iv-the-reparameterization-trick" class="level2">
<h2 class="anchored" data-anchor-id="iv-the-reparameterization-trick">IV: The Reparameterization Trick</h2>
<p>The key insight of DPO is to flip the relationship between reward and policy. Now, we frame the problem as: “given an optimal policy what reward function does it correspond to?”</p>
<p>Starting from the optimal policy equation (III.III):</p>
<p><span class="math display">\[
\pi_r(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta}r(x, y)\right)
\]</span></p>
<p>We solve for the reward <span class="math inline">\(r(x, y)\)</span> by first taking the log of both sides:</p>
<p><span class="math display">\[
\log \pi_r(y|x) = \log \pi_{\text{ref}}(y|x) + \frac{1}{\beta}r(x, y) - \log Z(x)
\]</span></p>
<p>Now rearrange to get <span class="math inline">\(r(x, y)\)</span> on left-side:</p>
<p><span class="math display">\[
\frac{1}{\beta}r(x, y) = \log \pi_r(y|x) - \log \pi_{\text{ref}}(y|x) + \log Z(x)
\]</span></p>
<p><span class="math display">\[
r(x, y) = \beta \log \pi_r(y|x) - \beta \log \pi_{\text{ref}}(y|x) + \beta \log Z(x)
\]</span></p>
<p>This can be written more compactly as:</p>
<p><span class="math display">\[
\boxed{r(x, y) = \beta \log \frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)} \tag{IV.I}
\]</span></p>
<p>The reward is expressed as: - A term involving the log-ratio of the optimal policy to the reference policy - <span class="math inline">\(\beta \log Z(x)\)</span> which depends only on <span class="math inline">\(x\)</span> (not on <span class="math inline">\(y\)</span>)</p>
</section>
<section id="v-deriving-the-dpo-loss" class="level2">
<h2 class="anchored" data-anchor-id="v-deriving-the-dpo-loss">V: Deriving the DPO Loss</h2>
<p>Finally, we have all the pieces to derive the DPO loss. From Section II, the Bradley-Terry preference model is:</p>
<p><span class="math display">\[
P(y_w \succ y_l | x) = \sigma\left(r(x, y_w) - r(x, y_l)\right)
\]</span></p>
<p>From Section IV, assuming we have access to an optimal policy <span class="math inline">\(\pi^*\)</span>, the reward can be written as:</p>
<p><span class="math display">\[
r(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)
\]</span></p>
<p>Substituting this into Bradley-Terry:</p>
<p><span class="math display">\[
P(y_w \succ y_l | x) = \sigma\left(\left[\beta \log \frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)} + \beta \log Z(x)\right] - \left[\beta \log \frac{\pi^*(y_l|x)}{\pi_{\text{ref}}(y_l|x)} + \beta \log Z(x)\right]\right)
\]</span></p>
<p>Simplifying the expression inside the sigmoid:</p>
<p><span class="math display">\[
= \sigma\left(\beta \log \frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)} + \beta \log Z(x) - \beta \log \frac{\pi^*(y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \beta \log Z(x)\right)
\]</span></p>
<p>The <span class="math inline">\(\beta \log Z(x)\)</span> terms cancel:</p>
<p><span class="math display">\[
= \sigma\left(\beta \log \frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi^*(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)
\]</span></p>
<blockquote class="blockquote">
<p>Now recall the critical insight from Section II where we mentioned that the Bradley-Terry model depends only on reward <strong>differences</strong>. Thus, when we compute <span class="math inline">\(r(x, y_w) - r(x, y_l)\)</span> the intractable partition function <span class="math inline">\(Z(x)\)</span> cancels out. This is what makes DPO possible.</p>
</blockquote>
<p>We can write this more cleanly by defining the <strong>implicit reward</strong> in terms of the optimal policy:</p>
<p><span class="math display">\[
\hat{r}(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)}
\]</span></p>
<p>Thus:</p>
<p><span class="math display">\[
P(y_w \succ y_l | x) = \sigma\left(\hat{r}(x, y_w) - \hat{r}(x, y_l)\right)
\]</span></p>
<p>We dont actually have access to the optimal policy <span class="math inline">\(\pi^*\)</span>. But we can <strong>parameterize</strong> a policy <span class="math inline">\(\pi_\theta\)</span> and optimize it to maximize the likelihood of the observed preferences. This is exactly what the reward model loss (II.III) does except now <strong>our reward is implicitly defined by the policy itself</strong>.</p>
<p>The DPO loss is the negative log-likelihood:</p>
<p><span class="math display">\[
\boxed{\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]} \tag{V.I}
\]</span></p>
<p>for implicit reward notation, it can be written as:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma\left(\hat{r}_\theta(x, y_w) - \hat{r}_\theta(x, y_l)\right)\right] \tag{V.II}
\]</span></p>
<p>Some key insights from the above DPO loss:</p>
<ul>
<li>The policy implicitly defines its own reward via the log-ratio with the reference. There is <strong>no separate reward model</strong>.</li>
<li>This is just a supervised classification loss on preference pairs with <strong>no RL</strong>.</li>
<li>DPO uses the fixed preference dataset <span class="math inline">\(\mathcal{D}\)</span>. Thus, <strong>no sampling during training</strong>.</li>
<li><strong>No value function</strong> needed since we’re not doing policy gradients</li>
<li>DPO still optimizes the KL-constrained reward maximization objective but in a different way</li>
</ul>
</section>
<section id="vi-building-intuition-for-dpo" class="level2">
<h2 class="anchored" data-anchor-id="vi-building-intuition-for-dpo">VI: Building Intuition for DPO</h2>
<p>Now that we have the DPO loss we can build some intuition around the implicit reward model and its gradient updates.</p>
<section id="implicit-reward-model" class="level3">
<h3 class="anchored" data-anchor-id="implicit-reward-model">Implicit Reward Model</h3>
<p>The DPO paper subtitle is <strong>“Your Language Model is Secretly a Reward Model”</strong> and this captures the key insight as we are using the LLM for implicit reward. The policy <span class="math inline">\(\pi_\theta\)</span> defines an implicit reward function:</p>
<p><span class="math display">\[
\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}
\]</span></p>
<p>This reward measures how much more likely the current policy is to generate response <span class="math inline">\(y\)</span> compared to the reference policy, scaled by <span class="math inline">\(\beta\)</span>.</p>
<ul>
<li>If <span class="math inline">\(\pi_\theta(y|x) &gt; \pi_{\text{ref}}(y|x)\)</span>: The implicit reward is positive (the policy “likes” this response more than reference)</li>
<li>If <span class="math inline">\(\pi_\theta(y|x) &lt; \pi_{\text{ref}}(y|x)\)</span>: The implicit reward is negative (the policy “likes” this response less than reference)</li>
</ul>
</section>
<section id="analyzing-the-gradient-update" class="level3">
<h3 class="anchored" data-anchor-id="analyzing-the-gradient-update">Analyzing the Gradient Update</h3>
<p>We can flex our brain muscles one more time and compute the gradient for the DPO objective:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{DPO}} = -\mathbb{E}\left[\log \sigma\left(\hat{r}_\theta(x, y_w) - \hat{r}_\theta(x, y_l)\right)\right]
\]</span></p>
<p>Let <span class="math inline">\(u = \hat{r}_\theta(x, y_w) - \hat{r}_\theta(x, y_l)\)</span>. Using the chain rule:</p>
<p><span class="math display">\[
\nabla_\theta \mathcal{L}_{\text{DPO}} = -\mathbb{E}\left[\frac{\sigma'(u)}{\sigma(u)} \nabla_\theta u\right]
\]</span></p>
<p>Using the property <span class="math inline">\(\sigma'(u) = \sigma(u)(1 - \sigma(u))\)</span>:</p>
<p><span class="math display">\[
= -\mathbb{E}\left[\frac{\sigma(u)(1-\sigma(u))}{\sigma(u)} \nabla_\theta u\right] = -\mathbb{E}\left[(1 - \sigma(u)) \nabla_\theta u\right]
\]</span></p>
<p>Using <span class="math inline">\(1 - \sigma(u) = \sigma(-u)\)</span>:</p>
<p><span class="math display">\[
= -\mathbb{E}\left[\sigma(-u) \nabla_\theta u\right]
\]</span></p>
<p>Now, <span class="math inline">\(-u = \hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w)\)</span>, and:</p>
<p><span class="math display">\[
\nabla_\theta u = \nabla_\theta \hat{r}_\theta(x, y_w) - \nabla_\theta \hat{r}_\theta(x, y_l) = \beta \nabla_\theta \log \pi_\theta(y_w|x) - \beta \nabla_\theta \log \pi_\theta(y_l|x)
\]</span></p>
<p>Putting it together:</p>
<p><span class="math display">\[
\boxed{\nabla_\theta \mathcal{L}_{\text{DPO}} = -\beta \mathbb{E}\left[\underbrace{\sigma\left(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w)\right)}_{\text{weight}}\left(\underbrace{\nabla_\theta \log \pi_\theta(y_w|x)}_{\text{increase } y_w} - \underbrace{\nabla_\theta \log \pi_\theta(y_l|x)}_{\text{decrease } y_l}\right)\right]} \tag{VI.I}
\]</span></p>
<ul>
<li><span class="math inline">\(\nabla_\theta \log \pi_\theta(y_w|x)\)</span> points in the direction that increases probability of the preferred response</li>
<li><span class="math inline">\(-\nabla_\theta \log \pi_\theta(y_l|x)\)</span> points in the direction that decreases probability of the dispreferred response</li>
<li>The weight term is high when <span class="math inline">\(\hat{r}_\theta(x, y_l) &gt; \hat{r}_\theta(x, y_w)\)</span>, i.e.&nbsp;when the model currently assigns higher implicit reward to the losing response than the winning response. In other words:
<ul>
<li><strong>When the model is wrong</strong> (ranks <span class="math inline">\(y_l\)</span> above <span class="math inline">\(y_w\)</span>), we get large gradient updates</li>
<li><strong>When the model is right</strong> (ranks <span class="math inline">\(y_w\)</span> above <span class="math inline">\(y_l\)</span>), we get small gradient updates</li>
</ul></li>
</ul>
<p>This dynamic sigmoid weighting is crucial. It naturally focuses learning on the examples the model currently gets wrong.</p>
</section>
</section>
<section id="vii-computing-log-probabilities-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="vii-computing-log-probabilities-in-practice">VII: Computing Log Probabilities in Practice</h2>
<blockquote class="blockquote">
<p>This section is fully adapted from Umar Jamil’s video. I think it is essential to understand how log probabilities are computed in practice.</p>
</blockquote>
<p>The DPO loss requires computing <span class="math inline">\(\log \pi_\theta(y|x)\)</span>, the log probability of a complete response <span class="math inline">\(y\)</span> given a prompt <span class="math inline">\(x\)</span>. Let’s see how this works in practice with LLMs.</p>
<p>Language models are autoregressive: they generate text one token at a time, conditioning on all previous tokens. For a response <span class="math inline">\(y = (y_1, y_2, \ldots, y_T)\)</span>, the probability factorizes as:</p>
<p><span class="math display">\[
\pi_\theta(y|x) = \prod_{t=1}^{T} \pi_\theta(y_t | x, y_1, \ldots, y_{t-1}) = \prod_{t=1}^{T} \pi_\theta(y_t | x, y_{&lt;t})
\]</span></p>
<p>Taking the logarithm:</p>
<p><span class="math display">\[
\boxed{\log \pi_\theta(y|x) = \sum_{t=1}^{T} \log \pi_\theta(y_t | x, y_{&lt;t})} \tag{VII.I}
\]</span></p>
<p><strong>The log probability of the full response is the sum of log probabilities at each position.</strong></p>
<p>Here’s how to compute <span class="math inline">\(\log \pi_\theta(y|x)\)</span>:</p>
<ol type="1">
<li><p><strong>Prepare input</strong>: Concatenate the prompt and response into a single sequence</p>
<p><span class="math display">\[\text{input} = [x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_T]\]</span></p></li>
<li><p><strong>Forward pass</strong>: Run the transformer to get hidden states at each position</p></li>
<li><p><strong>Project to logits</strong>: Apply the language model head (typically a linear layer) to get vocabulary logits at each position</p></li>
<li><p><strong>Log softmax</strong>: Convert logits to log probabilities over the vocabulary using <code>logsoftmax</code></p></li>
<li><p><strong>Gather relevant log probs</strong>: For each position <span class="math inline">\(t\)</span> in the response extract the log probability of the actual next token <span class="math inline">\(y_t\)</span> (since we know the output)</p></li>
<li><p><strong>Sum with masking</strong>: Sum the log probabilities but only for response tokens (not prompt tokens)</p>
<p><span class="math display">\[\log \pi_\theta(y|x) = \sum_{t \in \text{res pos}} \log \pi_\theta(y_t | x, y_{&lt;t})\]</span></p></li>
</ol>
<p>This gives us <span class="math inline">\(\log \pi_\theta(y|x)\)</span> for one response. We do this for both the preferred response <span class="math inline">\(y_w\)</span> and the dispreferred response <span class="math inline">\(y_l\)</span> and we also do it for both the policy model <span class="math inline">\(\pi_\theta\)</span> and the frozen reference model <span class="math inline">\(\pi_{\text{ref}}\)</span>. With these four log probabilities in hand, we can compute the DPO loss.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Once you derive the DPO loss, you start appreciating the simplicity and elegance of the solution especially when compared to PPO. The derivation hinges on one observation that the Bradley-Terry model only cares about reward differences and this causes the intractable partition function from analytical solution to cancel out completely. In turn, what remains is a straightforward classification loss.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><strong>Papers:</strong>
<ul>
<li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>: The original DPO paper</li>
<li><a href="https://arxiv.org/pdf/2203.02155">Training language models to follow instructions with human feedback</a>: The InstructGPT paper that established PPO-based RLHF</li>
</ul></li>
<li><strong>Videos:</strong>
<ul>
<li><a href="https://www.youtube.com/watch?v=hvGa5Mba4c8">Umar Jamil’s video on DPO</a>: Excellent walkthrough of the DPO derivation</li>
</ul></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/garg-aayush\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/garg-aayush">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aayush-garg-8b26a734">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Aayush_ander">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>