<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-12-03">
<meta name="description" content="A hands-on journey implementing SFT from scratch, covering reasoning SFT with Qwen2.5-Math and instruction SFT with Llama-3.1-8B, along with debugging lessons learned.">

<title>What I Learned Building SFT from the Ground Up â€“ Aayush Garg</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../static/img/aayush_cropped.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-2d3f84a5f37526cd3979248aedf2fbfb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog/"> 
<span class="menu-text">Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/garg-aayush"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aayush-garg-8b26a734"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Aayush_ander"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-i-built" id="toc-what-i-built" class="nav-link active" data-scroll-target="#what-i-built">What I Built</a></li>
  <li><a href="#part-1-reasoning-sft-with-qwen2.5-math-1.5b" id="toc-part-1-reasoning-sft-with-qwen2.5-math-1.5b" class="nav-link" data-scroll-target="#part-1-reasoning-sft-with-qwen2.5-math-1.5b">Part 1: Reasoning SFT with Qwen2.5-Math-1.5B</a>
  <ul class="collapse">
  <li><a href="#creating-the-dataset-first-challenge" id="toc-creating-the-dataset-first-challenge" class="nav-link" data-scroll-target="#creating-the-dataset-first-challenge">Creating the Dataset: First Challenge</a></li>
  <li><a href="#the-training-loop-per-token-vs.-sequence-loss" id="toc-the-training-loop-per-token-vs.-sequence-loss" class="nav-link" data-scroll-target="#the-training-loop-per-token-vs.-sequence-loss">The Training Loop: Per-Token vs.&nbsp;Sequence Loss</a></li>
  <li><a href="#vllm-integration-the-debugging-nightmare" id="toc-vllm-integration-the-debugging-nightmare" class="nav-link" data-scroll-target="#vllm-integration-the-debugging-nightmare">vLLM Integration: The Debugging Nightmare</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul></li>
  <li><a href="#part-2-instruction-sft-with-llama-3.1-8b" id="toc-part-2-instruction-sft-with-llama-3.1-8b" class="nav-link" data-scroll-target="#part-2-instruction-sft-with-llama-3.1-8b">Part 2: Instruction SFT with Llama-3.1-8B</a>
  <ul class="collapse">
  <li><a href="#the-prompt-masking-implementation-problem" id="toc-the-prompt-masking-implementation-problem" class="nav-link" data-scroll-target="#the-prompt-masking-implementation-problem">The Prompt Masking Implementation Problem</a></li>
  <li><a href="#setting-up-alpacaeval" id="toc-setting-up-alpacaeval" class="nav-link" data-scroll-target="#setting-up-alpacaeval">Setting Up AlpacaEval</a></li>
  <li><a href="#results-and-analysis" id="toc-results-and-analysis" class="nav-link" data-scroll-target="#results-and-analysis">Results and Analysis</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">What I Learned Building SFT from the Ground Up</h1>
</div>

<div>
  <div class="description">
    A hands-on journey implementing SFT from scratch, covering reasoning SFT with Qwen2.5-Math and instruction SFT with Llama-3.1-8B, along with debugging lessons learned.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 3, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Over the past few weeks, I implemented supervised fine-tuning (SFT) from scratch, continuing a series of projects where Iâ€™m building foundational LLM components as a learning exercise from the ground up. Previously, Iâ€™ve worked through <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/gpt-2">implementing GPT-2 from scratch</a> and <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/llm-inference">writing LLM inference scripts from the ground up</a>. Naturally, SFT was the next step in this series.</p>
<p>One thing I realized pretty quickly, writing the training scripts from scratch is not the most difficult part. However, making it actually work, producing results that seems reasonable is where the real challenge begins ðŸ˜…. You run into all sorts of difficulties: debugging annoying errors, dealing with gradient instabilities, getting vLLM to cooperate for intermediate evaluation (especially with limited GPU memory) etc. <strong>These are the things that eat up your time but teach you the most</strong>.</p>
<p>In this post, I want to share not just what I built, but the building and debugging journey that got me there.</p>
<section id="what-i-built" class="level2">
<h2 class="anchored" data-anchor-id="what-i-built">What I Built</h2>
<p>I loosely followed Stanfordâ€™s <a href="https://github.com/stanford-cs336/assignment5-alignment">CS336 Assignment 5</a> as a guide, wrote all the SFT core components, and ran two sets of experiments:</p>
<p><strong>1. Reasoning SFT</strong>: Fine-tuned <a href="https://huggingface.co/Qwen/Qwen2.5-Math-1.5B">Qwen2.5-Math-1.5B</a> on math reasoning traces to improve step-by-step problem solving capabilities.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/sft_reasoning_summary.png" alt="Reasoning SFT Results" width="600">
</p><p align="center">
<em>Best: <b>53.4% reward accuracy</b> (up from 2.9% baseline) with 99.3% format accuracy</em>
</p>
<p></p>
<strong>2. Instruction SFT</strong>: Fine-tuned <a href="https://huggingface.co/meta-llama/Llama-3.1-8B">Llama-3.1-8B</a> on UltraChat-200K + SafetyLlama for general instruction following and safety.
<p align="center">
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/instruct_finetune_results_nomask.png" alt="Reasoning SFT Results">
</p><p align="center">
<em>Best: GSM8K 16-&gt;<b>33%</b>, Safety 62-&gt;<b>78%</b>, AlpacaEval 1.6-&gt;<b>5.3%</b>, MMLU ~58%</em>
</p>
<p></p>
<p>All experiment code, training scripts, and detailed notes are available in my <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/sft">building-from-scratch</a> repo.</p>
</section>
<section id="part-1-reasoning-sft-with-qwen2.5-math-1.5b" class="level2">
<h2 class="anchored" data-anchor-id="part-1-reasoning-sft-with-qwen2.5-math-1.5b">Part 1: Reasoning SFT with Qwen2.5-Math-1.5B</h2>
<p>The idea behind reasoning SFT is simple. You take a base model that barely outputs correct answers, show it high-quality examples of <em>how</em> to solve problems step-by-step, and train it to replicate/mimic that reasoning process. The model learns to think in a structured format with first generating reasoning inside <code>&lt;think&gt;</code> tags, then outputting the final answer in <code>&lt;answer&gt;</code> tags.</p>
<p>My starting point was <code>Qwen2.5-Math-1.5B</code>, which had quite poor baseline accuracies on the math validation set: <strong>~2.9%</strong> for answers and <strong>~14%</strong> for format.</p>
<section id="creating-the-dataset-first-challenge" class="level3">
<h3 class="anchored" data-anchor-id="creating-the-dataset-first-challenge">Creating the Dataset: First Challenge</h3>
<p>The original CS336 MATH dataset used for SFT training is not publicly available, so I had to create my own. My dataset creation pipeline had three steps:</p>
<ol type="1">
<li><p><strong>Source problems</strong>: I used <a href="https://huggingface.co/datasets/hiyouga/math12k">hiyouga/math12k</a> dataset to create the training set, carefully filtering out any problems that appeared in the validation set to avoid data leakage.</p></li>
<li><p><strong>Generate reasoning traces</strong>: The next and most important step is to <strong>generate the reasoning traces for each problem</strong>. I used <code>gpt-oss-120b</code> model to generate them via Fireworks Batch Inference API. It costed me around ~$4 to generate the reasoning traces.</p></li>
<li><p><strong>Filter for quality</strong>: I also created a subset of around ~3.6K examples by filtering out the reasoning traces that led to wrong answers.</p></li>
</ol>
</section>
<section id="the-training-loop-per-token-vs.-sequence-loss" class="level3">
<h3 class="anchored" data-anchor-id="the-training-loop-per-token-vs.-sequence-loss">The Training Loop: Per-Token vs.&nbsp;Sequence Loss</h3>
<p>The original assignment uses sequence level loss normalization where you sum the loss over all tokens in a sequence and normalize by a constant, not by the variable number of tokens.</p>
<p>While running the initial experiments, I noticed the gradient norms were really large values, and training felt unstable. Even though the loss seemed to be going in the right direction, something didnâ€™t feel right. After some investigation, I realized the issue: with variable-length sequences (my training examples ranged from short to quite long), longer sequences contribute more to the gradient than shorter ones. This creates high variance in gradient updates.</p>
<table align="center">
<tbody><tr>
<td>
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/grad_norm_wo_token_loss.png" alt="Gradient Norm without Per-Token Loss" width="400">
</td>
<td>
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/grad_norm_w_token_loss.png" alt="Gradient Norm with Per-Token Loss" width="400">
</td>
</tr>
</tbody></table>
<p align="center">
<em>Left: Sequence-level loss (high variance gradients) | Right: Per-token loss (stable gradients)</em>
</p>
<p>Thus, I added a <code>per_token_loss</code> flag to my training step which when enabled normalizes the loss by the actual number of response tokens in each sequence. The difference was noticeable with subtle improved accuracy. More importantly, the gradients became much more stable with per-token normalization.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Run</th>
<th>Loss Normalization</th>
<th>Reward Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>run_filtered</td>
<td>Per-token</td>
<td>0.5204</td>
</tr>
<tr class="even">
<td>run_filtered-res-len</td>
<td>Sequence-level</td>
<td>0.5106</td>
</tr>
</tbody>
</table>
</section>
<section id="vllm-integration-the-debugging-nightmare" class="level3">
<h3 class="anchored" data-anchor-id="vllm-integration-the-debugging-nightmare">vLLM Integration: The Debugging Nightmare</h3>
<p>Hereâ€™s where things got really tricky and painful. I wanted to run intermediate evaluations during training using vLLM for fast inference. The assignment provided code for this but it was written for an older vLLM version and nothing worked out of the box ðŸ˜….</p>
<p><strong>Problem 1: vLLM initialization changed</strong></p>
<p>The assignmentâ€™s approach used a separate GPU dedicated to running vLLM as an inference server. I wasnâ€™t keen on this setup anyway as it meant paying for an extra GPU just for inference. But more importantly, the approach broke completely with the vLLM version I was using (0.7+). The initialization logic had changed, and the old code just wouldnâ€™t run.</p>
<p><strong><em>Solution</em></strong>: I switched to the <code>colocate</code> approach, running vLLM on the same device as the training model. I came across this in the excellent <a href="https://huggingface.co/blog/vllm-colocate">HuggingFace blog post on co-located vLLM</a>. Though, this required being more careful about GPU memory (setting appropriate values for <code>gpu_memory_utilization</code>, <code>max_model_len</code>, and <code>max_num_seqs</code>), but it actually works and saves on GPU costs.</p>
<p><strong>Problem 2: Missing <code>model_executor</code> attribute</strong></p>
<p>When I tried to load updated model weights into the vLLM instance during training, I hit this error:</p>
<pre><code>AttributeError: 'LLMEngine' object has no attribute 'model_executor'</code></pre>
<p>This was really annoying because the attribute clearly existed in the vLLM source code. After much debugging, I found two solutions: - Downgrade to vLLM 0.10.2, or - If using vLLM 0.11.0, set the environment variable <code>VLLM_ENABLE_V1_MULTIPROCESSING=0</code> at the start of the script</p>
<p>I went with the environment variable approach since I didnâ€™t want to deal with version conflicts.</p>
<p><strong>Problem 3: The <code>_orig_mod</code> issue</strong></p>
<p>With <code>torch.compile</code> enabled on my model (for faster training), loading weights into vLLM failed with the below error. The issue is that <code>torch.compile</code> wraps the original model and stores the actual weights under <code>_orig_mod</code>. When loading weights into vLLM, you need to access them through this attribute, not directly from the compiled model.</p>
<pre><code>ValueError: There is no module or parameter named '_orig_mod' in Qwen2ForCausalLM</code></pre>
<p><strong><em>Solution</em></strong>: In my <code>load_policy_into_vllm_instance</code> function, I made sure to load from <code>model._orig_mod</code> when the model is compiled.</p>
<p><strong>These three issues cost me almost a day. However, it was worth it because I learned a lot about vLLM and how to integrate it in training run</strong></p>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>After all that debugging, hereâ€™s what the training runs achieved:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/sft_train_reasoning_results.png" class="img-fluid figure-img"></p>
<figcaption>Reasoning SFT Results</figcaption>
</figure>
</div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Run</th>
<th>Training Data</th>
<th>Reward Accuracy</th>
<th>Format Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>baseline</td>
<td>-</td>
<td>0.0288</td>
<td>0.1438</td>
</tr>
<tr class="even">
<td>run_all</td>
<td>Full 4.8K (correct + incorrect)</td>
<td>0.4214</td>
<td>0.9924</td>
</tr>
<tr class="odd">
<td>run_filtered</td>
<td>Filtered 3.6K (correct only)</td>
<td>0.5204</td>
<td>0.9906</td>
</tr>
<tr class="even">
<td>run_filtered-2epoch</td>
<td>Filtered 3.6K (2 epochs)</td>
<td>0.5336</td>
<td>0.9926</td>
</tr>
</tbody>
</table>
<p><strong>Key takeaways</strong>: - Filtering out incorrect reasoning traces boosted accuracy from 42% to 52%. Training on wrong traces teaches the model wrong patterns. - The model quickly learned the output format (99%+ format accuracy after training). - Running for 2 epochs gave a boost in accuracy though a marginal one.</p>
</section>
</section>
<section id="part-2-instruction-sft-with-llama-3.1-8b" class="level2">
<h2 class="anchored" data-anchor-id="part-2-instruction-sft-with-llama-3.1-8b">Part 2: Instruction SFT with Llama-3.1-8B</h2>
<p>With the reasoning SFT working, I moved on to the second part: instruction fine-tuning. This loosely follows the <a href="https://github.com/stanford-cs336/assignment5-alignment/blob/main/cs336_spring2025_assignment5_supplement_safety_rlhf.pdf">CS336 Supplementary Assignment 5</a>, where the goal is to build a model that can follow diverse instructions and refuse harmful requests.</p>
<p>Unlike reasoning SFT, instruction fine-tuning uses conversational instruction-response pairs. The training data combines <strong>UltraChat-200K</strong> (diverse multi-turn conversations) and <strong>SafetyLlama</strong> (safety-focused examples) totaling around 200K examples, formatted using the Alpaca prompt template.</p>
<p>For evaluation, I used four benchmarks as specified in the assignment: - <strong>GSM8K</strong>: Grade-school math problems (tests math reasoning) - <strong>MMLU</strong>: Multiple-choice questions across 57 subjects (tests factual knowledge) - <strong>AlpacaEval</strong>: Open-ended instructions judged by LLM-as-judge (tests instruction-following quality)<br>
- <strong>Simple Safety Tests (SST)</strong>: Harmful prompts to test refusal behavior (tests safety)</p>
<section id="the-prompt-masking-implementation-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-prompt-masking-implementation-problem">The Prompt Masking Implementation Problem</h3>
<p>I wanted to experiment with <strong>prompt masking</strong> i.e.&nbsp;masking prompt tokens (labels = -100) so the loss is computed only on response tokens, helping the model focus on generating good responses.</p>
<p><strong>Problem 1: BPE tokenization boundary issues</strong></p>
<p>Implementing this led to an interesting debugging session. When I tokenized the prompt separately (ending with <code>"### Response:\n"</code>) and compared it to the tokens in the full sequence (prompt + response), the boundary tokens didnâ€™t match. This is a known issue of BPE tokenization: subword merging behavior changes based on context.</p>
<p>My first instinct was to try to implement complex boundary detection logic. However, I thought letâ€™s try the simplest fix that works.</p>
<p><strong><em>Solution</em></strong>: I decided to drop the last token from the prompt before masking. This is a bit quick fix. However, I might train on one extra formatting token (likely just a newline) but will never accidentally mask response tokens.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conservative fix: drop last prompt token to avoid boundary issues</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>prompt_length <span class="op">=</span> <span class="bu">len</span>(prompt_tokens) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>labels[:prompt_length] <span class="op">=</span> <span class="op">-</span><span class="dv">100</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Problem 2: Very short or empty responses</strong></p>
<p>Another issue I ran into with prompt masking, some training examples had very short or empty responses. When all tokens are masked leaving only a few response tokens, the cross-entropy loss calculation can produce extreme values or NaNs.</p>
<p><strong><em>Solution</em></strong>: The fix was simple. I filtered out examples with very short responses (0-2 words) from both training and validation sets.</p>
</section>
<section id="setting-up-alpacaeval" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-alpacaeval">Setting Up AlpacaEval</h3>
<p>A quick note on the AlpacaEval evaluation setup. It uses an LLM-as-judge approach where an annotator model compares outputs from your mode against GPT-4 reference responses.</p>
<p>The assignment suggested deploying <code>Llama-3.3-70B-Instruct</code> locally as the annotator, but that requires at least two GPUs which is not cost effective (atleast for my case). Instead, I used <code>Llama-3.3-70B-Instruct</code> via Fireworks API. This required some config tweaking (API key mapping, judge configuration) but works well.</p>
</section>
<section id="results-and-analysis" class="level3">
<h3 class="anchored" data-anchor-id="results-and-analysis">Results and Analysis</h3>
<p>I ran two experiments: one with prompt masking (<code>mask</code>) and one without (<code>no-mask</code>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/instruct_finetune_results_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Instruction Fine-tuning Comparison</figcaption>
</figure>
</div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Baseline</th>
<th>No-Mask</th>
<th>Mask</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>GSM8K</strong></td>
<td>16.4%</td>
<td>29.0%</td>
<td><strong>32.7%</strong></td>
</tr>
<tr class="even">
<td><strong>MMLU</strong></td>
<td>58.1%</td>
<td>58.4%</td>
<td>58.2%</td>
</tr>
<tr class="odd">
<td><strong>SST Safety</strong></td>
<td>62.0%</td>
<td><strong>78.0%</strong></td>
<td>77.0%</td>
</tr>
<tr class="even">
<td><strong>AlpacaEval</strong></td>
<td>1.57%</td>
<td><strong>5.3%</strong></td>
<td>4.5%</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>GSM8K (16% -&gt; 29-33%)</strong>: Both approaches significantly improved math reasoning, but masking helped more (32.7% vs 29.0%).</p></li>
<li><p><strong>Safety (62% -&gt; 78%)</strong>: You see big improvement as expected since the training data includes SafetyLlama examples.</p></li>
<li><p><strong>AlpacaEval (1.6% -&gt; 5.3%)</strong>: The conversational instruction-following improved substantially. Interestingly, no-mask performed slightly better (5.3% vs 4.5%). My guess: training on the full sequence helps the model learn overall conversational patterns and produce more naturally flowing responses that match the prompt style.</p></li>
<li><p><strong>MMLU (~58% -&gt; ~58%)</strong>: This stayed flat and thatâ€™s actually good news. MMLU tests factual knowledge which is encoded during pre-training. SFT teaches the model <em>how</em> to respond, not <em>what</em> to know. The fact that MMLU didnâ€™t drop means we avoided catastrophic forgetting issue.</p></li>
</ul>
<p align="center">
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/instruct_finetune_mmlu_comparison.png" alt="MMLU Subject Comparison">
</p><p align="center">
<em>Looking at individual MMLU subjects, some regressed slightly (college math: 33% -&gt; 26%) while others improved slightly, leading to near-zero net change.</em>
</p>
<p></p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>While writing the SFT code from scratch, I ran into a lot of debugging challenges. It was at times painstaking and frustrating but was also a valuable learning experience. By debugging, I learned a lot about how things work under the hood, and the whole experience prepares you for how to go about debugging code/projects in the future.</p>
<p>I leave you with some of the debugging tips I came across:</p>
<ul>
<li><strong>vLLM OOM</strong>: Tune <code>max_model_len</code>, <code>max_num_seqs</code>, and <code>gpu_memory_utilization</code> and start conservative.</li>
<li><strong>Per-token loss</strong>: Normalize by response token count to prevent long sequences from dominating gradients.</li>
<li><strong>torch.compile + vLLM</strong>: Access weights via <code>model._orig_mod</code> when loading into vLLM.</li>
<li><strong>BPE boundaries</strong>: Drop last prompt token before masking to avoid tokenization edge cases.</li>
<li><strong>Data quality matters</strong>: Filtering incorrect traces gave me a 10% accuracy boost.</li>
<li><strong>vLLM version issues</strong>: Set <code>VLLM_ENABLE_V1_MULTIPROCESSING=0</code> if <code>model_executor</code> is missing.</li>
</ul>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>I have made all the code, datasets, and model checkpoints publicly accessible.</p>
<ul>
<li><strong>Code</strong>: <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/sft">building-from-scratch/sft</a></li>
<li><strong>Datasets</strong>: <a href="https://huggingface.co/datasets/garg-aayush/sft-cs336-assign5-datasets">garg-aayush/sft-cs336-assign5-datasets</a></li>
<li><strong>Checkpoints</strong>:
<ul>
<li>Reasoning:
<ul>
<li>run_all: <a href="https://huggingface.co/garg-aayush/qwen-2.5-math-sft-all">qwen-2.5-math-sft-all-2epoch</a></li>
<li>run_filtered: <a href="https://huggingface.co/garg-aayush/qwen-2.5-math-sft-filtered">qwen-2.5-math-sft-filtered-2epoch</a></li>
<li>run_filtered-res-len: <a href="https://huggingface.co/garg-aayush/qwen-2.5-math-sft-filtered-res-len">qwen-2.5-math-sft-filtered-res-len</a></li>
<li>run_filtered-2epoch: <a href="https://huggingface.co/garg-aayush/qwen-2.5-math-sft-filtered-2epoch">qwen-2.5-math-sft-filtered-2epoch</a></li>
</ul></li>
<li>Instruction d:
<ul>
<li>run_mask: <a href="https://huggingface.co/garg-aayush/llama31-8b-sft-mask">llama31-8b-sft-mask</a></li>
<li>run_nomask: <a href="https://huggingface.co/garg-aayush/llama31-8b-sft-nomask">llama31-8b-sft-nomask</a></li>
</ul></li>
</ul></li>
<li><strong>Training logs</strong>: <a href="https://wandb.ai/garg-aayush/sft">wandb/sft</a> and <a href="https://wandb.ai/garg-aayush/sft_instruct">wandb/sft_instruct</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/garg-aayush\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/garg-aayush">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aayush-garg-8b26a734">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Aayush_ander">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>