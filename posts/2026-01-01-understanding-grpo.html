<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2026-01-01">
<meta name="description" content="A blog post explaining the Group Relative Policy Optimization (GRPO) objective, showing how it simplifies PPO by replacing the learned critic with group sampling.">

<title>Understanding GRPO: PPO without the Critic – Aayush Garg</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../static/img/aayush_cropped.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-2d3f84a5f37526cd3979248aedf2fbfb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-16JWTLXJNG"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-16JWTLXJNG', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog/"> 
<span class="menu-text">Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/garg-aayush"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aayush-garg-8b26a734"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Aayush_ander"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#i-the-ppo-objective-and-the-critic-problem" id="toc-i-the-ppo-objective-and-the-critic-problem" class="nav-link active" data-scroll-target="#i-the-ppo-objective-and-the-critic-problem">I: The PPO Objective and the Critic Problem</a>
  <ul class="collapse">
  <li><a href="#the-value-function-problem-in-ppo" id="toc-the-value-function-problem-in-ppo" class="nav-link" data-scroll-target="#the-value-function-problem-in-ppo">The Value Function Problem in PPO</a></li>
  </ul></li>
  <li><a href="#ii-replacing-the-critic-with-group-sampling" id="toc-ii-replacing-the-critic-with-group-sampling" class="nav-link" data-scroll-target="#ii-replacing-the-critic-with-group-sampling">II: Replacing the Critic with Group Sampling</a>
  <ul class="collapse">
  <li><a href="#monte-carlo-baseline" id="toc-monte-carlo-baseline" class="nav-link" data-scroll-target="#monte-carlo-baseline">Monte Carlo Baseline</a></li>
  <li><a href="#group-relative-advantage" id="toc-group-relative-advantage" class="nav-link" data-scroll-target="#group-relative-advantage">Group-Relative Advantage</a></li>
  </ul></li>
  <li><a href="#iii-the-grpo-objective" id="toc-iii-the-grpo-objective" class="nav-link" data-scroll-target="#iii-the-grpo-objective">III: The GRPO Objective</a>
  <ul class="collapse">
  <li><a href="#the-full-grpo-objective" id="toc-the-full-grpo-objective" class="nav-link" data-scroll-target="#the-full-grpo-objective">The Full GRPO Objective</a></li>
  <li><a href="#single-gradient-step-simplification" id="toc-single-gradient-step-simplification" class="nav-link" data-scroll-target="#single-gradient-step-simplification">Single Gradient Step Simplification</a></li>
  </ul></li>
  <li><a href="#iv-kl-divergence-in-grpo" id="toc-iv-kl-divergence-in-grpo" class="nav-link" data-scroll-target="#iv-kl-divergence-in-grpo">IV: KL Divergence in GRPO</a></li>
  <li><a href="#v-outcome-vs.-process-supervision" id="toc-v-outcome-vs.-process-supervision" class="nav-link" data-scroll-target="#v-outcome-vs.-process-supervision">V: Outcome vs.&nbsp;Process Supervision</a></li>
  <li><a href="#vi-connection-to-reinforce-leave-one-out-rloo" id="toc-vi-connection-to-reinforce-leave-one-out-rloo" class="nav-link" data-scroll-target="#vi-connection-to-reinforce-leave-one-out-rloo">VI: Connection to REINFORCE Leave-One-Out (RLOO)</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Understanding GRPO: PPO without the Critic</h1>
</div>

<div>
  <div class="description">
    A blog post explaining the Group Relative Policy Optimization (GRPO) objective, showing how it simplifies PPO by replacing the learned critic with group sampling.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>In my previous posts, I worked through the derivations of <a href="https://aayushgarg.dev/posts/2025-12-25-deriving-ppo-loss.html">PPO</a> and <a href="https://aayushgarg.dev/posts/2025-12-30-deriving-dpo-loss.html">DPO</a> for LLM post-training. PPO gave us a full-fledged RL approach with clipped surrogate objectives, value functions and GAE-based advantage estimation. DPO on the other hand, showed a clever way to bypass RL entirely by reformulating the optimization as a simple classification loss on preference pairs.</p>
<p>That brings us to Group Relative Policy Optimization (GRPO), introduced in the <a href="https://arxiv.org/abs/2402.03300">DeepSeekMath</a> paper. If you have been following recent developments in reasoning models throughout 2025, GRPO has become one of the most widely used post-training algorithms behind open-source reasoning models.</p>
<p>In simple terms, GRPO can be thought of as PPO without the critic (value function). Recall that PPO trains a value function in addition to the policy to estimate baselines for advantage computation. GRPO takes a simpler approach where it samples multiple completions (“group”) for each prompt and uses their rewards to form a baseline for advantage computation. This group-derived baseline replaces the learned value function entirely (no need to train a critic!).</p>
<p>The practical implication is lower memory consumption and reduced training complexity relative to PPO while still preserving PPO’s core stability mechanisms, including the clipped surrogate objective and KL regularization.</p>
<p>In this blog, I will discuss and derive the GRPO objective step by step showing exactly how it simplifies PPO.</p>
<section id="i-the-ppo-objective-and-the-critic-problem" class="level2">
<h2 class="anchored" data-anchor-id="i-the-ppo-objective-and-the-critic-problem">I: The PPO Objective and the Critic Problem</h2>
<p>Let’s briefly recap the key relevant elements of PPO. For the full derivation and PPO details, see my <a href="https://aayushgarg.dev/posts/2025-12-25-deriving-ppo-loss.html">previous blog on PPO</a>.</p>
<p>PPO optimizes an LLM by maximizing a <strong>clipped surrogate objective</strong> (constrained using KL regularization):</p>
<p><span class="math display">\[
L^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta) \hat{A}_t, \; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot \hat{A}_t\right)\right]
\]</span></p>
<p>where <span class="math inline">\(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span> is the probability ratio between current and old policies.</p>
<p>The critical component here is the <strong>advantage estimate</strong> (<span class="math inline">\(\hat{A}_t\)</span>). The advantage measures how much better (or worse) a specific action is compared to what we expected:</p>
<p><span class="math display">\[
A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)
\]</span></p>
<p>To compute it, PPO uses a <strong>value function</strong> <span class="math inline">\(V^\pi(s)\)</span> (baseline) also called the <strong>critic</strong>) that predicts expected future rewards from any state. The critic is trained alongside the policy and PPO uses <strong>Generalized Advantage Estimation (GAE)</strong> to compute advantages from per-token value predictions.</p>
<section id="the-value-function-problem-in-ppo" class="level3">
<h3 class="anchored" data-anchor-id="the-value-function-problem-in-ppo">The Value Function Problem in PPO</h3>
<p>The value function is implemented as a <strong>learned critic model</strong> with the same architecture as the policy (i.e.&nbsp;another full LLM copy). This critic is trained alongside the policy using a regression loss:</p>
<p><span class="math display">\[
L^{\text{VF}}(\theta) = \mathbb{E}_t\left[\left(V_\theta(s_t) - V_t^{\text{target}}\right)^2\right]
\]</span></p>
<p>where <span class="math inline">\(V_t^{\text{target}}\)</span> is typically the discounted return-to-go from the sampled trajectory.</p>
<p>In PPO, we train two large neural networks (policy and critic) together rather than a single model. This substantially increases computational and memory overhead. Maintaining and training the critic alongside the policy not only increases memory consumption but also adds significant complexity to the training pipeline. In practice, PPO requires four models to be resident in memory at the same time: the policy, the critic, the reference model and the reward model.</p>
<p>One more issue with PPO is that GAE needs <strong>per-token rewards</strong> to compute Temporal Difference (TD) residuals at each position. But in LLM fine-tuning, we typically get <strong>outcome rewards</strong> which is a single score for the entire completion assigned only at the final token.</p>
<blockquote class="blockquote">
<p><em>From DeepSeekMath</em>: “During RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, <strong>usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token</strong>.”</p>
</blockquote>
<p>It raises a fundamental question: how can the critic learn accurate per-token values when all training signal comes from a single final reward?</p>
</section>
</section>
<section id="ii-replacing-the-critic-with-group-sampling" class="level2">
<h2 class="anchored" data-anchor-id="ii-replacing-the-critic-with-group-sampling">II: Replacing the Critic with Group Sampling</h2>
<p>As mentioned earlier, in PPO the value function <span class="math inline">\(V(s)\)</span> acts as a <strong>baseline <span class="math inline">\(b(s)\)</span></strong> for advantage estimation: <span class="math display">\[
\hat{A}_t = Q(s_t, a_t) - V(s_t).
\]</span> Subtracting this baseline reduces the variance of the policy gradient estimator which in turn stabilizes training.</p>
<blockquote class="blockquote">
<p><strong>Key insight:</strong> the value function is just <em>one possible</em> choice of baseline. In principle, <strong>any function <span class="math inline">\(b(s)\)</span> that depends only on the state and not on the action can be used without introducing bias into the gradient estimates.</strong></p>
</blockquote>
<p>Common baseline choices are:</p>
<ul>
<li><strong>Constant baseline:</strong> The average reward across samples. This is the simplest option and is used in vanilla REINFORCE.</li>
<li><strong>Learned value function:</strong> <span class="math inline">\(V(s)\)</span> trained alongside the policy as in PPO.</li>
<li><strong>Monte Carlo estimate:</strong> An empirical average of returns computed from multiple samples starting from the same state.</li>
</ul>
<p><strong>GRPO adopts the third approach. Instead of learning a value function, it directly estimates the expected return using multiple samples.</strong></p>
<section id="monte-carlo-baseline" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-baseline">Monte Carlo Baseline</h3>
<p>For each prompt <span class="math inline">\(q\)</span>, GRPO samples <strong>multiple completions</strong> <span class="math inline">\(\{o_1, o_2, \ldots, o_G\}\)</span> from the policy and obtains their rewards <span class="math inline">\(\{r_1, r_2, \ldots, r_G\}\)</span>. The average reward across these completions provides a Monte Carlo estimate of the expected return:</p>
<p><span class="math display">\[
b(q) = \frac{1}{G}\sum_{i=1}^G r_i \approx \mathbb{E}_{o \sim \pi_\theta(o|q)}[r(q, o)]
\]</span></p>
<p>This is a natural and unbiased estimator. With enough samples it converges to the true expected reward for that prompt, <strong>similar to what a well-trained value function would predict and can eliminates the need to train a separate critic model</strong>.</p>
</section>
<section id="group-relative-advantage" class="level3">
<h3 class="anchored" data-anchor-id="group-relative-advantage">Group-Relative Advantage</h3>
<p>Using the average reward as a baseline, the advantage for completion <span class="math inline">\(i\)</span> becomes:</p>
<p><span class="math display">\[
\hat{A}_i = r_i - \frac{1}{G}\sum_{j=1}^G r_j = r_i - \text{mean}(r_1, \ldots, r_G)
\]</span></p>
<p>GRPO normalizes the advantage by the standard deviation of rewards in the group:</p>
<p><span class="math display">\[
\hat{A}_i = \frac{r_i - \text{mean}(r_1, \ldots, r_G)}{\text{std}(r_1, \ldots, r_G)} \tag{II.I}
\]</span></p>
<p>This normalization ensures that advantages are on a comparable scale regardless of the prompt’s inherent difficulty.</p>
<blockquote class="blockquote">
<p>Think it this way: different prompts can have vastly different reward scales. For example, a simple arithmetic question might yield rewards clustered around 0.9 while a challenging proof might have rewards spread across say 0.1-0.9. Without normalization, the policy gradient updates would be dominated by high-variance prompts which can possibly destabilizing training.</p>
</blockquote>
<p>GRPO’s group-relative advantage mirrors the comparative nature of rewards models as we are asking “<em>how good is this completion relative to other completions for the same prompt</em>?”.</p>
</section>
</section>
<section id="iii-the-grpo-objective" class="level2">
<h2 class="anchored" data-anchor-id="iii-the-grpo-objective">III: The GRPO Objective</h2>
<p>We now have all the pieces needed to construct the full GRPO objective. The construction follows three key modifications:</p>
<p><strong>1. Start with PPO’s clipped surrogate.</strong> Recall from Section I that PPO optimizes:</p>
<p><span class="math display">\[
L^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta) \hat{A}_t, \; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot \hat{A}_t\right)\right]
\]</span></p>
<p>Here <span class="math inline">\(clip(.)\)</span> is <span class="math inline">\(\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\)</span> and this clipping mechanism provides a <strong>soft trust region</strong> that prevents destructively large policy updates.</p>
<p><strong>2. Replace GAE advantage with group-relative advantage.</strong> Instead of computing <span class="math inline">\(\hat{A}_t\)</span> using a learned critic and GAE, we substitute the group-relative advantage from Section II:</p>
<p><span class="math display">\[
\hat{A}_i = \frac{r_i - \text{mean}(r_1, \ldots, r_G)}{\text{std}(r_1, \ldots, r_G)}
\]</span></p>
<p>This is the <strong>key simplification</strong>. &gt; We no longer need per-token value predictions. Instead we estimate the baseline directly from sampled completions.</p>
<p><strong>3. Move KL penalty from reward to loss.</strong> In PPO, the KL penalty is typically subtracted from the reward signal (<span class="math inline">\(r_t\)</span>) before computing advantages:</p>
<p><span class="math display">\[
\tilde{r}_t = r_t - \beta \cdot \log \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{ref}}(a_t|s_t)}
\]</span></p>
<p>GRPO takes a different approach by adding the KL divergence directly as a penalty term in the loss function. This is a design choice that simplifies advantage computation since we dont need to consider KL penalties in the baseline estimation.</p>
<blockquote class="blockquote">
<p><em>From DeepSeekMath</em>: “Also note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of <span class="math inline">\(\hat{A}_i\)</span>.”</p>
</blockquote>
<section id="the-full-grpo-objective" class="level3">
<h3 class="anchored" data-anchor-id="the-full-grpo-objective">The Full GRPO Objective</h3>
<p>Combining these modifications, the GRPO objective (to be maximized) is:</p>
<p><span class="math display">\[
J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim \mathcal{D},\, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(o|q)}\left[\frac{1}{G}\sum_{i=1}^G \left( \min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}\hat{A}_i, \; \text{clip}(\cdot) \cdot \hat{A}_i\right) - \beta \, D_{\text{KL}}\left(\pi_\theta \| \pi_{\text{ref}}\right) \right)\right]
\]</span></p>
<p>where: - <span class="math inline">\(q\)</span> is a prompt sampled from the training distribution <span class="math inline">\(\mathcal{D}\)</span> - <span class="math inline">\(\{o_1, o_2, \ldots, o_G\}\)</span> are <span class="math inline">\(G\)</span> completions sampled from the old policy <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> - <span class="math inline">\(\hat{A}_i\)</span> is the group-relative advantage for completion <span class="math inline">\(i\)</span> (from II.I) - <span class="math inline">\(\beta\)</span> is the KL penalty coefficient - <span class="math inline">\(\pi_{\text{ref}}\)</span> is the frozen reference model (typically the SFT checkpoint)</p>
<p>The objective averages over all completions in the group, treating each completion equally in the policy update.</p>
<p>For implementation, we expand the sequence-level objective over individual tokens. Since autoregressive models factor the probability of a completion as a product of token probabilities:</p>
<p><span class="math display">\[
\pi_\theta(o_i|q) = \prod_{t=1}^{|o_i|} \pi_\theta(o_{i,t}|q, o_{i,&lt;t})
\]</span></p>
<p>The per-token formulation of GRPO becomes:</p>
<p><span class="math display">\[
J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim \mathcal{D},\, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(o|q)} \left[\frac{1}{G}\sum_{i=1}^G \frac{1}{|o_i|}\sum_{t=1}^{|o_i|} \left( \min\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,&lt;t})}\hat{A}_{i,t}, \; \text{clip}(\cdot) \cdot \hat{A}_{i,t}\right) - \beta \, D_{\text{KL}}^{(t)} \right)\right]
\]</span></p>
<p>where <span class="math inline">\(D_{\text{KL}}^{(t)}\)</span> is the per-token KL divergence between the current policy and the reference model.</p>
<p><strong>Note, all tokens in completion <span class="math inline">\(i\)</span> receive the same advantage:</strong></p>
<p><span class="math display">\[
\hat{A}_{i,t} = \hat{A}_i \quad \forall \, t \in \{1, 2, \ldots, |o_i|\}
\]</span></p>
<p>This is a deliberate simplification. Since we only receive a single reward for the entire completion trying to learn which specific tokens were “good” or “bad” can be difficult.</p>
<blockquote class="blockquote">
<p><em>From DeepSeekMath</em>: “While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token.”</p>
</blockquote>
</section>
<section id="single-gradient-step-simplification" class="level3">
<h3 class="anchored" data-anchor-id="single-gradient-step-simplification">Single Gradient Step Simplification</h3>
<p>In practice (as mentioned in RLHF Book), GRPO is often run with only <strong>one gradient step per batch</strong> of sampled data. In this case <span class="math inline">\(\pi_\theta = \pi_{\theta_{\text{old}}}\)</span> at the start of the update which means the policy ratio equals 1 and the clipping mechanism has no effect:</p>
<p><span class="math display">\[
r_t(\theta) = \frac{\pi_\theta(o_{i,t}|q, o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,&lt;t})} = 1
\]</span></p>
<p>The objective then simplifies to a weighted policy gradient:</p>
<p><span class="math display">\[
J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim \mathcal{D},\, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(o|q)} \left[\frac{1}{G}\sum_{i=1}^G \frac{1}{|o_i|}\sum_{t=1}^{|o_i|} \left( \hat{A}_i \cdot \log \pi_\theta(o_{i,t}|q, o_{i,&lt;t}) - \beta \, D_{\text{KL}}^{(t)} \right)\right]
\]</span></p>
</section>
</section>
<section id="iv-kl-divergence-in-grpo" class="level2">
<h2 class="anchored" data-anchor-id="iv-kl-divergence-in-grpo">IV: KL Divergence in GRPO</h2>
<p>The KL Divergence is a measure of the difference between two probability distributions. It is defined as:</p>
<p><span class="math display">\[
D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) = \mathbb{E}_{x \sim \pi_\theta}\left[\log \frac{\pi_\theta(x)}{\pi_{\text{ref}}(x)}\right]
\]</span></p>
<p>It can simply be estimated as:</p>
<p><span class="math display">\[
D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) \approx \log \pi_\theta(x) - \log \pi_{\text{ref}}(x)
\]</span></p>
<p>However this can be <strong>negative</strong> for individual samples (when <span class="math inline">\(\pi_\theta &lt; \pi_{\text{ref}}\)</span>) even though KL divergence is always non-negative. This may lead to high variance in gradient estimates.</p>
<p>GRPO uses an alternative estimator that is both <strong>unbiased</strong> and <strong>guaranteed non-negative</strong>:</p>
<p><span class="math display">\[
D_{\text{KL}}^{(t)} = \frac{\pi_{\text{ref}}(o_{i,t}|q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}|q, o_{i,&lt;t})} - \log \frac{\pi_{\text{ref}}(o_{i,t}|q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}|q, o_{i,&lt;t})} - 1 \tag{IV.I}
\]</span></p>
<p>This estimator can be understood as measuring the gap between <span class="math inline">\(\log(x)\)</span> and its tangent line at <span class="math inline">\(x=1\)</span>. Since <span class="math inline">\(\log\)</span> is concave. This gap is always non-negative ensuring the KL penalty never incorrectly suggests that diverging from the reference <em>decreases</em> the penalty.</p>
<blockquote class="blockquote">
<p>For a detailed derivation of why this estimator is unbiased and non-negative, see <a href="http://joschu.net/blog/kl-approx.html">John Schulman’s excellent blog post on approximating KL divergence</a>.</p>
</blockquote>
</section>
<section id="v-outcome-vs.-process-supervision" class="level2">
<h2 class="anchored" data-anchor-id="v-outcome-vs.-process-supervision">V: Outcome vs.&nbsp;Process Supervision</h2>
<p>The GRPO formulation presented so far assumes <strong>outcome supervision</strong> which provides a single reward at the end of each completion with the same advantage assigned to every token. However for complex reasoning tasks knowing only the final answer reward might not be sufficient.</p>
<blockquote class="blockquote">
<p><em>From DeepSeekMath Paper</em>: “Outcome supervision only provides a reward at the end of each output, which may not be sufficient and efficient to supervise the policy in complex mathematical tasks.”</p>
</blockquote>
<p><strong>Process supervision</strong> addresses this by providing rewards at the end of each reasoning step. Given a completion <span class="math inline">\(o_i\)</span> with <span class="math inline">\(K_i\)</span> reasoning steps, a <strong>process reward model (PRM)</strong> assigns rewards <span class="math inline">\(\{r_i^{\text{index}(1)}, \ldots, r_i^{\text{index}(K_i)}\}\)</span> at step boundaries where <span class="math inline">\(\text{index}(j)\)</span> is the end token index of the <span class="math inline">\(j\)</span>-th step.</p>
<p>GRPO extends to process supervision with two modifications:</p>
<p><strong>1. Normalize across all step rewards in the group:</strong></p>
<p><span class="math display">\[
\tilde{r}_i^{\text{index}(j)} = \frac{r_i^{\text{index}(j)} - \text{mean}(\mathcal{R})}{\text{std}(\mathcal{R})}
\]</span></p>
<p>where <span class="math inline">\(\mathcal{R}\)</span> contains all step rewards across all <span class="math inline">\(G\)</span> completions.</p>
<p><strong>2. Compute advantages as cumulative future rewards:</strong></p>
<p><span class="math display">\[
\hat{A}_{i,t} = \sum_{\text{index}(j) \geq t} \tilde{r}_i^{\text{index}(j)} \tag{V.I}
\]</span></p>
<p>This mirrors <strong>return-to-go</strong> in traditional RL where earlier tokens accumulate rewards from all subsequent steps, while tokens near the end see only remaining rewards.</p>
<blockquote class="blockquote">
<p>The DeepSeekMath experiments found process supervision can accelerate learning, though the gap narrows with iterative training. For domains with reliable verifiers (code execution, math answer checking), outcome supervision with RLVR has become dominant. DeepSeek-R1 uses only outcome-level verification.</p>
</blockquote>
</section>
<section id="vi-connection-to-reinforce-leave-one-out-rloo" class="level2">
<h2 class="anchored" data-anchor-id="vi-connection-to-reinforce-leave-one-out-rloo">VI: Connection to REINFORCE Leave-One-Out (RLOO)</h2>
<p>GRPO is not the only critic-free algorithm leveraging group sampling. <a href="https://arxiv.org/html/2402.14740v1">REINFORCE Leave-One-Out (RLOO)</a> takes a similar approach but computes the baseline as the mean reward over all <em>other</em> completions, <strong>excluding</strong> the current sample:</p>
<p><span class="math display">\[
A_i^{\text{RLOO}} = r_i - \frac{1}{G-1}\sum_{j=1, j \neq i}^{G} r_j \tag{VI.I}
\]</span></p>
<p>This “leave-one-out” baseline avoids a subtle correlation that exists when the baseline includes the sample being evaluated.</p>
<p>The two algorithms are conceptually very similar. However, there are some key differences:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Aspect</th>
<th>RLOO</th>
<th>GRPO</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Baseline</strong></td>
<td>Mean of <em>other</em> samples</td>
<td>Mean of <em>all</em> samples</td>
</tr>
<tr class="even">
<td><strong>Normalization</strong></td>
<td>None</td>
<td>Divide by std</td>
</tr>
<tr class="odd">
<td><strong>Clipping</strong></td>
<td>No</td>
<td>Yes (PPO-style)</td>
</tr>
<tr class="even">
<td><strong>KL Placement</strong></td>
<td>In reward</td>
<td>In loss</td>
</tr>
</tbody>
</table>
<p><strong>GRPO can be understood as inheriting PPO’s clipping mechanism for stability while adopting RLOO-style group sampling to eliminate the critic.</strong></p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>GRPO ingenuity comes from recognizing that PPO value function is fundamentally just a baseline for advantage computation and that an stimate obtained via group sampling can serve the same role. By sampling multiple completions per prompt and using their mean reward as the baseline, GRPO achieves the stability provided by PPO clipped surrogate objective without the memory overhead or training complexity of training a separate critic model.</p>
<p>This design choice is what makes GRPO a preferred approach for RLVR training of LLMs focused on reasoning capabilities.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p><strong>Papers:</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a>: The original GRPO paper</li>
<li><a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a>: DeepSeek’s reasoning model trained with GRPO</li>
<li><a href="https://arxiv.org/abs/2402.14740">Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs</a>: The RLOO paper comparing critic-free methods</li>
</ul>
<p><strong>Blogs:</strong></p>
<ul>
<li><a href="http://joschu.net/blog/kl-approx.html">John Schulman’s post on approximating KL divergence</a>: Derivation of the unbiased non-negative KL estimator</li>
<li><a href="https://rlhfbook.com/">RLHF Book by Nathan Lambert</a>: Comprehensive resource on RLHF algorithms including GRPO implementation details</li>
<li><a href="https://substack.com/home/post/p-177823868">Understanding GRPO by Cameron R. Wolfe</a>: Deep dive into GRPO mechanics and implementation</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/garg-aayush\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/garg-aayush">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aayush-garg-8b26a734">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Aayush_ander">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>