[
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html",
    "href": "posts/2026-01-01-understanding-grpo.html",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "",
    "text": "In my previous posts, I worked through the derivations of PPO and DPO for LLM post-training. PPO gave us a full-fledged RL approach with clipped surrogate objectives, value functions and GAE-based advantage estimation. DPO on the other hand, showed a clever way to bypass RL entirely by reformulating the optimization as a simple classification loss on preference pairs.\nThat brings us to Group Relative Policy Optimization (GRPO), introduced in the DeepSeekMath paper. If you have been following recent developments in reasoning models throughout 2025, GRPO has become one of the most widely used post-training algorithms behind open-source reasoning models.\nIn simple terms, GRPO can be thought of as PPO without the critic (value function). Recall that PPO trains a value function in addition to the policy to estimate baselines for advantage computation. GRPO takes a simpler approach where it samples multiple completions (‚Äúgroup‚Äù) for each prompt and uses their rewards to form a baseline for advantage computation. This group-derived baseline replaces the learned value function entirely (no need to train a critic!).\nThe practical implication is lower memory consumption and reduced training complexity relative to PPO while still preserving PPO‚Äôs core stability mechanisms, including the clipped surrogate objective and KL regularization.\nIn this blog, I will discuss and derive the GRPO objective step by step showing exactly how it simplifies PPO."
  },
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html#i-the-ppo-objective-and-the-critic-problem",
    "href": "posts/2026-01-01-understanding-grpo.html#i-the-ppo-objective-and-the-critic-problem",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "I: The PPO Objective and the Critic Problem",
    "text": "I: The PPO Objective and the Critic Problem\nLet‚Äôs briefly recap the key relevant elements of PPO. For the full derivation and PPO details, see my previous blog on PPO.\nPPO optimizes an LLM by maximizing a clipped surrogate objective (constrained using KL regularization):\n\\[\nL^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\hat{A}_t, \\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\cdot \\hat{A}_t\\right)\\right]\n\\]\nwhere \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\) is the probability ratio between current and old policies.\nThe critical component here is the advantage estimate (\\(\\hat{A}_t\\)). The advantage measures how much better (or worse) a specific action is compared to what we expected:\n\\[\nA^\\pi(s_t, a_t) = Q^\\pi(s_t, a_t) - V^\\pi(s_t)\n\\]\nTo compute it, PPO uses a value function \\(V^\\pi(s)\\) (baseline) also called the critic) that predicts expected future rewards from any state. The critic is trained alongside the policy and PPO uses Generalized Advantage Estimation (GAE) to compute advantages from per-token value predictions.\n\nThe Value Function Problem in PPO\nThe value function is implemented as a learned critic model with the same architecture as the policy (i.e.¬†another full LLM copy). This critic is trained alongside the policy using a regression loss:\n\\[\nL^{\\text{VF}}(\\theta) = \\mathbb{E}_t\\left[\\left(V_\\theta(s_t) - V_t^{\\text{target}}\\right)^2\\right]\n\\]\nwhere \\(V_t^{\\text{target}}\\) is typically the discounted return-to-go from the sampled trajectory.\nIn PPO, we train two large neural networks (policy and critic) together rather than a single model. This substantially increases computational and memory overhead. Maintaining and training the critic alongside the policy not only increases memory consumption but also adds significant complexity to the training pipeline. In practice, PPO requires four models to be resident in memory at the same time: the policy, the critic, the reference model and the reward model.\nOne more issue with PPO is that GAE needs per-token rewards to compute Temporal Difference (TD) residuals at each position. But in LLM fine-tuning, we typically get outcome rewards which is a single score for the entire completion assigned only at the final token.\n\nFrom DeepSeekMath: ‚ÄúDuring RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token.‚Äù\n\nIt raises a fundamental question: how can the critic learn accurate per-token values when all training signal comes from a single final reward?"
  },
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html#ii-replacing-the-critic-with-group-sampling",
    "href": "posts/2026-01-01-understanding-grpo.html#ii-replacing-the-critic-with-group-sampling",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "II: Replacing the Critic with Group Sampling",
    "text": "II: Replacing the Critic with Group Sampling\nAs mentioned earlier, in PPO the value function \\(V(s)\\) acts as a baseline \\(b(s)\\) for advantage estimation: \\[\n\\hat{A}_t = Q(s_t, a_t) - V(s_t).\n\\] Subtracting this baseline reduces the variance of the policy gradient estimator which in turn stabilizes training.\n\nKey insight: the value function is just one possible choice of baseline. In principle, any function \\(b(s)\\) that depends only on the state and not on the action can be used without introducing bias into the gradient estimates.\n\nCommon baseline choices are:\n\nConstant baseline: The average reward across samples. This is the simplest option and is used in vanilla REINFORCE.\nLearned value function: \\(V(s)\\) trained alongside the policy as in PPO.\nMonte Carlo estimate: An empirical average of returns computed from multiple samples starting from the same state.\n\nGRPO adopts the third approach. Instead of learning a value function, it directly estimates the expected return using multiple samples.\n\nMonte Carlo Baseline\nFor each prompt \\(q\\), GRPO samples multiple completions \\(\\{o_1, o_2, \\ldots, o_G\\}\\) from the policy and obtains their rewards \\(\\{r_1, r_2, \\ldots, r_G\\}\\). The average reward across these completions provides a Monte Carlo estimate of the expected return:\n\\[\nb(q) = \\frac{1}{G}\\sum_{i=1}^G r_i \\approx \\mathbb{E}_{o \\sim \\pi_\\theta(o|q)}[r(q, o)]\n\\]\nThis is a natural and unbiased estimator. With enough samples it converges to the true expected reward for that prompt, similar to what a well-trained value function would predict and can eliminates the need to train a separate critic model.\n\n\nGroup-Relative Advantage\nUsing the average reward as a baseline, the advantage for completion \\(i\\) becomes:\n\\[\n\\hat{A}_i = r_i - \\frac{1}{G}\\sum_{j=1}^G r_j = r_i - \\text{mean}(r_1, \\ldots, r_G)\n\\]\nGRPO normalizes the advantage by the standard deviation of rewards in the group:\n\\[\n\\hat{A}_i = \\frac{r_i - \\text{mean}(r_1, \\ldots, r_G)}{\\text{std}(r_1, \\ldots, r_G)} \\tag{II.I}\n\\]\nThis normalization ensures that advantages are on a comparable scale regardless of the prompt‚Äôs inherent difficulty.\n\nThink it this way: different prompts can have vastly different reward scales. For example, a simple arithmetic question might yield rewards clustered around 0.9 while a challenging proof might have rewards spread across say 0.1-0.9. Without normalization, the policy gradient updates would be dominated by high-variance prompts which can possibly destabilizing training.\n\nGRPO‚Äôs group-relative advantage mirrors the comparative nature of rewards models as we are asking ‚Äúhow good is this completion relative to other completions for the same prompt?‚Äù."
  },
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html#iii-the-grpo-objective",
    "href": "posts/2026-01-01-understanding-grpo.html#iii-the-grpo-objective",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "III: The GRPO Objective",
    "text": "III: The GRPO Objective\nWe now have all the pieces needed to construct the full GRPO objective. The construction follows three key modifications:\n1. Start with PPO‚Äôs clipped surrogate. Recall from Section I that PPO optimizes:\n\\[\nL^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\hat{A}_t, \\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\cdot \\hat{A}_t\\right)\\right]\n\\]\nHere \\(clip(.)\\) is \\(\\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\) and this clipping mechanism provides a soft trust region that prevents destructively large policy updates.\n2. Replace GAE advantage with group-relative advantage. Instead of computing \\(\\hat{A}_t\\) using a learned critic and GAE, we substitute the group-relative advantage from Section II:\n\\[\n\\hat{A}_i = \\frac{r_i - \\text{mean}(r_1, \\ldots, r_G)}{\\text{std}(r_1, \\ldots, r_G)}\n\\]\nThis is the key simplification. &gt; We no longer need per-token value predictions. Instead we estimate the baseline directly from sampled completions.\n3. Move KL penalty from reward to loss. In PPO, the KL penalty is typically subtracted from the reward signal (\\(r_t\\)) before computing advantages:\n\\[\n\\tilde{r}_t = r_t - \\beta \\cdot \\log \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\text{ref}}(a_t|s_t)}\n\\]\nGRPO takes a different approach by adding the KL divergence directly as a penalty term in the loss function. This is a design choice that simplifies advantage computation since we dont need to consider KL penalties in the baseline estimation.\n\nFrom DeepSeekMath: ‚ÄúAlso note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of \\(\\hat{A}_i\\).‚Äù\n\n\nThe Full GRPO Objective\nCombining these modifications, the GRPO objective (to be maximized) is:\n\\[\nJ_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{q \\sim \\mathcal{D},\\, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o|q)}\\left[\\frac{1}{G}\\sum_{i=1}^G \\left( \\min\\left(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}\\hat{A}_i, \\; \\text{clip}(\\cdot) \\cdot \\hat{A}_i\\right) - \\beta \\, D_{\\text{KL}}\\left(\\pi_\\theta \\| \\pi_{\\text{ref}}\\right) \\right)\\right]\n\\]\nwhere: - \\(q\\) is a prompt sampled from the training distribution \\(\\mathcal{D}\\) - \\(\\{o_1, o_2, \\ldots, o_G\\}\\) are \\(G\\) completions sampled from the old policy \\(\\pi_{\\theta_{\\text{old}}}\\) - \\(\\hat{A}_i\\) is the group-relative advantage for completion \\(i\\) (from II.I) - \\(\\beta\\) is the KL penalty coefficient - \\(\\pi_{\\text{ref}}\\) is the frozen reference model (typically the SFT checkpoint)\nThe objective averages over all completions in the group, treating each completion equally in the policy update.\nFor implementation, we expand the sequence-level objective over individual tokens. Since autoregressive models factor the probability of a completion as a product of token probabilities:\n\\[\n\\pi_\\theta(o_i|q) = \\prod_{t=1}^{|o_i|} \\pi_\\theta(o_{i,t}|q, o_{i,&lt;t})\n\\]\nThe per-token formulation of GRPO becomes:\n\\[\nJ_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{q \\sim \\mathcal{D},\\, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o|q)} \\left[\\frac{1}{G}\\sum_{i=1}^G \\frac{1}{|o_i|}\\sum_{t=1}^{|o_i|} \\left( \\min\\left(\\frac{\\pi_\\theta(o_{i,t}|q, o_{i,&lt;t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,&lt;t})}\\hat{A}_{i,t}, \\; \\text{clip}(\\cdot) \\cdot \\hat{A}_{i,t}\\right) - \\beta \\, D_{\\text{KL}}^{(t)} \\right)\\right]\n\\]\nwhere \\(D_{\\text{KL}}^{(t)}\\) is the per-token KL divergence between the current policy and the reference model.\nNote, all tokens in completion \\(i\\) receive the same advantage:\n\\[\n\\hat{A}_{i,t} = \\hat{A}_i \\quad \\forall \\, t \\in \\{1, 2, \\ldots, |o_i|\\}\n\\]\nThis is a deliberate simplification. Since we only receive a single reward for the entire completion trying to learn which specific tokens were ‚Äúgood‚Äù or ‚Äúbad‚Äù can be difficult.\n\nFrom DeepSeekMath: ‚ÄúWhile in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token.‚Äù\n\n\n\nSingle Gradient Step Simplification\nIn practice (as mentioned in RLHF Book), GRPO is often run with only one gradient step per batch of sampled data. In this case \\(\\pi_\\theta = \\pi_{\\theta_{\\text{old}}}\\) at the start of the update which means the policy ratio equals 1 and the clipping mechanism has no effect:\n\\[\nr_t(\\theta) = \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,&lt;t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,&lt;t})} = 1\n\\]\nThe objective then simplifies to a weighted policy gradient:\n\\[\nJ_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{q \\sim \\mathcal{D},\\, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o|q)} \\left[\\frac{1}{G}\\sum_{i=1}^G \\frac{1}{|o_i|}\\sum_{t=1}^{|o_i|} \\left( \\hat{A}_i \\cdot \\log \\pi_\\theta(o_{i,t}|q, o_{i,&lt;t}) - \\beta \\, D_{\\text{KL}}^{(t)} \\right)\\right]\n\\]"
  },
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html#iv-kl-divergence-in-grpo",
    "href": "posts/2026-01-01-understanding-grpo.html#iv-kl-divergence-in-grpo",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "IV: KL Divergence in GRPO",
    "text": "IV: KL Divergence in GRPO\nThe KL Divergence is a measure of the difference between two probability distributions. It is defined as:\n\\[\nD_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\mathbb{E}_{x \\sim \\pi_\\theta}\\left[\\log \\frac{\\pi_\\theta(x)}{\\pi_{\\text{ref}}(x)}\\right]\n\\]\nIt can simply be estimated as:\n\\[\nD_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}}) \\approx \\log \\pi_\\theta(x) - \\log \\pi_{\\text{ref}}(x)\n\\]\nHowever this can be negative for individual samples (when \\(\\pi_\\theta &lt; \\pi_{\\text{ref}}\\)) even though KL divergence is always non-negative. This may lead to high variance in gradient estimates.\nGRPO uses an alternative estimator that is both unbiased and guaranteed non-negative:\n\\[\nD_{\\text{KL}}^{(t)} = \\frac{\\pi_{\\text{ref}}(o_{i,t}|q, o_{i,&lt;t})}{\\pi_\\theta(o_{i,t}|q, o_{i,&lt;t})} - \\log \\frac{\\pi_{\\text{ref}}(o_{i,t}|q, o_{i,&lt;t})}{\\pi_\\theta(o_{i,t}|q, o_{i,&lt;t})} - 1 \\tag{IV.I}\n\\]\nThis estimator can be understood as measuring the gap between \\(\\log(x)\\) and its tangent line at \\(x=1\\). Since \\(\\log\\) is concave. This gap is always non-negative ensuring the KL penalty never incorrectly suggests that diverging from the reference decreases the penalty.\n\nFor a detailed derivation of why this estimator is unbiased and non-negative, see John Schulman‚Äôs excellent blog post on approximating KL divergence."
  },
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html#v-outcome-vs.-process-supervision",
    "href": "posts/2026-01-01-understanding-grpo.html#v-outcome-vs.-process-supervision",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "V: Outcome vs.¬†Process Supervision",
    "text": "V: Outcome vs.¬†Process Supervision\nThe GRPO formulation presented so far assumes outcome supervision which provides a single reward at the end of each completion with the same advantage assigned to every token. However for complex reasoning tasks knowing only the final answer reward might not be sufficient.\n\nFrom DeepSeekMath Paper: ‚ÄúOutcome supervision only provides a reward at the end of each output, which may not be sufficient and efficient to supervise the policy in complex mathematical tasks.‚Äù\n\nProcess supervision addresses this by providing rewards at the end of each reasoning step. Given a completion \\(o_i\\) with \\(K_i\\) reasoning steps, a process reward model (PRM) assigns rewards \\(\\{r_i^{\\text{index}(1)}, \\ldots, r_i^{\\text{index}(K_i)}\\}\\) at step boundaries where \\(\\text{index}(j)\\) is the end token index of the \\(j\\)-th step.\nGRPO extends to process supervision with two modifications:\n1. Normalize across all step rewards in the group:\n\\[\n\\tilde{r}_i^{\\text{index}(j)} = \\frac{r_i^{\\text{index}(j)} - \\text{mean}(\\mathcal{R})}{\\text{std}(\\mathcal{R})}\n\\]\nwhere \\(\\mathcal{R}\\) contains all step rewards across all \\(G\\) completions.\n2. Compute advantages as cumulative future rewards:\n\\[\n\\hat{A}_{i,t} = \\sum_{\\text{index}(j) \\geq t} \\tilde{r}_i^{\\text{index}(j)} \\tag{V.I}\n\\]\nThis mirrors return-to-go in traditional RL where earlier tokens accumulate rewards from all subsequent steps, while tokens near the end see only remaining rewards.\n\nThe DeepSeekMath experiments found process supervision can accelerate learning, though the gap narrows with iterative training. For domains with reliable verifiers (code execution, math answer checking), outcome supervision with RLVR has become dominant. DeepSeek-R1 uses only outcome-level verification."
  },
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html#vi-connection-to-reinforce-leave-one-out-rloo",
    "href": "posts/2026-01-01-understanding-grpo.html#vi-connection-to-reinforce-leave-one-out-rloo",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "VI: Connection to REINFORCE Leave-One-Out (RLOO)",
    "text": "VI: Connection to REINFORCE Leave-One-Out (RLOO)\nGRPO is not the only critic-free algorithm leveraging group sampling. REINFORCE Leave-One-Out (RLOO) takes a similar approach but computes the baseline as the mean reward over all other completions, excluding the current sample:\n\\[\nA_i^{\\text{RLOO}} = r_i - \\frac{1}{G-1}\\sum_{j=1, j \\neq i}^{G} r_j \\tag{VI.I}\n\\]\nThis ‚Äúleave-one-out‚Äù baseline avoids a subtle correlation that exists when the baseline includes the sample being evaluated.\nThe two algorithms are conceptually very similar. However, there are some key differences:\n\n\n\nAspect\nRLOO\nGRPO\n\n\n\n\nBaseline\nMean of other samples\nMean of all samples\n\n\nNormalization\nNone\nDivide by std\n\n\nClipping\nNo\nYes (PPO-style)\n\n\nKL Placement\nIn reward\nIn loss\n\n\n\nGRPO can be understood as inheriting PPO‚Äôs clipping mechanism for stability while adopting RLOO-style group sampling to eliminate the critic."
  },
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html#conclusion",
    "href": "posts/2026-01-01-understanding-grpo.html#conclusion",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "Conclusion",
    "text": "Conclusion\nGRPO ingenuity comes from recognizing that PPO value function is fundamentally just a baseline for advantage computation and that an stimate obtained via group sampling can serve the same role. By sampling multiple completions per prompt and using their mean reward as the baseline, GRPO achieves the stability provided by PPO clipped surrogate objective without the memory overhead or training complexity of training a separate critic model.\nThis design choice is what makes GRPO a preferred approach for RLVR training of LLMs focused on reasoning capabilities."
  },
  {
    "objectID": "posts/2026-01-01-understanding-grpo.html#references",
    "href": "posts/2026-01-01-understanding-grpo.html#references",
    "title": "Understanding GRPO: PPO without the Critic",
    "section": "References",
    "text": "References\nPapers:\n\nDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models: The original GRPO paper\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning: DeepSeek‚Äôs reasoning model trained with GRPO\nBack to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs: The RLOO paper comparing critic-free methods\n\nBlogs:\n\nJohn Schulman‚Äôs post on approximating KL divergence: Derivation of the unbiased non-negative KL estimator\nRLHF Book by Nathan Lambert: Comprehensive resource on RLHF algorithms including GRPO implementation details\nUnderstanding GRPO by Cameron R. Wolfe: Deep dive into GRPO mechanics and implementation"
  },
  {
    "objectID": "posts/2025-12-28-sft-from-scratch.html",
    "href": "posts/2025-12-28-sft-from-scratch.html",
    "title": "What I Learned Building SFT from the Ground Up",
    "section": "",
    "text": "Over the past few weeks, I implemented supervised fine-tuning (SFT) from scratch, continuing a series of projects where I‚Äôm building foundational LLM components as a learning exercise from the ground up. Previously, I‚Äôve worked through implementing GPT-2 from scratch and writing LLM inference scripts from the ground up. Naturally, SFT was the next step in this series.\nOne thing I realized pretty quickly, writing the training scripts from scratch is not the most difficult part. However, making it actually work, producing results that seems reasonable is where the real challenge begins üòÖ. You run into all sorts of difficulties: debugging annoying errors, dealing with gradient instabilities, getting vLLM to cooperate for intermediate evaluation (especially with limited GPU memory) etc. These are the things that eat up your time but teach you the most.\nIn this post, I want to share not just what I built, but the building and debugging journey that got me there."
  },
  {
    "objectID": "posts/2025-12-28-sft-from-scratch.html#what-i-built",
    "href": "posts/2025-12-28-sft-from-scratch.html#what-i-built",
    "title": "What I Learned Building SFT from the Ground Up",
    "section": "What I Built",
    "text": "What I Built\nI loosely followed Stanford‚Äôs CS336 Assignment 5 as a guide, wrote all the SFT core components, and ran two sets of experiments:\n1. Reasoning SFT: Fine-tuned Qwen2.5-Math-1.5B on math reasoning traces to improve step-by-step problem solving capabilities.\n\n\n\nBest: 53.4% reward accuracy (up from 2.9% baseline) with 99.3% format accuracy\n\n\n2. Instruction SFT: Fine-tuned Llama-3.1-8B on UltraChat-200K + SafetyLlama for general instruction following and safety.\n\n\n\nBest: GSM8K 16-&gt;33%, Safety 62-&gt;78%, AlpacaEval 1.6-&gt;5.3%, MMLU ~58%\n\n\nAll experiment code, training scripts, and detailed notes are available in my building-from-scratch repo."
  },
  {
    "objectID": "posts/2025-12-28-sft-from-scratch.html#part-1-reasoning-sft-with-qwen2.5-math-1.5b",
    "href": "posts/2025-12-28-sft-from-scratch.html#part-1-reasoning-sft-with-qwen2.5-math-1.5b",
    "title": "What I Learned Building SFT from the Ground Up",
    "section": "Part 1: Reasoning SFT with Qwen2.5-Math-1.5B",
    "text": "Part 1: Reasoning SFT with Qwen2.5-Math-1.5B\nThe idea behind reasoning SFT is simple. You take a base model that barely outputs correct answers, show it high-quality examples of how to solve problems step-by-step, and train it to replicate/mimic that reasoning process. The model learns to think in a structured format with first generating reasoning inside &lt;think&gt; tags, then outputting the final answer in &lt;answer&gt; tags.\nMy starting point was Qwen2.5-Math-1.5B, which had quite poor baseline accuracies on the math validation set: ~2.9% for answers and ~14% for format.\n\nCreating the Dataset: First Challenge\nThe original CS336 MATH dataset used for SFT training is not publicly available, so I had to create my own. My dataset creation pipeline had three steps:\n\nSource problems: I used hiyouga/math12k dataset to create the training set, carefully filtering out any problems that appeared in the validation set to avoid data leakage.\nGenerate reasoning traces: The next and most important step is to generate the reasoning traces for each problem. I used gpt-oss-120b model to generate them via Fireworks Batch Inference API. It costed me around ~$4 to generate the reasoning traces.\nFilter for quality: I also created a subset of around ~3.6K examples by filtering out the reasoning traces that led to wrong answers.\n\n\n\nThe Training Loop: Per-Token vs.¬†Sequence Loss\nThe original assignment uses sequence level loss normalization where you sum the loss over all tokens in a sequence and normalize by a constant, not by the variable number of tokens.\nWhile running the initial experiments, I noticed the gradient norms were really large values, and training felt unstable. Even though the loss seemed to be going in the right direction, something didn‚Äôt feel right. After some investigation, I realized the issue: with variable-length sequences (my training examples ranged from short to quite long), longer sequences contribute more to the gradient than shorter ones. This creates high variance in gradient updates.\n\n\n\n\n\n\n\n\n\n\n\nLeft: Sequence-level loss (high variance gradients) | Right: Per-token loss (stable gradients)\n\nThus, I added a per_token_loss flag to my training step which when enabled normalizes the loss by the actual number of response tokens in each sequence. The difference was noticeable with subtle improved accuracy. More importantly, the gradients became much more stable with per-token normalization.\n\n\n\nRun\nLoss Normalization\nReward Accuracy\n\n\n\n\nrun_filtered\nPer-token\n0.5204\n\n\nrun_filtered-res-len\nSequence-level\n0.5106\n\n\n\n\n\nvLLM Integration: The Debugging Nightmare\nHere‚Äôs where things got really tricky and painful. I wanted to run intermediate evaluations during training using vLLM for fast inference. The assignment provided code for this but it was written for an older vLLM version and nothing worked out of the box üòÖ.\nProblem 1: vLLM initialization changed\nThe assignment‚Äôs approach used a separate GPU dedicated to running vLLM as an inference server. I wasn‚Äôt keen on this setup anyway as it meant paying for an extra GPU just for inference. But more importantly, the approach broke completely with the vLLM version I was using (0.7+). The initialization logic had changed, and the old code just wouldn‚Äôt run.\nSolution: I switched to the colocate approach, running vLLM on the same device as the training model. I came across this in the excellent HuggingFace blog post on co-located vLLM. Though, this required being more careful about GPU memory (setting appropriate values for gpu_memory_utilization, max_model_len, and max_num_seqs), but it actually works and saves on GPU costs.\nProblem 2: Missing model_executor attribute\nWhen I tried to load updated model weights into the vLLM instance during training, I hit this error:\nAttributeError: 'LLMEngine' object has no attribute 'model_executor'\nThis was really annoying because the attribute clearly existed in the vLLM source code. After much debugging, I found two solutions: - Downgrade to vLLM 0.10.2, or - If using vLLM 0.11.0, set the environment variable VLLM_ENABLE_V1_MULTIPROCESSING=0 at the start of the script\nI went with the environment variable approach since I didn‚Äôt want to deal with version conflicts.\nProblem 3: The _orig_mod issue\nWith torch.compile enabled on my model (for faster training), loading weights into vLLM failed with the below error. The issue is that torch.compile wraps the original model and stores the actual weights under _orig_mod. When loading weights into vLLM, you need to access them through this attribute, not directly from the compiled model.\nValueError: There is no module or parameter named '_orig_mod' in Qwen2ForCausalLM\nSolution: In my load_policy_into_vllm_instance function, I made sure to load from model._orig_mod when the model is compiled.\nThese three issues cost me almost a day. However, it was worth it because I learned a lot about vLLM and how to integrate it in training run\n\n\nResults\nAfter all that debugging, here‚Äôs what the training runs achieved:\n\n\n\nReasoning SFT Results\n\n\n\n\n\nRun\nTraining Data\nReward Accuracy\nFormat Accuracy\n\n\n\n\nbaseline\n-\n0.0288\n0.1438\n\n\nrun_all\nFull 4.8K (correct + incorrect)\n0.4214\n0.9924\n\n\nrun_filtered\nFiltered 3.6K (correct only)\n0.5204\n0.9906\n\n\nrun_filtered-2epoch\nFiltered 3.6K (2 epochs)\n0.5336\n0.9926\n\n\n\nKey takeaways: - Filtering out incorrect reasoning traces boosted accuracy from 42% to 52%. Training on wrong traces teaches the model wrong patterns. - The model quickly learned the output format (99%+ format accuracy after training). - Running for 2 epochs gave a boost in accuracy though a marginal one."
  },
  {
    "objectID": "posts/2025-12-28-sft-from-scratch.html#part-2-instruction-sft-with-llama-3.1-8b",
    "href": "posts/2025-12-28-sft-from-scratch.html#part-2-instruction-sft-with-llama-3.1-8b",
    "title": "What I Learned Building SFT from the Ground Up",
    "section": "Part 2: Instruction SFT with Llama-3.1-8B",
    "text": "Part 2: Instruction SFT with Llama-3.1-8B\nWith the reasoning SFT working, I moved on to the second part: instruction fine-tuning. This loosely follows the CS336 Supplementary Assignment 5, where the goal is to build a model that can follow diverse instructions and refuse harmful requests.\nUnlike reasoning SFT, instruction fine-tuning uses conversational instruction-response pairs. The training data combines UltraChat-200K (diverse multi-turn conversations) and SafetyLlama (safety-focused examples) totaling around 200K examples, formatted using the Alpaca prompt template.\nFor evaluation, I used four benchmarks as specified in the assignment: - GSM8K: Grade-school math problems (tests math reasoning) - MMLU: Multiple-choice questions across 57 subjects (tests factual knowledge) - AlpacaEval: Open-ended instructions judged by LLM-as-judge (tests instruction-following quality)\n- Simple Safety Tests (SST): Harmful prompts to test refusal behavior (tests safety)\n\nThe Prompt Masking Implementation Problem\nI wanted to experiment with prompt masking i.e.¬†masking prompt tokens (labels = -100) so the loss is computed only on response tokens, helping the model focus on generating good responses.\nProblem 1: BPE tokenization boundary issues\nImplementing this led to an interesting debugging session. When I tokenized the prompt separately (ending with \"### Response:\\n\") and compared it to the tokens in the full sequence (prompt + response), the boundary tokens didn‚Äôt match. This is a known issue of BPE tokenization: subword merging behavior changes based on context.\nMy first instinct was to try to implement complex boundary detection logic. However, I thought let‚Äôs try the simplest fix that works.\nSolution: I decided to drop the last token from the prompt before masking. This is a bit quick fix. However, I might train on one extra formatting token (likely just a newline) but will never accidentally mask response tokens.\n# Conservative fix: drop last prompt token to avoid boundary issues\nprompt_length = len(prompt_tokens) - 1\nlabels[:prompt_length] = -100\nProblem 2: Very short or empty responses\nAnother issue I ran into with prompt masking, some training examples had very short or empty responses. When all tokens are masked leaving only a few response tokens, the cross-entropy loss calculation can produce extreme values or NaNs.\nSolution: The fix was simple. I filtered out examples with very short responses (0-2 words) from both training and validation sets.\n\n\nSetting Up AlpacaEval\nA quick note on the AlpacaEval evaluation setup. It uses an LLM-as-judge approach where an annotator model compares outputs from your mode against GPT-4 reference responses.\nThe assignment suggested deploying Llama-3.3-70B-Instruct locally as the annotator, but that requires at least two GPUs which is not cost effective (atleast for my case). Instead, I used Llama-3.3-70B-Instruct via Fireworks API. This required some config tweaking (API key mapping, judge configuration) but works well.\n\n\nResults and Analysis\nI ran two experiments: one with prompt masking (mask) and one without (no-mask).\n\n\n\nInstruction Fine-tuning Comparison\n\n\n\n\n\nBenchmark\nBaseline\nNo-Mask\nMask\n\n\n\n\nGSM8K\n16.4%\n29.0%\n32.7%\n\n\nMMLU\n58.1%\n58.4%\n58.2%\n\n\nSST Safety\n62.0%\n78.0%\n77.0%\n\n\nAlpacaEval\n1.57%\n5.3%\n4.5%\n\n\n\n\nGSM8K (16% -&gt; 29-33%): Both approaches significantly improved math reasoning, but masking helped more (32.7% vs 29.0%).\nSafety (62% -&gt; 78%): You see big improvement as expected since the training data includes SafetyLlama examples.\nAlpacaEval (1.6% -&gt; 5.3%): The conversational instruction-following improved substantially. Interestingly, no-mask performed slightly better (5.3% vs 4.5%). My guess: training on the full sequence helps the model learn overall conversational patterns and produce more naturally flowing responses that match the prompt style.\nMMLU (~58% -&gt; ~58%): This stayed flat and that‚Äôs actually good news. MMLU tests factual knowledge which is encoded during pre-training. SFT teaches the model how to respond, not what to know. The fact that MMLU didn‚Äôt drop means we avoided catastrophic forgetting issue.\n\n\n\n\nLooking at individual MMLU subjects, some regressed slightly (college math: 33% -&gt; 26%) while others improved slightly, leading to near-zero net change."
  },
  {
    "objectID": "posts/2025-12-28-sft-from-scratch.html#conclusion",
    "href": "posts/2025-12-28-sft-from-scratch.html#conclusion",
    "title": "What I Learned Building SFT from the Ground Up",
    "section": "Conclusion",
    "text": "Conclusion\nWhile writing the SFT code from scratch, I ran into a lot of debugging challenges. It was at times painstaking and frustrating but was also a valuable learning experience. By debugging, I learned a lot about how things work under the hood, and the whole experience prepares you for how to go about debugging code/projects in the future.\nI leave you with some of the debugging tips I came across:\n\nvLLM OOM: Tune max_model_len, max_num_seqs, and gpu_memory_utilization and start conservative.\nPer-token loss: Normalize by response token count to prevent long sequences from dominating gradients.\ntorch.compile + vLLM: Access weights via model._orig_mod when loading into vLLM.\nBPE boundaries: Drop last prompt token before masking to avoid tokenization edge cases.\nData quality matters: Filtering incorrect traces gave me a 10% accuracy boost.\nvLLM version issues: Set VLLM_ENABLE_V1_MULTIPROCESSING=0 if model_executor is missing."
  },
  {
    "objectID": "posts/2025-12-28-sft-from-scratch.html#resources",
    "href": "posts/2025-12-28-sft-from-scratch.html#resources",
    "title": "What I Learned Building SFT from the Ground Up",
    "section": "Resources",
    "text": "Resources\nI have made all the code, datasets, and model checkpoints publicly accessible.\n\nCode: building-from-scratch/sft\nDatasets: garg-aayush/sft-cs336-assign5-datasets\nCheckpoints:\n\nReasoning:\n\nrun_all: qwen-2.5-math-sft-all-2epoch\nrun_filtered: qwen-2.5-math-sft-filtered-2epoch\nrun_filtered-res-len: qwen-2.5-math-sft-filtered-res-len\nrun_filtered-2epoch: qwen-2.5-math-sft-filtered-2epoch\n\nInstruction d:\n\nrun_mask: llama31-8b-sft-mask\nrun_nomask: llama31-8b-sft-nomask\n\n\nTraining logs: wandb/sft and wandb/sft_instruct"
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "",
    "text": "ComfyUI is by far my favorite open-source software right now. Its intuitive node-based interface has transformed the way we build AI image and video generation workflows.\nWhat I really appreciate about ComfyUI is its flexibility. You can easily extend it with your own custom nodes. Here, I‚Äôll show you how to create custom nodes that let you add exactly the tools you need. I‚Äôll use parts from my Svg2Raster nodes as the running example for this purpose."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#svg2raster",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#svg2raster",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Svg2Raster",
    "text": "Svg2Raster\nComfyUI does not natively support vector graphics like SVGs. I often work with them and needed lightweight nodes to load (SVG-&gt;JPEGs/PNGs) and manipulate SVGs in ComfyUI.\nThus, I built Svg2Raster, a small custom node package that makes it easy to use SVGs with other nodes."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-1-validate-the-core-logic-first",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-1-validate-the-core-logic-first",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 1: Validate the core logic first",
    "text": "Step 1: Validate the core logic first\nI prefer not to start with the ComfyUI node API. First, I like to write a simple Python notebook to test the functionalities I actually need. This validates your core code logic and packages in isolation.\nIn my case, I needed a way to read, rasterize and manipulate the SVGs. Thus, I tested all the relevant operations using the core packages CairoSVG and Pillow.\nFor example:\n# Simple SVG read and conversion check\nimport cairosvg\nfrom PIL import Image, ImageOps\nimport io\n\n# Read SVG file\nwith open('logo.svg', 'r', encoding='utf-8') as f:\n    svg_text = f.read()\n\n# Basic conversion to PNG\nimg_bytes = cairosvg.svg2png(bytestring=svg_text.encode('utf-8'), \n                              output_width=600)\nimg = Image.open(io.BytesIO(img_bytes)).convert('RGBA')\nprint(f\"Image size: {img.size}\")\nIf you want to see all the code snippets (width/height controls, color and border manipulations etc.), please check out the full notebook.\nOnce you have a working standalone script or code, wrapping it as a ComfyUI node is mostly boilerplate."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-2-understand-the-anatomy-of-a-custom-node",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-2-understand-the-anatomy-of-a-custom-node",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 2: Understand the Anatomy of a Custom Node",
    "text": "Step 2: Understand the Anatomy of a Custom Node\nEvery ComfyUI node is a Python class with specific methods that ComfyUI expects. Here are the essential components:\n\n\n\nComponent\nDescription\n\n\n\n\nINPUT_TYPES\nWhat inputs your node accepts\n\n\nRETURN_TYPES\nWhat it outputs to other nodes\n\n\nRETURN_NAMES\nOptional labels for outputs\n\n\nFUNCTION\nThe method name that runs your logic\n\n\nCATEGORY\nWhere it appears in ComfyUI‚Äôs node menu\n\n\n\nComfyUI handles the rest of UI, connections and execution order. For example, this is how a simple custom node class will looks like:\nclass LoadSVG:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"svg_file\": (\"STRING\", {\"default\": \"file.svg\"}),\n            }\n        }\n    \n    RETURN_TYPES = (\"IMAGE\", \"STRING\")\n    RETURN_NAMES = (\"image\", \"svg_text\")\n    FUNCTION = \"load_svg\"\n    CATEGORY = \"Svg2Raster\"\n    \n    def load_svg(self, svg_file):\n        # Your actual logic here\n        return (image_tensor, svg_text)\nThis is a minimal ComfyUI node class explanation that I believe is good enough to start writing your own nodes. If you want more details, check the official ComfyUI custom node documentation."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-3-implementing-the-loadsvgimage-node",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-3-implementing-the-loadsvgimage-node",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 3: Implementing the LoadSVGImage Node",
    "text": "Step 3: Implementing the LoadSVGImage Node\nFirst, I set up the file structure in ComfyUI‚Äôs custom_nodes folder for my nodes package:\ncd ComfyUI/custom_nodes\nmkdir svg2raster\ncd svg2raster\nThen I create two essential files:\n__init__.py: it allows ComfyUI to import your custom nodes.\nfrom .svg2raster_node import *\n\n__all__ = [ \"NODE_CLASS_MAPPINGS\",\n            \"NODE_DISPLAY_NAME_MAPPINGS\"]\nsvg2raster_node.py: this is where the actual nodes code is written.\nYou can find the complete code for these nodes here: svg2raster_node.py. Here‚Äôs the boilerplate structure of LoadSVGImage node:\nclass LoadSVGImage:\n    @classmethod\n    def INPUT_TYPES(cls):\n        \"\"\"Define what inputs this node accepts\"\"\"\n        # Use `folder_paths` to access ComfyUI's input directory\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f)) and f.lower().endswith('.svg')]\n        return {\n            \"required\": {\n                \"svg\": (sorted(files), {\"image_upload\": True}),\n            }\n        }\n    \n    # Output configuration\n    RETURN_TYPES = (\"STRING\", \"IMAGE\")\n    RETURN_NAMES = (\"svg_text\", \"preview_image\")\n    FUNCTION = \"load_svg\"  # Method name to execute\n    CATEGORY = \"FromSVG/Tools\"  # Menu location\n    \n    def load_svg(self, svg, background=\"#FFFFFF\"):\n        \"\"\"Main execution method - does the actual work\"\"\"\n        # Your logic here\n        return (svg_text, image_tensor)\n    \n    @classmethod\n    def IS_CHANGED(cls, svg):\n        # Returns file hash or modification time\n        pass\n    \n    @classmethod\n    def VALIDATE_INPUTS(cls, svg):\n        # Check if file exists, return error string if invalid\n        return True\nHere, the helper methods serve crucial purposes: - IS_CHANGED: Tells ComfyUI when to re-execute the node - VALIDATE_INPUTS: Prevents crashes by validating inputs before execution\nComfyUI expects images as tensors in BHWC format (batch, height, width, channels) with values normalized to 0-1. Thus, you need to have a pil to tensor function.\ndef _pil_to_tensor(pil_img: Image.Image):\n    \"\"\"Convert PIL image to ComfyUI IMAGE tensor: (B, H, W, C) in [0,1]\"\"\"\n    # conversion logic\nFinally, you need the mappings for ComfyUI to discover your nodes:\nNODE_CLASS_MAPPINGS = {\n    \"LoadSVGImage\": LoadSVGImage,\n}\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"LoadSVGImage\": \"Load SVG Image\",\n}\nWithout these mappings, ComfyUI won‚Äôt find your nodes even if the code is correct.\nSimilarly, I also wrote a RasterizeSVG class for manipulating the loaded SVG. It takes the SVG text from LoadSVGImage and lets you adjust scale, dimensions, borders, and more. You will see the pattern is identical: define inputs, process with CairoSVG/PIL, convert to tensor, return.\nThat‚Äôs how you implement a node. Write a standalone functionality script, wrap it in the ComfyUI class structure, handle the tensor conversions, add the helper methods, and register it with the mappings."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-4-testing-your-custom-node-in-comfyui",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-4-testing-your-custom-node-in-comfyui",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 4: Testing Your Custom Node in ComfyUI",
    "text": "Step 4: Testing Your Custom Node in ComfyUI\nOnce you are done implementing, testing is straightforward.\n\nEnsure all your dependencies are installed, including CairoSVG:\nRestart ComfyUI for it to detect the new node.\nFind your nodes under the defined category\n\nNote: I have also added the installation steps here."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-5-sharing-and-publishing-your-node",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-5-sharing-and-publishing-your-node",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 5: Sharing and Publishing Your Node",
    "text": "Step 5: Sharing and Publishing Your Node\nOnce everything worked, I created a GitHub repo for the nodes. You can see how I structured mine in the Svg2Raster repo.\nSome Essential files in the repo are:\n\n\n\nFile\nDescription\n\n\n\n\nREADME.md\nClear installation instructions and usage examples\n\n\nrequirements.txt\nPython dependencies (cairosvg in my case)\n\n\npyproject.toml\nRequired if you plan to publish to ComfyUI Registry\n\n\nexamples/\nOptional Sample SVG files and workflow JSON files\n\n\n\nNote: Having a good Readme and examples makes a huge difference for users trying to understand and use your nodes.\nOnce your repo is ready, you can even publish it to the ComfyUI Registry. There‚Äôs an excellent guide on publishing to ComfyUI Registry - just follow those steps.\nI also set up a GitHub Actions workflow that automatically publishes updates to the ComfyUI Registry whenever I push changes to my repo. This ensures the registry always has the latest version. You can check out my workflow file to see how I did it."
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "",
    "text": "A couple of months back, I enrolled in AI Evals for Engineers and PMs, a course by Hamel and Shreya. The live cohort for ot ran from July to mid-August, but due to work commitments I couldn‚Äôt follow along in real time.\nI have now started following it as a self-paced course and plans to write a blog for each lesson as I progress. This will be my way to capture what I learn and to reflect on the material. In this first blog ü§û, I‚Äôll walk through my key takeaways from introductory Lecture 1."
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#evaluation-isnt-optional-but-fundamental",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#evaluation-isnt-optional-but-fundamental",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "1. Evaluation isn‚Äôt Optional but Fundamental",
    "text": "1. Evaluation isn‚Äôt Optional but Fundamental\nAnyone who has built or worked with LLM pipelines knows that their outputs are open-ended, subjective, and unstructured (unless you enforce it). If you rely on ad-hoc checks which I have been guilty of, it often leads to knee-jerk fixes. Moreover, it completely miss the long-term need of continuous tracking which is essential for improving your pipeline reliability and usefulness. This is why Evaluation‚Äîthe systematic measurement of an LLM pipeline quality‚Äîis critical!"
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#the-three-gulfs",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#the-three-gulfs",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "2. The Three Gulfs",
    "text": "2. The Three Gulfs\nThe below image beautifully captures and categorizes the challenges associated with any LLM application: \n\nGulf of Comprehension: This is a result of limited understanding of the input data (user queries) and the pipeline‚Äôs outputs (behavior). Bridging it requires examining examples to identify common failure modes. This brings it own challenge: ‚ÄúHow to manually review every input or output to identify failure modes?‚Äù\nGulf of Specification: It refers to the difficulty of translating a user‚Äôs high-level intent into unambiguous precise instructions for the LLM. Bridging it requires writing detailed prompts that captures ‚Äútrue intent‚Äù which in itself is challenging due to ambiguous nature of natural language.\nGulf of Generalizaton: This is due to LLMs unexpected and inconsistent behavior on new or unusual (out of distribution) inputs. Bridging it requires a good understanding of your LLM model capabilities. This leads to the question: ‚ÄúHow to improve LLM model?‚Äù"
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#analyze-measure-improve-lifecycle",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#analyze-measure-improve-lifecycle",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "3. Analyze ‚Üí Measure ‚Üí Improve Lifecycle",
    "text": "3. Analyze ‚Üí Measure ‚Üí Improve Lifecycle\nHamel and Shreya introduced a structured way to bridge the above gulfs: Analyze ‚Üí Measure ‚Üí Improve lifecycle.\n\n\n\nAnalyze ‚Üí Measure ‚Üí Improve Lifecycle\n\n\nHowever, the most important takeaways for me was not what each phase means but the pitfalls that often derail them:\n\n\n\n\n\n\n\n\nPhase\nPitfalls\nNotes\n\n\n\n\nAnalyze\nOutsourcing annotation; looking at too few examples and forming shaky hypotheses\nThis is where you learn the most. Spend ~75‚Äì80% of your time here‚Äîgood analysis sets up everything else.\n\n\nMeasure\nMisaligned or poorly designed LLM judges; ‚Äúoverfitting‚Äù by testing judges on the same examples used in the judge prompt\nIn this phase, you need the rigor of data science. NEVER leak test data into judge prompts.\n\n\nImprove\nPrematurely jumping to fixes; defaulting to the most complex solution first (fine-tuning, bigger models)\nStart simple. Prompt tweaks and improvements often go a long way before heavier changes are needed."
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#llms-are-imperfectprompt-iteratively",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#llms-are-imperfectprompt-iteratively",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "4. LLMs are Imperfect‚ÄîPrompt Iteratively",
    "text": "4. LLMs are Imperfect‚ÄîPrompt Iteratively\nWhen we write prompts it‚Äôs easy to ignore that LLMs are non-deterministic, prompt-sensitive and can confidently hallucinate. Thus, always remember: ‚ÄúLLMs are powerful but imperfect components. Leverage strengths, anticipate weaknesses.‚Äù\n\n\n\nLLM Strengths vs.¬†Weaknesses\n\n\nEffective prompting starts with you. You should not delegate the prompting to an LLM or you will miss important failure modes. Instead, write your own draft prompt and if needed, use an LLM only to polish clarity.\nFrom there on, treat prompting as an iterative process where the first draft is a starting point which you refine based on observed outputs."
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#reference-based-vs-reference-free-metrics",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#reference-based-vs-reference-free-metrics",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "5. Reference-based vs Reference-free Metrics",
    "text": "5. Reference-based vs Reference-free Metrics\nThe evaluation metrics broadly fall into two categories: reference-free and reference-based. Both of them are useful but in different contexts.\n\n\n\n\n\n\n\n\n\nReference-Free\nReference-Based\n\n\n\n\nWhat it means\nEvaluates properties of the output itself (no golden answer required)\nCompares output against a golden reference or ground truth\n\n\nWhen to use\nCreative or open-ended tasks, formatting/structure checks, validity tests\nTasks with clearly defined correct answers (e.g., factual QA, deterministic outputs)\n\n\nExamples\n- Does the output follow the JSON format?- Does generated code/SQL run without errors?\n- Exact match against a gold SQL query- ROUGE/BLEU score for text generation"
  },
  {
    "objectID": "posts/2024-07-09-compare-models-structured-data.html",
    "href": "posts/2024-07-09-compare-models-structured-data.html",
    "title": "Part II: Comparison of Model Performances on Structured Functional Representation Extraction",
    "section": "",
    "text": "In the previous blog post, I established a performance baseline using GPT-4o for generating structured data, particularly functional representations, from text using the ViGGO Dataset.\nBuilding on that foundation, I expand the experiment to include a broader range of models, both open-source and proprietary. This comparison aims to provide insights on how well these models perform out of the box in structured data extraction tasks, which is quite crucial for RAG applications, knowledge base construction, and reasoning systems.\nI evaluate and compare the performance of these six LLM models:\n\nGPT-4o: OpenAI‚Äôs latest iteration of the GPT-4 model, known for its faster generation, advanced natural language understanding and generation capabilities. Currently one of the most popular and capable models.\nClaude Sonnet-3.5: Anthropic‚Äôs refined language model with enhanced reasoning abilities. It aims to provide more context-aware outputs compared to earlier versions and has recently outperformed GPT-4o on many benchmarks.\nGemini-1.5-Flash: Google DeepMind‚Äôs streamlined version of the Gemini model, optimized for faster inference and reduced computational requirements.\nllama-3-70b-instruct: Meta‚Äôs large-scale instruction-tuned language model, part of the latest LLaMA 3 family, with 70 billion parameters. It‚Äôs designed to follow complex instructions and generate high-quality text across diverse domains.\nmixtral-8x7b-instruct-v0.1: Mistral AI‚Äôs instruction-tuned variant of the Mixtral 8x7B model, known for its mixture-of-experts architecture.\nllama-3-8b-instruct: A more compact version of Meta‚Äôs LLaMA 3 family, with 8 billion parameters, optimized for instruction following.\n\n\nNote: I‚Äôve included the smaller Llama-3-8B model as I plan to finetune it‚Äôs base 8B model in coming days. It would help me compare the general instruction finetuned 8B model performance."
  },
  {
    "objectID": "posts/2024-07-09-compare-models-structured-data.html#references",
    "href": "posts/2024-07-09-compare-models-structured-data.html#references",
    "title": "Part II: Comparison of Model Performances on Structured Functional Representation Extraction",
    "section": "References",
    "text": "References\n\nPart I blog post of series\nViGGO Dataset\nNotebook I: Generate_responses_all_llms.ipynb\nNotebook II: Compare_models_performances.ipynb\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "posts/2024-06-30-remote-gpu-server.html",
    "href": "posts/2024-06-30-remote-gpu-server.html",
    "title": "Step-by-Step Guide to Setup Your Personal GPU Server",
    "section": "",
    "text": "Github Gist Link\nI‚Äôve been using a GPU workstation with an RTX 4090 for almost a year now, and it‚Äôs been one of the best decisions I‚Äôve made. With a personal GPU server, you no longer need to rely on cloud-based GPU instances from services like RunPod or Vast.ai every time you want to run a job or try new models. The best part? No stress about recurring GPU instance costs! :-)\nHowever, I rarely work directly on my workstation. Instead, I prefer the flexibility of accessing the GPU remotely using my MacBook, whether I‚Äôm working from different locations within my home, from a co-working space, or a cozy cafe in another part of town.\nIn this blog, I will walk you through the steps to configure a personal GPU Ubuntu server.\nFor this guide, I assume you already have a workstation running Ubuntu with a GPU and it is connected to your local network"
  },
  {
    "objectID": "posts/2024-06-30-remote-gpu-server.html#setting-up-local-remote-access",
    "href": "posts/2024-06-30-remote-gpu-server.html#setting-up-local-remote-access",
    "title": "Step-by-Step Guide to Setup Your Personal GPU Server",
    "section": "Setting Up Local Remote Access",
    "text": "Setting Up Local Remote Access\nLet‚Äôs start by setting up local access, which will allow you to ssh into your GPU server when you‚Äôre on the same home Wi-Fi network. This is ideal for a work-from-home (WFH) setup where your workstation is running in a corner of your living space.\n\nInstall the SSH server\nFirst, we need to install an SSH (Secure Shell) server. This will allow you to securely access your GPU machine remotely. Open a terminal on your Ubuntu machine and run the following commands: bash  sudo apt update &&  sudo apt install openssh-server This command updates your package lists and installs the OpenSSH server.\nStart and Enable SSH Service\nNext, enable the SSH service using this command: bash  sudo systemctl enable --now ssh You can verify if the service is enabled by running: bash  sudo systemctl status ssh\nLook for a line starting with Active: active (running) for ssh.service. This indicates that the SSH service is up and running.\n\nNote: The OpenSSH server starts running on boot by default.\n\nConfigure the firewall\nTo allow SSH connections through the system firewall, you need to open the appropriate port. Ubuntu‚Äôs default firewall, UFW (Uncomplicated Firewall), makes this process straightforward: bash  sudo ufw allow ssh This command adds an exception to your firewall rules, permitting incoming SSH connections. You can check the SSH status with: bash  sudo ufw status You should see the output similar to: bash  To                         Action      From  --                         ------      ----  22/tcp                     ALLOW       Anywhere  22/tcp(v6)                 ALLOW       Anywhere (v6)\nConnect to the local server\nNow that your GPU server is set up, it‚Äôs time to test the connection. From your laptop (which should be on the same local network as your GPU machine), open a terminal and use the following command: bash  ssh user@local-ip-address Replace user with your Ubuntu user and local-ip-address with the IP address of your GPU machine on the local network.\n\nTo find your username on the workstation, you can use the whoami command.\nTo find your local IP address, use one of these methods on your workstation:\n\nRun hostname -I and use the first address listed.\nUse ip addr show | grep -w inet for more detailed network information.\nHow to find my IP address on Ubuntu Linux is a great blog on it. It explains multiple commands like ip addr show | grep -w inet or networkctl status to get the local IP address.\n\n\n\nYour local IP address typically starts with 192.168.\nNote: If your router dynamically changes the local IP address of your workstation, it‚Äôs best to log into your router and assign a fixed local IP address to ensure consistent access.\n\nIf everything is configured correctly, you‚Äôll be prompted to enter your password, after which you‚Äôll gain remote access to your GPU server.\nSet Up SSH Keys for Passwordless Login\nIt is recommended to set up key-based authentication for better security and convenience purposes. This allows you to connect to your remote server without entering a password each time.\n\nIt is quite common to setup ssh key-based authentication.\nFor detailed instructions on setting up SSH keys, refer to the DigitalOcean guide on Setting up SSH keys on Ubuntu 20.04."
  },
  {
    "objectID": "posts/2024-06-30-remote-gpu-server.html#setting-up-external-remote-access",
    "href": "posts/2024-06-30-remote-gpu-server.html#setting-up-external-remote-access",
    "title": "Step-by-Step Guide to Setup Your Personal GPU Server",
    "section": "Setting Up External Remote Access",
    "text": "Setting Up External Remote Access\nWhile local access is great for working within your home network, sometimes you need to access your GPU workstation from outside your local network, such as from co-working spaces or a cozy cafe.\nOne simple and secure way to achieve this is by using ngrok.\n\nngrok helps creates secure tunnels from public endpoints to locally running services. It allows you to expose your personal server to the internet, enabling remote access from anywhere without complex network configurations.\n\nHere‚Äôs how to set it up:\n\nInstall ngrok\nFirst, you need to install ngrok on your GPU workstation. Open a terminal and run this command: bash  snap install ngrok\n\nFor more installation options, see https://dashboard.ngrok.com/get-started/setup/linux.\n\nCreate and connect to ngrok Account\nVisit ngrok‚Äôs website and sign up for a free account if you haven‚Äôt already. After signing up, you‚Äôll receive an auth token. On your GPU workstation, run: bash  ngrok config add-authtoken YOUR_AUTH_TOKEN You can get the config file path and edit using ngrok config check and vim &lt;path&gt;, respectively.\nStart the ngrok Tunnel\nNow, you can create a secure tunnel to your SSH service: bash  ngrok tcp 22 This command will display a URL that looks like tcp://X.tcp.ngrok.io:PORT. Note down this URL.\nConnect to Your Workstation\nFrom any external laptop, you can now SSH into your GPU workstation using: bash  ssh -p YYYY user@X.tcp.ngrok.io Replace PORT with the port number and X with the subdomain from the ngrok URL. Replace user with your Ubuntu username.\nThe above steps ensure that you can remotely access the workstation from external network. However, no one is going to manually start the ngrok every time before heading out.\nMake ngrok start automatically on boot\nTo ensure ngrok starts automatically when your workstation boots:\n\nCreate a new service file:\n\nsudo vim /etc/systemd/system/ngrok.service\n\nAdd the following content:\n\n[Unit]\nDescription=start ngrok tunnel on startup\nAfter=network.target\n\n[Service]\nExecStart=/snap/bin/ngrok tcp 22\nRestart=on-failure\nUser=&lt;your_username&gt;\n\n[Install]\nWantedBy=multi-user.target\nReplace &lt;your_username&gt; with your Ubuntu username. Save the file and exit the editor.\n\nEnable and start the service:\n\nsudo systemctl enable ngrok.service\nsudo systemctl start ngrok.service\nNow ngrok will automatically start and create a tunnel when your workstation boots.\n\nNote: With a free account, ngrok assigns a new port (YYYY) each time your workstation boots. You can get the new port from the ngrok dashboard.\n\nPaid ngrok account for dedicated port\nFor a dedicated TCP endpoint port that doesn‚Äôt change on reboot, you need a paid ngrok personal account ($10/month).\n\nReserve a tcp endpoint\n\nOnce you have a paid account, reserve a TCP endpoint at https://dashboard.ngrok.com/cloud-edge/tcp-addresses.\n\nUpdate the ngrok service file\n\nAdd the following content:\n[Unit]\nDescription=start ngrok tunnel on startup\nAfter=network.target\n\n[Service]\nExecStart=/snap/bin/ngrok tcp --region=&lt;region&gt; --remote-addr=&lt;remote-address&gt; 22\nRestart=on-failure\nUser=&lt;your_username&gt;\n\n[Install]\nWantedBy=multi-user.target\nReplace &lt;region&gt;, &lt;remote-address&gt;, and &lt;your_username&gt; with the appropriate values from your reserved TCP endpoint config.\n\nWith this setup, your SSH remote endpoint will remain the same even if the system reboots.\n\nReference Links:\n\nUbuntu SSH Documentation\nDigitalOcean Guide on Setting Up SSH Keys\nngrok Documentation\nIP Address Information for Ubuntu\nUFW (Uncomplicated Firewall) Guide\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "pages/overlap-aware-sc.html",
    "href": "pages/overlap-aware-sc.html",
    "title": "Multi-class Spectral Clustering with Overlaps for Speaker Diarization",
    "section": "",
    "text": "This post describes the implementation of our paper ‚ÄúMulti-class spectral clustering with overlaps for speaker diarization‚Äù, accepted for publication at IEEE SLT 2021.\nThe code consists of two parts: overlap detector, and our modified spectral clustering method for overlap-aware diarization.\nWe have implemented the diarization recipe in Kaldi, and modified scikit-learn‚Äôs spectral clustering class for our modification. The entire code to reproduce our results is available at: https://github.com/desh2608/kaldi/tree/slt21_spectral\n\nInstallation and setup\nSince the recipe is implemented using Kaldi, you first need to install Kaldi by following the instructions at: https://github.com/kaldi-asr/kaldi/blob/master/INSTALL\nIn the Kaldi installation, miniconda is not installed by default. To install it, go to tools/ and run:\nextras/install_miniconda.sh\nInstall a modified version of scikit-learn in the miniconda Python installation:\n$HOME/miniconda3/bin/python -m pip install git+https://github.com/desh2608/scikit-learn.git@overlap\n\n\nUsage\nThe recipe containing the overlap detector and the spectral clustering for AMI can be found at egs/ami/s5c. Additionally, the recipe also contains example for different clustering methods, namely AHC, VBx, and spectral clustering, to reproduce the single-speaker baselines in the paper.\nThe run.sh script does not contain stages for training an x-vector extractor, since we used the same extractor from the CHiME-6 baseline system.\nThe key stages in the script are as follows.\n\n--stage 8: Trains the overlap detector.\n--stage 9: Performs decoding with a trained overlap detector.\n--stage 10: Performs spectral clustering informed by the output from stage 9.\n\n\n\nWhere to find the clustering implementation?\nWe use scikit-learn‚Äôs spectral clustering class to implement our modified clustering method. In the default scikit-learn class, the argument assign_labels can take on two values:\n\nkmeans: This performs the conventional spectral clustering using the Ng-Jordan-Weiss method.\ndiscretize: This implements the clustering described in this paper and we modify it for our implementation.\n\nWe modified the discretize() function here by adding an additional argument which specifies the overlap vector. The vector is used in L141-148 to assign a second label to the overlapping segments.\n\n\nPre-trained model\nFor all our clustering-based diarization experiments, we used the x-vector extractor that was provided with the CHiME-6 baseline system, and is available here. To use it, first download the extractor using wget and then extract it using tar -xvzf and copy the contents to your exp directory.\n\n\nCitation\nIf you find this code useful, consider citing our paper:\n@inproceedings{Raj2021MultiSC,\n  title={Multi-class spectral clustering with overlaps for speaker diarization},\n  author={Desh Raj and Zili Huang and Sanjeev Khudanpur},\n  booktitle={IEEE Spoken Language Technology Workshop},\n  year={2021},\n}\n\n\nQuestions\nFor any questions about using the code, you can contact me at draj@cs.jhu.edu."
  },
  {
    "objectID": "pages/doverlap.html",
    "href": "pages/doverlap.html",
    "title": "DOVER-Lap",
    "section": "",
    "text": "The code is publicly available under the MIT license: Github repo"
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Speaker profile",
    "section": "",
    "text": "Desh is a PhD student at Johns Hopkins University, working in the Center for Language and Speech Processing (CLSP), advised by Sanjeev Khudanpur and Dan Povey. His research interests lie in the application of machine learning methods for speech and language tasks. He is currently working on speech recognition (a.k.a speech-to-text) and speaker diarization (a.k.a who spoke when) for multi-party conversations. He wants to build systems which can identify the speaker and transcribe their speech in real time, and do this for all of the world‚Äôs languages.\nHe spent summer 2021 building end-to-end multi-talker ASR systems at Microsoft. In summer 2022, he was an intern in the AI Speech team at Meta, where he explored target-speaker ASR models for improving transducer models in the presence of background speech. Desh has several publications at ICASSP, InterSpeech, SLT, and ASRU, with over 450 citations.\nPreviously, he graduated from the Indian Institute of Technology Guwahati in 2017 with a major in Computer Science, where his thesis was on deep learning methods for biomedical text. He has worked on building smart assistants at Samsung Research (India) and was an intern at Microsoft India, devising statistics APIs for the Enterprise Commerce team.\nWhen he is not doing ML, he likes to work out, climb boulders, play guitar, and read fiction."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Target-oriented elastic parameter models estimation from surface seismic data using JMI-res\n\n\nGarg, A., Verschuur, D. J.\n\n\nInvited speaker for FWI workshop in 82nd EAGE Conference and Exhibition\n\n\n\n\n\n\n\n\n\nFrom surface seismic data to reservoir elastic parameters using a full-wavefield redatuming approach\n\n\nGarg, A., Verschuur, D. J.\n\n\nGeophysical Journal International, Volume 221, Issue 1, April 2020, Pages 115-128\n\n\n\n\nSpatial aliasing removal using deep learning super-resolution\n\n\nGarg, A., Vos, A., Bortych, N., Gupta, D. K., Verschuur, D. J.\n\n\nFirst Break, 37(9), 87-92\n\n\n\n\nSpatial aliasing removal using deep learning super-resolution\n\n\nGarg, A., Verschuur, D. J.\n\n\n89th Annual International Meeting, SEG Technical Program Expanded Abstracts, Pages 644-648\n\n\n\n\nBlock-Krylov methods for multi-dimensional deconvolution\n\n\nLuiken N., Garg, A.\n\n\n89th Annual International Meeting, SEG Technical Program Expanded Abstracts\n\n\n\n\nElastic parameters estimation using reservoir-oriented Joint Migration Inversion for Norwegian Sea field data\n\n\nGarg, A., H. Masoomzadeh, S. Baldock, Verschuur, D. J.\n\n\n81st EAGE Conference and Exhibition 2019, Jun 2019, Volume 2019, Pages 1-5\n\n\n\n\n\n\n\n\n\nIncluding internal multiples in joint migration inversion and redatuming of North Sea field data\n\n\nGarg, A., Verschuur, D. J.\n\n\n88th Annual International Meeting, SEG, Extended abstracts, Pages 4342-4346\n\n\n\n\nReservoir elastic parameters estimation from surface seismic data using JMI-res: A full-wavefield approach\n\n\nGarg, A., S. Sharma, Verschuur, D. J.\n\n\n80th EAGE Conference and Exhibition 2018, Jun 2018, Volume 2018, Pages 1-5\n\n\n\n\nUsing one-way propagators to build a full wavefield inversion process\n\n\nVerschuur, D. J., Davydenko, M., Garg, A.\n\n\n80th EAGE Conference and Exhibition 2018 Workshop Programme, Jun 2018, cp-556-00082\n\n\n\n\n\n\n\n\n\nElastic reflectivity preserving full-wavefield inversion\n\n\nGarg, A., Verschuur, D. J.\n\n\n87th Annual International Meeting, SEG Technical Program Expanded Abstracts, Pages 5561-5566\n\n\n\n\nHigh-resolution reservoir impulse response estimation\n\n\nGarg, A., Verschuur, D. J.\n\n\n87th SEG Technical Program Expanded Abstracts, Pages: 3516-3520\n\n\n\n\nAVP-preserving estimation of reservoir impulse responses\n\n\nGarg, A., Verschuur, D. J.\n\n\n79th EAGE Conference and Exhibition 2017, Jun 2017, Volume 2017, Pages 1-5\n\n\n\n\n\n\n\n\n\nReservoir impulse response estimation using joint migration inversion\n\n\nGarg, A., Verschuur, D. J.\n\n\n78th EAGE Conference and Exhibition 2016, May 2016, Volume 2016, Pages 1-5"
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "",
    "text": "Target-oriented elastic parameter models estimation from surface seismic data using JMI-res\n\n\nGarg, A., Verschuur, D. J.\n\n\nInvited speaker for FWI workshop in 82nd EAGE Conference and Exhibition"
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "",
    "text": "From surface seismic data to reservoir elastic parameters using a full-wavefield redatuming approach\n\n\nGarg, A., Verschuur, D. J.\n\n\nGeophysical Journal International, Volume 221, Issue 1, April 2020, Pages 115-128\n\n\n\n\nSpatial aliasing removal using deep learning super-resolution\n\n\nGarg, A., Vos, A., Bortych, N., Gupta, D. K., Verschuur, D. J.\n\n\nFirst Break, 37(9), 87-92\n\n\n\n\nSpatial aliasing removal using deep learning super-resolution\n\n\nGarg, A., Verschuur, D. J.\n\n\n89th Annual International Meeting, SEG Technical Program Expanded Abstracts, Pages 644-648\n\n\n\n\nBlock-Krylov methods for multi-dimensional deconvolution\n\n\nLuiken N., Garg, A.\n\n\n89th Annual International Meeting, SEG Technical Program Expanded Abstracts\n\n\n\n\nElastic parameters estimation using reservoir-oriented Joint Migration Inversion for Norwegian Sea field data\n\n\nGarg, A., H. Masoomzadeh, S. Baldock, Verschuur, D. J.\n\n\n81st EAGE Conference and Exhibition 2019, Jun 2019, Volume 2019, Pages 1-5"
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "",
    "text": "Including internal multiples in joint migration inversion and redatuming of North Sea field data\n\n\nGarg, A., Verschuur, D. J.\n\n\n88th Annual International Meeting, SEG, Extended abstracts, Pages 4342-4346\n\n\n\n\nReservoir elastic parameters estimation from surface seismic data using JMI-res: A full-wavefield approach\n\n\nGarg, A., S. Sharma, Verschuur, D. J.\n\n\n80th EAGE Conference and Exhibition 2018, Jun 2018, Volume 2018, Pages 1-5\n\n\n\n\nUsing one-way propagators to build a full wavefield inversion process\n\n\nVerschuur, D. J., Davydenko, M., Garg, A.\n\n\n80th EAGE Conference and Exhibition 2018 Workshop Programme, Jun 2018, cp-556-00082"
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications",
    "section": "",
    "text": "Elastic reflectivity preserving full-wavefield inversion\n\n\nGarg, A., Verschuur, D. J.\n\n\n87th Annual International Meeting, SEG Technical Program Expanded Abstracts, Pages 5561-5566\n\n\n\n\nHigh-resolution reservoir impulse response estimation\n\n\nGarg, A., Verschuur, D. J.\n\n\n87th SEG Technical Program Expanded Abstracts, Pages: 3516-3520\n\n\n\n\nAVP-preserving estimation of reservoir impulse responses\n\n\nGarg, A., Verschuur, D. J.\n\n\n79th EAGE Conference and Exhibition 2017, Jun 2017, Volume 2017, Pages 1-5"
  },
  {
    "objectID": "publications.html#section-4",
    "href": "publications.html#section-4",
    "title": "Publications",
    "section": "",
    "text": "Reservoir impulse response estimation using joint migration inversion\n\n\nGarg, A., Verschuur, D. J.\n\n\n78th EAGE Conference and Exhibition 2016, May 2016, Volume 2016, Pages 1-5"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aayush Garg",
    "section": "",
    "text": "I am an applied machine learning engineer focused on building AI-driven products and systems people can rely on. At Jiffy, I build image enhancement pipelines, deploy ML models at scale, and develop LLM/VLM applications across product verticals."
  },
  {
    "objectID": "index.html#current-work",
    "href": "index.html#current-work",
    "title": "Aayush Garg",
    "section": "üíº Current Work",
    "text": "üíº Current Work\nI am currently building an in-house AI design platform, developing evaluation and prompt-optimization workflows, and exploring applied LLM fine-tuning for structured data extraction. My background is in computational geophysics, which keeps me grounded in first principles and pragmatic experimentation.\nIf you‚Äôre interested in my work or want to collaborate, feel free to reach out via email or connect on LinkedIn."
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Aayush Garg",
    "section": "üìÆ Blog",
    "text": "üìÆ Blog\nI write about practical ML engineering, LLMs, and lessons learned from building AI products. Below are my recent posts.\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\n1/1/26\n\n\nUnderstanding GRPO: PPO without the Critic\n\n\n\n\n\n\n12/30/25\n\n\nDeriving the DPO Loss from First Principles\n\n\n\n\n\n\n12/25/25\n\n\nDeriving the PPO Loss from First Principles\n\n\n\n\n\n\n12/3/25\n\n\nWhat I Learned Building SFT from the Ground Up\n\n\n\n\n\n\n9/10/25\n\n\nA Guide to Building Custom Nodes in ComfyUI\n\n\n\n\n\n\n9/8/25\n\n\nBuilding GPT from Scratch: Following Karpathy‚Äôs Tutorial\n\n\n\n\n\n\n9/2/25\n\n\nKey Takeaways from Lecture 1: LLM Evaluation Lifecycle\n\n\n\n\n\n\n7/15/24\n\n\nPart III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction\n\n\n\n\n\n\n7/9/24\n\n\nPart II: Comparison of Model Performances on Structured Functional Representation Extraction\n\n\n\n\n\n\n7/3/24\n\n\nPart I: Baseline Evaluation of GPT-4o for Functional Representation Extraction\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#reach-out",
    "href": "index.html#reach-out",
    "title": "Aayush Garg",
    "section": "üì¨ Reach Out",
    "text": "üì¨ Reach Out\nThe best way to follow my work is on GitHub, Twitter, or LinkedIn.\n\nTemplate inspired by hamel.dev and Lil‚ÄôLog."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an applied machine learning engineer passionate about building AI-driven products to solve real-world problems. My motto is ‚ÄúApplied AI should always be product-driven‚Äù.\nCurrently, I am working as a Senior Machine Learning Engineer at Jiffy, where I build image enhancement pipelines, deploy custom in-house and open-source ML models, and develop LLM/VLM applications across different verticals. My work also spans improving search and recommendation systems and designing automated prompt-optimization workflows.\nPreviously at Flixstock, I predominantly developed vision generative AI solutions, with a focus on building virtual try-on and background generation solutions emphasizing identity and pose control.\nUnlike many in this field, my core background is not in computer science. My graduate and Ph.D.¬†research experience is in computational geophysics, and I worked for Shell as a research geophysicist for about three years. I believe this diverse background helps me approach problems from unique perspectives.\nI love connecting, chatting, and collaborating with new people. Whether you are a fellow professional, a new graduate, or someone interested in discussing AI-powered products, please do not hesitate to reach out.\nYou can also book a meeting with me using the Calendly link."
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n\nJiffy\n\n\nJan 2024 - Present\n\n\n\nSenior Machine Learning Engineer\n\n\nBuilding generative AI image pipelines, LLM/VLM applications, and evals across product verticals.\n\n\n\n\n\nFlixstock\n\n\nAug 2022 - Dec 2024\n\n\n\nSoftware Development Engineer III\n\n\nDeveloped vision-based generative AI solutions for virtual try-on and background generation.\n\n\n\n\n\nShell\n\n\nOct 2019 - Jul 2022\n\n\n\nResearch Geophysicist\n\n\nFocused on uncertainty quantification and denoising of subsurface seismic data.\n\n\n\n\n\nTU Delft\n\n\nAug 2015 - Sep 2019\n\n\n\nPhD Researcher\n\n\nWorked on a new imaging approach, JMI-res, for high-resolution subsurface physical properties."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Date\n\n\n\nTitle\n\n\n\n\n\n\n\n\n1/1/26\n\n\nUnderstanding GRPO: PPO without the Critic\n\n\n\n\n\n\n12/30/25\n\n\nDeriving the DPO Loss from First Principles\n\n\n\n\n\n\n12/25/25\n\n\nDeriving the PPO Loss from First Principles\n\n\n\n\n\n\n12/3/25\n\n\nWhat I Learned Building SFT from the Ground Up\n\n\n\n\n\n\n9/10/25\n\n\nA Guide to Building Custom Nodes in ComfyUI\n\n\n\n\n\n\n9/8/25\n\n\nBuilding GPT from Scratch: Following Karpathy‚Äôs Tutorial\n\n\n\n\n\n\n9/2/25\n\n\nKey Takeaways from Lecture 1: LLM Evaluation Lifecycle\n\n\n\n\n\n\n7/15/24\n\n\nPart III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction\n\n\n\n\n\n\n7/9/24\n\n\nPart II: Comparison of Model Performances on Structured Functional Representation Extraction\n\n\n\n\n\n\n7/3/24\n\n\nPart I: Baseline Evaluation of GPT-4o for Functional Representation Extraction\n\n\n\n\n\n\n6/30/24\n\n\nStep-by-Step Guide to Setup Your Personal GPU Server\n\n\n\n\n\n\n5/19/24\n\n\nManaging multiple CUDA versions using environment modules in Ubuntu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/jsalt.html",
    "href": "pages/jsalt.html",
    "title": "Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis",
    "section": "",
    "text": "This post describes how to reproduce and extend our pipeline of speech separation, diarization and ASR. We have provided separated audio files along with entire end-to-end reproducible recipes to supplement our SLT 2021 paper.\nHere is a summary of contents of this post:\n\nSummary of our pipeline\nDatasets\nReproducible recipes\nExample use cases (for extending this work)\nCredits\n\n\n\nSummary of our pipeline\nOur speech processing pipeline consists of the following 3 stages in sequence. For each of these stages, we experimented with the methods mentioned below.\n\nSpeech separation\n\nMask-based MVDR\nSequential neural beamforming\n\nSpeaker diarization\n\nClustering: Agglomerative hierarchical clustering, spectral clustering, Variational Bayes based x-vector clustering (VBx)\nRegion proposal networks\nTarget speaker voice activity detection\n\nSpeech recognition (ASR)\n\nHybrid TDNNF-based\nEnd-to-end transformer based\n\n\n\n\n\nDatasets\nIn our paper, we compare the performance of our models on mixed and separated audio, where the separation is performed using the methods mentioned earlier.\n\nDownloading and preparing the mixed (original) LibriCSS data\n$ git clone https://github.com/chenzhuo1011/libri_css.git\n$ conda env create -f conda_env.yml\n$ conda activate libricss_release\n$ cd libri_css && ./dataprep/scripts/dataprep.sh\n\n\nDownloading the separated audio data\nAdditionally, we also provided 2-stream and 3-stream separated audio wav files through Zenodo. You can download them using the following commands:\n$ wget https://zenodo.org/record/4415163/files/libricss_mvdr_2stream.tar.gz\n$ wget https://zenodo.org/record/4415163/files/libricss_sequential_3stream.tar.gz\nOnce the archived files are downloaded, you can extract them and then use the path in the Kaldi or ESPNet recipes.\n\n\n\n\nReproducible recipes\n\nKaldi recipe\nWe have provided Kaldi recipes s5_mono and s5_css here.\n\ns5_mono: This is a single channel diarization + ASR recipe which takes as the input a long single-channel recording containing mixed audio. It then performs SAD, diarization, and ASR on it and outputs speaker-attributed transcriptions, which are then evaluated with cpWER (similar to CHiME6 Track 2).\ns5_css: This pipeline uses a speech separation module at the beginning, so the input is 2-3 separated audio streams. We assume that the separation is window-based, so that the same speaker may be split across different streams in different windows, thus making diarization necessary.\n\ns5_mono evaluates diarization and ASR on mixed audio, while s5_css does the same for separated audio streams.\nNote: Only clustering-based diarization is available in this recipe at the time of making this post, but we are also preparing RPN and TS-VAD setups.\nFor ease of reproduction, we have included training stages in the s5_mono recipe. We also provide pretrained models for both diarization and ASR systems:\n\nSAD: CHiME-6 baseline TDNN-Stats SAD available here.\nSpeaker diarization: CHiME-6 baseline x-vector + AHC diarizer, trained on VoxCeleb with simulated RIRs available here.\nASR: We used the chain model trained on 960h clean LibriSpeech training data available here. It was then additionally fine-tuned for 1 epoch on LibriSpeech + simulated RIRs. For LM, we trained a TDNN-LSTM language model for rescoring. All of these models are available at this Google Drive link.\n\n\n\nESPNet recipe\nThe ESPNet recipe corresponding to the Kaldi s5_mono recipe is available here as asr1. A recipe corresponding to s5_css is not available yet, but it should be simple to extend asr1 similar to the s5_css recipe, since it follows Kaldi-style diarization. For help or other details, please contact Pavel Denisov who created the ESPNet LibriCSS recipe.\n\n\n\n\nExample use cases (for extending this work)\nLet us now look at how this research may be extended through 2 examples.\n\nExample 1: A new window-based separation method\nSuppose you have a new window-based ‚Äúcontinuous‚Äù speech separation model, and you want to evaluate the downstream ASR performance (and compare it with the methods in our paper). This can be done as follows:\n\nDownload and prepare the ‚Äúmixed‚Äù LibriCSS audio data as described here.\nRun your separation method on this data and store the generated audio streams using the following naming convention: overlap_ratio_10.0_sil0.1_1.0_session7_actual10.1_channel_1.wav. It is similar to the naming of the original LibriCSS files, with the addition of *_channel_1* at the end which denotes the stream.\n\nNote: channel here does not refer to the microphone; it refers to the separated stream; so if your model separated the audio into 2 streams, they would have the suffixes *_channel_0* and *_channel_1*.\n\nStore all the output wav files in a directory. They can have any hierarchy within this directory as long as they follow the naming convention.\nDownload and install Kaldi, and navigate to the egs/libri_css/s5_css folder.\nIn the run.sh, replace the paths to the mixed LibriCSS data and your own separated audio files.\nRun the script. It is recommended to run the stages one by one, since the evaluation outputs after the SAD and diarization stage are also printed to the standard output.\nAt the end of decoding, the cpWERs will be printed as follows:\n\nDev WERs:\nbest_wer_session0_CH0_0L %WER 10.98 [ 130 / 1184, 34 ins, 12 del, 84 sub ]\nbest_wer_session0_CH0_0S %WER 15.10 [ 269 / 1782, 67 ins, 23 del, 179 sub ]\nbest_wer_session0_CH0_OV10 %WER 25.12 [ 465 / 1851, 156 ins, 85 del, 224 sub ]\nbest_wer_session0_CH0_OV20 %WER 18.86 [ 342 / 1813, 94 ins, 33 del, 215 sub ]\nbest_wer_session0_CH0_OV30 %WER 20.42 [ 395 / 1934, 117 ins, 40 del, 238 sub ]\nbest_wer_session0_CH0_OV40 %WER 28.47 [ 636 / 2234, 236 ins, 137 del, 263 sub ]\nEval WERs:\n0L %WER 22.36 [ 2446 / 10938, 785 ins, 413 del, 1248 sub ]\n0S %WER 19.81 [ 2970 / 14994, 861 ins, 431 del, 1678 sub ]\nOV10 %WER 21.39 [ 3412 / 15951, 1060 ins, 580 del, 1772 sub ]\nOV20 %WER 23.49 [ 3984 / 16963, 1128 ins, 747 del, 2109 sub ]\nOV30 %WER 26.06 [ 4789 / 18376, 1415 ins, 988 del, 2386 sub ]\nOV40 %WER 25.45 [ 4818 / 18932, 1410 ins, 676 del, 2732 sub ]\nThese can also be found at: exp/chain_cleaned/tdnn_1d_sp/decode_dev${data_affix}_diarized_2stage_rescore/scoring_kaldi_multispeaker/best_wer\nNote: To evaluate performance using end-to-end Transformer based ASR, you would need to first create an s5_css equivalent in ESPNet by extending the asr1 recipe.\n\n\nExample 2: A new cross-stream diarizer\nOne of the observations in our paper was that it is hard to perform good diarization on top of separated audio streams:\n\nMethods such as VBx cannot be used for this purpose because of their time continuity constraint.\nAlthough models like RPN and TS-VAD do well on mixed audio, they fail on separated audio due to a train-test mismatch (they were trained on simulated overlapping mixtures).\n\nYet, this ‚Äúseparation+diarization‚Äù system is very promising, especially considering that such a system from Microsoft obtained the best performance in the recent VoxConverse diarization challenge.\nSuppose you have a new diarization method which works across separated audio streams. To evaluate your method on LibriCSS:\n\nDownload the separated audio data from Zenodo.\nIf your method is implemented in Kaldi, clone and install Kaldi and follow the s5_css recipe until the diarization stage. Otherwise, run your implementation on the separated audio files and compute the final DER.\nYou can compare your performance against the results reported in Table 3 of our paper. The baselines can be reproduced by running the s5_css recipe till stage 3.\n\n\n\n\n\nCredits\nThis work was conducted during JSALT 2020 with support from Microsoft, Amazon, and Google. If you use the data or code in your research, consider citing:\n@article{Raj2021IntegrationOS,\n  title={Integration of speech separation, diarization, and recognition for multi-speaker \n  meetings: System description, comparison, and analysis},\n  author={Desh Raj and Pavel Denisov and Zhuo Chen and Hakan Erdogan and Zili Huang \n  and Maokui He and Shinji Watanabe and Jun Du and Takuya Yoshioka and Yi Luo and \n  Naoyuki Kanda and Jinyu Li and Scott Wisdom and John R. Hershey},\n  journal={2021 IEEE Spoken Language Technology ({SLT}) Workshop},\n  year={2021}}\n}\n\n\nOther resources\n\nPaper: Link\nSlides: Link"
  },
  {
    "objectID": "posts/2024-05-19-manage-multiple-cuda.html",
    "href": "posts/2024-05-19-manage-multiple-cuda.html",
    "title": "Managing multiple CUDA versions using environment modules in Ubuntu",
    "section": "",
    "text": "Latest Update: May 19th, 2024\nThis blog contains all the steps required to: - Install multiple CUDA versions (e.g., CUDA 11.8 andCUDA 12.1 - Manage multiple CUDA environments on Ubuntu using the utility called environment modules. - Use this approach to avoid CUDA environment conflicts."
  },
  {
    "objectID": "posts/2024-05-19-manage-multiple-cuda.html#resources-and-helpful-links",
    "href": "posts/2024-05-19-manage-multiple-cuda.html#resources-and-helpful-links",
    "title": "Managing multiple CUDA versions using environment modules in Ubuntu",
    "section": "Resources and helpful links",
    "text": "Resources and helpful links\n\nhttps://ubuntu.com/server/docs/nvidia-drivers-installation\nhttps://developer.nvidia.com/cuda-toolkit-archive\nhttps://developer.nvidia.com/cudnn-downloads\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "posts/2024-07-03-baseline-gpt4o-structured-data.html",
    "href": "posts/2024-07-03-baseline-gpt4o-structured-data.html",
    "title": "Part I: Baseline Evaluation of GPT-4o for Functional Representation Extraction",
    "section": "",
    "text": "Introduction\nExtracting structured data from unstructured texts allow us to condense the information present in the text. This representation then can be used for efficient indexing and other downstream RAG applications.¬†\nI want to evaluate GPT-4o‚Äôs performance in extracting structural data, specifically, functional representation, from the unstructured domain-specific text. I will use ViGGO dataset to evaluate it on custom evaluation criteria and will set it as baseline performance for that can be used for comparison in future work with other models such as Claude, Gemini, and custom fine-tuned open-source models.\n\n\nViGGO Dataset\nThis is a dataset for generating text opinions in the video game domain. Strictly speaking, it is intended to generate coherent conversational responses based on input functional representations (set of attributes and values).\nHowever, I use the reverse task, where I generate structured functional representations from the given text input. A typical ViGGO dataset example has the output structured functional representation consisting of a single function with attributes and attribute values.\nText:\nYou said that you liked Crysis. Do you often play first person games from Crytek Frankfurt?\n\nFunctional Representation:\nverify_attribute(name[Crysis], developer[Crytek Frankfurt], rating[good], player_perspective[first person])\nThe function and attributes must be one of the following, respectively:\nFunction:\n['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', \n'suggest', 'request_explanation', 'recommend', 'request_attribute']\n\nAttributes:\n['name', 'release_year', 'esrb', 'genres', 'platforms', 'available_on_steam',\n'has_linux_release', 'has_mac_release', 'specifier', 'rating', \n'player_perspective', 'has_multiplayer', 'developer', 'exp_release_date']\nNote: Since I am not training/fine-tuning any model, I will only consider the ViGGO validation datasetfor this exercise.\n\n\nPrompt for generating the functional representation\nI use modified version of prompt template used in Anyscale‚Äôs blog for the ViGGO dataset. The prompt template is a few-shot prompt with examples from each function category to assist the model in understanding the intended output response representation.\nPROMPT_TEMPLATE = \"\"\"\nGiven a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. \nThis function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n\nThe attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']. The order your list the attributes within the function must follow the order listed above. For example the 'name' attribute must always come before the 'exp_release_date' attribute, and so forth.\n\nFor each attribute, fill in the corresponding value of the attribute within brackets. A couple of examples are below. Note: you are to output the string after \"Output: \". Do not include \"Output: \" in your answer.\n\nExample 1)\nSentence: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\nOutput: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n\nExample 2) \nSentence: Were there even any terrible games in 2014?\nOutput: request(release_year[2014], specifier[terrible])\n\nExample 3)\nSentence: Adventure games that combine platforming and puzzles  can be frustrating to play, but the side view perspective is perfect for them. That's why I enjoyed playing Little Nightmares.\nOutput: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\n\nExample 4)\nSentence: Since we're on the subject of games developed by Telltale Games, I'm wondering, have you played The Wolf Among Us?\nOutput: recommend(name[The Wolf Among Us], developer[Telltale Games])\n\nExample 5) \nSentence: Layers of Fear, the indie first person point-and-click adventure game?\nOutput: confirm(name[Layers of Fear], genres[adventure, indie, point-and-click], player_perspective[first person])  \n\nExample 6) \nSentence: I bet you like it when you can play games on Steam, like Worms: Reloaded, right?  \nOutput: suggest(name[Worms: Reloaded], available_on_steam[yes])\n\nExample 7)\nSentence: I recall you saying that you really enjoyed The Legend of Zelda: Ocarina of Time. Are you typically a big fan of games on Nintendo rated E (for Everyone)?    \nOutput: verify_attribute(name[The Legend of Zelda: Ocarina of Time], esrb[E (for Everyone)], rating[excellent], platforms[Nintendo])\n\nExample 8)\nSentence: So what is it about the games that were released in 2005 that you find so excellent?  \nOutput: request_explanation(release_year[2005], rating[excellent])\n\nExample 9)\nSentence: Do you think Mac is a better gaming platform than others?\nOutput: request_attribute(has_mac_release[])\n\nGive the output for the following sentence:\n{input}\n\"\"\"\nThe typical responses for the above template are not perfect but it can be used for generating structured output for the full dataset.\nGround Truth: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[simulation, sport])\nGPT Response: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[sports, simulation])\n\n\nGround Truth: request(player_perspective[side view], specifier[easy])\nGPT Response: Output: request(genres[side view], rating[top], specifier[easy])\n\n\nGround Truth: recommend(name[Need for Speed: The Run], platforms[Xbox])\nGPT Response: confirm(name[Need for Speed: The Run], platforms[Xbox])\n\n\nEvaluation criteria\nOften, you require custom evaluation criteria for custom tasks. For structured functional representation extraction, I define the following binary criteria:\n\nFunction Name Match: The function name must match the ground truth function name.\nFunction and Attributes Match: The generated function name and attributes must match the ground truth function attributes. However, the order of the attributes does not matter.\nFunction, Attributes, and Values Match: The generated function name, attributes, and values must match the ground truth function attributes and values. The order of the attributes and values does not matter.\nExact Match: The generated function must exactly match the ground truth function.\n\nThe above criteria are in order of increasing strictness. The first criterion is the least strict, and the last criterion is the most strict. These criteria will help me evaluate the model‚Äôs performance.\n\n\nEvaluation Strategy\nAlthough not ideal, I ask the model to evaluate its own performance on the given task. This approach has limitations, as it could potentially introduce bias in the evaluation process. However, it serves as a starting point for our analysis. I ask the model to compare the generated function with the ground truth function and provide a boolean score based on the above evaluation criteria.\nSince, I need the evaluation scores in a more structured format to analyze the model‚Äôs performance effectively. I use pydantic and instructor packages to obtain the evaluation scores in a structured format. I use the following prompt and pydantic model to evaluate the GPT-4o performance:\nclass EvaluateFunctionRepresentation(BaseModel):\n    function_match: bool = Field(description=\"The function name is the same but the attributes and values can be different.\")\n    function_attribute_match: bool = Field(description=\"The function and the attributes are the same but the values can be different.\")\n    function_attributes_values_match: bool = Field(description=\"The generated representation has same function and attributes and corresponding values without the same attributes and values order.\")\n    exact_match: bool = Field(description=\"The generated representation is exactly the same as the ground truth representation.\")\n\nPROMPT_TEMPLATE_SCORE_EVAL = \"\"\"I will provide you with two functional representations strings. One will be the ground truth representation (ground_truth) and the other will be a generated representation (generated). You need to compare the generated representation with the ground truth representation and provide the following similarity match in true or false:\n1) function_match\n2) function_attributes_match\n3) function_attributes_values_match\n4) exact_match\n\nA typical functional representation is of this form: function(attribute1[values], attribute2[values], attribute3[values], ...). \n\nGiven the following two functional representation, provide the similarity scores for the following:\n\nground_truth: {ground_truth}\n\ngenerated: {generated}\n\nLet's think step by step.\n\"\"\"\nPlease go through the GPT-4o baseline evaluation notebook for more details on prompt template and evaluation process and responses.\n\n\nEvaluating the performance\nUsing the above evaluation strategy, I generate the average evaluation scores for the full dataset.\n\n\n\nGPT-4o Evaluation Metrics\n\n\nThe task of generating functional representations from natural language sentences is challenging as seen in the above scores. The best the model can do is to provide the exact match only for 30% of the examples. Even the correct function name evaluation score is only about 80%. These results indicate that while GPT-4o shows some capability in extracting functional representations, there‚Äôs significant room for improvement.\n\n\nConclusion\nThe above evaluation exercise provides a good baseline and useful evaluation criteria/metrics that can be used to assess the performance of other models on the same task in the future. Generating functional representations is a complex task and is not easily accomplished using prompt-engineering techniques like few-shot learning as seen in the results.\n\n\nNext steps‚Ä¶\n\nEvaluate other large models like Claude, Gemini, and Llama-3‚Äì70B on the same task to compare their performance. This will provide a broader perspective on how different LLMs handle structured data extraction.\nExplore alternative evaluation methods that don‚Äôt rely on the model evaluating itself. Most importantly, to eliminate potential biases in the evaluation process.\nFine-tune smaller models like Llama-3‚Äì8B or Mistral-7B for this domain-specific structured representation task. Fine-tuning will not only improve the model‚Äôs performance but also enhance latency and reduce the number of input tokens required to generate the output.\nInvestigate the impact of different prompt engineering techniques on the model‚Äôs performance.\n\n\n\nReferences\n\nGPT-4o baseline evaluation notebook\nOpenAI GPT-4o\nViGGO Dataset\nAnyscale Blog on Fine-tuning Llama 2\nPydantic Documentation\nInstructor Package\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "",
    "text": "Last week, I published the second blog in my LLM fine-tuning series, comparing various models performance in functional representation extraction.\nIn this third part of the series, I discuss the first steps toward fine-tuning an (open-source)-LLM for functional representation extraction. My aim is to give you all a sneak peek at the kind of performance you can expect from fine-tuning an LLM for a custom task. To streamline this step (and to satisfy my own curiosity üòä), I will use Predibase. It is a fast, cheap, and efficient open-source LLM fine-tuning and deployment platform."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#task-and-dataset",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#task-and-dataset",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Task and Dataset",
    "text": "Task and Dataset\nSimilar to my previous blogs, the custom task is to predict the structured functional representation from the given text video game opinions of the ViGGO validation dataset.\nTo make this exercise interesting and challenging for future experiments, I will use a maximum of 1000 examples for fine-tuning any LLM model, instead of the full ~5K train dataset.\nBelow is an example from the randomly selected 1K train dataset:\nText                      : I remember you saying that you loved The Room. Do you tend to enjoy PC games from 2012?\nfunctional_representation : verify_attribute(name[The Room], release_year[2012], rating[excellent], platforms[PC])\nAs shown in the graph below, the selected 1K dataset is a fairly representative sample of the full ViGGO train dataset.\n\n\n\nUnderstanding Data Distribution"
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#upload-the-dataset-to-predibase",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#upload-the-dataset-to-predibase",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Upload the dataset to Predibase",
    "text": "Upload the dataset to Predibase\nPredibase requires you to upload the instruction fine-tuning dataset in particular format. This is from Predibase docs:\n\nFor instruction fine-tuning, your dataset must contain two columns named prompt and completion: - prompt: Your input prompt. It serves as the starting point or the guiding information for the model. - completion: The expected response that corresponds to the input provided in the ‚Äúprompt‚Äù column. - split (optional): Should be either train or evaluation. To learn more, check out this section.\n\nMake sure to add the prompt template to the examples and convert them to the correct format. For this exercise, I use the following prompt template:\nprompt_template = \"\"\"Given a target sentence convert it structured functional representation.\n\n### Target sentence: {text}\n\n### Output Functional representation:\n\"\"\"\nYou can connect your dataset to Predibase via the UI or Python SDK. Here, I will upload the dataset using SDK.\n# Initialize Predibase client\npb = Predibase(api_token=os.environ[\"PREDIBASE_API_TOKEN\"])\n\n# Upload the dataset\ndataset = pb.datasets.from_file(\"viggo_train_val_dataset_1K.csv\", \n                                name=\"viggo_train_val_dataset_1K\")\nOnce uploade, you can check the uploaded dataset on the Predibase UI.  \nFor detailed steps on uploading the dataset to Predibase, please refer to the companion blog notebook."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#setup-and-finetune",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#setup-and-finetune",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Setup and Finetune",
    "text": "Setup and Finetune\nOnce you have uploaded the dataset, running the fine-tuning process is refreshingly simple. For this example, I fine-tune the base llama-3-8b model with the following parameters: epochs=3, rank=16, and learning_rate=2e-4.\n# Create an adapter repository\nrepo = pb.repos.create(name=\"viggo-finetune-1K\", \n                description=\"Llama-3-8b adapter repository for viggo 1K examples\"\n                )\n\n# Create and run the fine-tuning job\nadapter = pb.adapters.create(\n   config=FinetuningConfig(\n       base_model=\"llama-3-8b\",\n       epochs=3,\n       rank=16,\n       learning_rate=0.0002,\n   ),\n   dataset=dataset,\n   repo=repo,\n   description=\"baseline-llama-3-8b\",\n)\nThat‚Äôs all you need to do to submit a job! Once completed, it will be available on the Predibase platform.\n \nYou can always tweak multiple hyperparameters (see Finetuning Config) and run the fine-tune job again. All your fine-tune jobs will be available on the Predibase platform."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#evaluate-the-fine-tuned-model",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#evaluate-the-fine-tuned-model",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Evaluate the Fine-tuned Model",
    "text": "Evaluate the Fine-tuned Model\nPredibase provides both popular Serverless endpoints and Dedicated deployments options for opens-source LLMs and their fine-tuned LORA checkpoints. I will create serverless endpoint for this case.\n\nNote, atleast for now, serverless deployments are available for free.\n\n\nGenerate the responses for validation dataset\nSimilar to my previous blogs, I will evaluate the finetuned model on ViGGO validation dataset and calculate custom performance metrics metrics for a better understanding of finetuned model performance.\nFirst, I generate the responses for the validation dataset:\n# Initialize the Predibase deployment client\nlorax_client = pb.deployments.client(\"llama-3-8b\")\n\n# Load the validation dataset\nviggo_dataset = load_dataset(\"GEM/viggo\")\nval_dataset = viggo_dataset['validation']\n\n# finetuned adapter id\nadapter_id = \"viggo-finetune-1K/2\" \n\nresponses_dict = {}\nfor idx in range(len(val_dataset)):\n    if idx % 50 == 0: print(f\"Processing {idx}/{len(val_dataset)}\")\n    output = lorax_client.generate(prompt_template.format(text=val_dataset[\"target\"][idx]), adapter_id=\"viggo-finetune-1K/2\", max_new_tokens=150).generated_text\n    ground_truth = val_dataset[\"meaning_representation\"][idx]\n    text = val_dataset[\"target\"][idx]\n    responses_dict[idx] = {\"output\": output, \"ground_truth\": ground_truth, \"text\": text}\nNote: Remember to replace ‚Äúviggo-finetune-1K/2‚Äù with the correct adapter ID. You can find the adapter ID in the Predibase dashboard.\nNow, I can generate the evaluation scores using custom evaluation metrics and compare them with previously calculated GPT-4 and Claude 3.5 Sonnet scores:\n\n\n\nplot-finetuned-model\n\n\nThe initial finetuning of LLaMA-3-8B using 1,000 random examples from the ViGGO dataset, while not surpassing GPT-4 and Claude 3.5 Sonnet, shows promising results and outperforms several models from our previous blog. Notably, the exact_match score is even better than that of the two best-performing models."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#improved-performance-with-updated-prompt-template",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#improved-performance-with-updated-prompt-template",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Improved Performance with Updated Prompt Template",
    "text": "Improved Performance with Updated Prompt Template\nA simple yet effective way to enhance the model‚Äôs performance is by refining the prompt template. By providing clearer instructions that convey the structure of the functional representation, we can guide the model to produce more accurate outputs.\nI updated the prompt template as follows:\nprompt_template = \"\"\"Given a target sentence construct the underlying meaningful functional representation of the input sentence as a single function with attributes and attribute values.\n\n### Target sentence: {text}\n\n### Output Functional representation:\n\"\"\"\nAfter uploading this new dataset, finetuning the model, and evaluating it with the new adapter viggo-finetune-1K/3, there is significantly improved evaluation metrics. Notably, the model now surpasses GPT-4o‚Äôs scores for exact_match and function_name_match.\n\n\n\nplot-finetuned-model\n\n\nThis improvement highlights the importance of clear and specific instructions in prompt engineering, even when working with finetuned models."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#conclusions..",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#conclusions..",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Conclusions..",
    "text": "Conclusions..\n\nFirst of all, My overall experience with Predibase has been positive, particularly in terms of rapid finetuning of models. While there are some limitations such as restricted hyperparameter tuning, standardized dataset format, and inability to download adapters in the developer tier, it offers a user-friendly platform for fine-tuning (LORA) large language models. I was able to quickly upload, setut, finetune and infer the llm models.\nI achieve out-of-the-box performance using only random 1K examples. Although the fine-tuned llama-3-8b model doesn‚Äôt match the performance of GPT-4 and Sonnet 3.5 on all metrics. This demonstrates the potential of fine-tuning with limited data, highlighting the efficiency of the approach for task-specific model adaptation."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#next-steps",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#next-steps",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Next steps‚Ä¶",
    "text": "Next steps‚Ä¶\n\nMy next goal is to further enhance the model‚Äôs performance on evaluation metrics while maintaining a limit of 1,000 training examples. LIMA: Less Is More for Alignment paper has demonstrated in the past that even 1,000 well-curated examples can lead to strong finetuning performance.\nCareful curated selection of examples and hyerparameters will definitely improve the performance benchmarks on evaluation metrics.\nIn addition to it, I will deep dive into one of my favorite LLM fine-tuning tools, Axolotl."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#references",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#references",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "References",
    "text": "References\n\nPart II blog post of series\nNotebook I: Analyze_Viggo_Dataset.ipynb\nNotebook II: Finetune_llama-3-8b_Predibase.ipynb\nPredibase Platform\nLORA blog I: Finetuning Large Language Models (LLMs)\nLORA blog II: LLM Research Insights: Instruction Tuning & Training Paradigms\nLlama 3\nLIMA: Less Is More for Alignment\nAxolotl: A Framework for Fine-tuning LLMs\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "",
    "text": "The Transformer architecture has become the workhorse behind modern LLMs. GPT-2/3/4/5, Llama, Claude, Gemini: they all are built on top of the same core architecture or its variants from the 2017 ‚ÄúAttention Is All You Need‚Äù paper. I wanted to understand this architecture properly, so I followed Andrej Karpathy‚Äôs ‚ÄúLet‚Äôs Build GPT from Scratch‚Äù video. It‚Äôs a 2-hour walkthrough where you start from an empty file and end up with a working Transformer.\nI followed Karpathy‚Äôs video and captured each architectural addition as a separate commit. This let me see exactly how each component pulled down the validation loss. In this walkthrough, the training data is ~1M characters of Shakespeare and the goal is to generate Shakespeare-like text.\nYou can find all the code and notebooks in the repo: building-from-scratch/basic-gpt"
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#baseline-bigram-model-e0b5864",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#baseline-bigram-model-e0b5864",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "Baseline: Bigram Model (e0b5864)",
    "text": "Baseline: Bigram Model (e0b5864)\nKarpathy starts with the simplest possible language model: a bigram model. It predicts the next character based only on the current character. No context at all. The tokens aren‚Äôt talking to each other.\nThis still works somewhat because some characters naturally follow others (the letter ‚Äòq‚Äô is almost always followed by ‚Äòu‚Äô). But the output is complete gibberish because the model has no way to look at what came before.\nResult: ~2.49 validation loss."
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#update-1-self-attention-7b0e03a",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#update-1-self-attention-7b0e03a",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "Update 1: Self-Attention (7b0e03a)",
    "text": "Update 1: Self-Attention (7b0e03a)\nWe want tokens to communicate with each other and predictions to consider context from previous tokens, not just the current one. A token at position 5 should be able to look at tokens 1-4 and gather information from them. But at the same time, it can‚Äôt look at tokens 6, 7, 8 because those are the future we‚Äôre trying to predict.\nSelf-attention solves this. Every token is represented by 3 vectors: - Query: ‚ÄúWhat am I looking for?‚Äù - Key: ‚ÄúWhat do I contain?‚Äù\n- Value: ‚ÄúIf you find me interesting, here‚Äôs what I‚Äôll tell you.‚Äù\nThe query dot-products with all the keys. High dot product means high affinity: ‚ÄúI find you interesting.‚Äù The values of interesting tokens get aggregated via weighted sum.\n\nNote: Attention is really a communication mechanism. You can think of it as nodes in a directed graph where every node aggregates information from nodes that point to it. In our case, token 5 can receive information from tokens 1-4 (and itself), but not from tokens 6-8. The triangular mask creates this directed structure and is what makes this a ‚Äúdecoder‚Äù block.\n\nOne subtle but important point: attention has no notion of space. The tokens don‚Äôt inherently know where they are in the sequence. That‚Äôs why we add positional embeddings. Each position gets its own learned embedding that‚Äôs added to the token embedding, giving the model spatial information.\nResult: ~2.4 validation loss. Tokens can now see context."
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#update-2-multi-head-attention-9d2a7b5",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#update-2-multi-head-attention-9d2a7b5",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "Update 2: Multi-Head Attention (9d2a7b5)",
    "text": "Update 2: Multi-Head Attention (9d2a7b5)\nTokens have a lot to talk about. One head might look for consonants, another for vowels, another for word boundaries, another for patterns at specific positions. Having multiple independent communication channels lets the model gather diverse types of data in parallel.\n\nNote: This is similar to grouped convolutions. Instead of one large convolution, you do it in groups. With 4 heads of 8 dimensions each, we get the same total dimensionality (32) but with 4 separate communication channels. Each head can specialize in different patterns.\n\n\n\n\nMulti-Head Attention\n\n\nResult: ~2.28 validation loss."
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#update-3-feed-forward-network-c4c46ff",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#update-3-feed-forward-network-c4c46ff",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "Update 3: Feed-Forward Network (c4c46ff)",
    "text": "Update 3: Feed-Forward Network (c4c46ff)\nThe FFN layer addresses a key problem. Until now, ‚Äúthe tokens looked at each other but didn‚Äôt have enough time to think about what they found.‚Äù\nSelf-attention is the communication phase. Tokens gather data from each other. But then they need to compute on that data individually. That‚Äôs what the feed-forward network does. It operates on a per-token level. All the tokens process their gathered information independently.\n\n\n\nFeed-Forward Network\n\n\nSo the Transformer block becomes: communicate (attention) ‚Üí compute (feed-forward). This pattern repeats for every layer.\nResult: ~2.27 validation loss. The architecture now has both communication and computation."
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#update-4-residual-connections-0239c07",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#update-4-residual-connections-0239c07",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "Update 4: Residual Connections (0239c07)",
    "text": "Update 4: Residual Connections (0239c07)\nThis is one of two optimizations that make deep networks actually trainable. Without it, stacking many layers leads to vanishing gradients and optimization difficulties.\nKarpathy visualizes it nicely: imagine a residual pathway running from top to bottom. You can ‚Äúfork off‚Äù from this pathway, do some computation, and project back via addition. The path from inputs to outputs is just a series of additions.\n\nNote: Why does this help? During backpropagation, addition distributes gradients equally to both branches. The gradients ‚Äúhop‚Äù through every addition node directly to the input. This creates a ‚Äúgradient superhighway‚Äù from supervision to input, unimpeded. The residual blocks are initialized to contribute very little at first, then ‚Äúcome online‚Äù over time during optimization.\n\nResult: ~2.09 validation loss. Now we can stack layers without vanishing gradients."
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#update-5-6-layer-normalization-63ef5f8-4f5bef8",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#update-5-6-layer-normalization-63ef5f8-4f5bef8",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "Update 5 & 6: Layer Normalization (63ef5f8, 4f5bef8)",
    "text": "Update 5 & 6: Layer Normalization (63ef5f8, 4f5bef8)\nBatch normalization normalizes columns (across examples in a batch). Layer normalization normalizes rows (across features for each example). The implementation is almost identical, you just change which dimension you normalize over.\nLayer norm has advantages for Transformers: - No dependency on batch size (works even with batch size 1) - No running buffers to maintain - No distinction between training and test time\nThe original Transformer paper used post-layer norm (normalize after attention/FFN). Modern implementations use pre-layer norm (normalize before). Pre-layer norm creates a cleaner residual pathway since the transformation happens on normalized inputs, leading to more stable training.\nResult: ~2.076 validation loss."
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#update-7-scaling-up-d4141d7",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#update-7-scaling-up-d4141d7",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "Update 7: Scaling Up (d4141d7)",
    "text": "Update 7: Scaling Up (d4141d7)\nWith all the architectural pieces in place, Karpathy scales up the architecture:\n\n\n\nParameter\nBefore\nAfter\n\n\n\n\nBlock size (context)\n8\n256\n\n\nEmbedding dim\n32\n384\n\n\nHeads\n4\n6\n\n\nLayers\n3\n6\n\n\nDropout\n0\n0.2\n\n\n\nDropout is added for regularization. It randomly shuts off neurons during training, effectively training an ensemble of sub-networks. At test time, everything is enabled and the sub-networks merge.\nResult: ~1.48 validation loss. The generated text now looks like Shakespeare (structure, dialogue formatting, character names) even though it‚Äôs nonsensical when you actually read it."
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#how-this-compares-to-gpt-3",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#how-this-compares-to-gpt-3",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "How This Compares to GPT-3",
    "text": "How This Compares to GPT-3\n\n\n\n\nMy Model\nGPT-3\n\n\n\n\nParameters\n~10M\n175B\n\n\nDataset\n~300K tokens\n300B tokens\n\n\nArchitecture\nNearly identical\nNearly identical\n\n\n\nThe architecture we built is essentially the same as GPT-3. The difference is pure scale: 17,500x more parameters trained on 1 million times more data. By today‚Äôs standards, even GPT-3‚Äôs 300B tokens is considered modest. Current models train on 1T+ tokens.\nThis is what makes the Transformer architecture so remarkable. The same fundamental design (attention for communication, feed-forward for computation, residual connections, layer norm) scales from a 10M parameter Shakespeare generator to a 175B parameter model!"
  },
  {
    "objectID": "posts/2025-09-08-building-gpt-from-scratch.html#resources",
    "href": "posts/2025-09-08-building-gpt-from-scratch.html#resources",
    "title": "Building GPT from Scratch: Following Karpathy‚Äôs Tutorial",
    "section": "Resources",
    "text": "Resources\n\nCode: building-from-scratch/basic-gpt\nVideo: Let‚Äôs Build GPT from Scratch by Andrej Karpathy"
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html",
    "href": "posts/2025-12-25-deriving-ppo-loss.html",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "",
    "text": "I have been trying to wrap my head around reinforcement learning methods like DPO, GRPO, and RLVR for a while now, especially with all the recent work showing how effective they can be for LLM post-training. Since I amm still pretty new to RL, I figured the best place to start was Proximal Policy Optimization (PPO), the algorithm OpenAI used to show how reinforcement learning could meaningfully improve LLM alignment (InstructGPT paper). My hope is that getting comfortable with PPO will give me the right mental model for the policy-gradient side of things and make it easier to understand the newer LLM-specific RL methods built on similar ideas.\nIf you start learning RL, you quickly realize it involves a lot of math! So I decided to lean into that and do a few (possibly annoying) derivation sessions to really understand the PPO objective by building it up from first principles, similar to how Umar Jamil does in his video.\nBelow is my attempt at the derivation based on the original PPO and InstructGPT papers and Umar Jamil‚Äôs video."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#i-reinforcement-learning-core-definitions",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#i-reinforcement-learning-core-definitions",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "I: Reinforcement Learning: Core Definitions",
    "text": "I: Reinforcement Learning: Core Definitions\n\n\n\n\n\n\n\n\nConcept\nGeneral RL Definition\nLLM Context (RLHF)\n\n\n\n\nReinforcement Learning\nA learning setup where an agent learns to act in an environment to maximize expected cumulative reward.\nFine-tuning a language model to generate responses that better match human preferences using reward-based feedback.\n\n\nEnvironment\nEverything outside the agent that it interacts with and that produces observations and rewards.\nThe prompt distribution and interaction loop and the reward signal from a reward model evaluating generated responses.\n\n\nAgent\nThe learner/decision-maker that observes states, takes actions, and receives rewards.\nThe language model generating text token by token.\n\n\nAction (\\(a\\))\nA choice made by the agent, usually conditioned on the state \\(s\\).\nPicking the next token at each step of generation.\n\n\nState (\\(s\\))\nThe information available to the agent at a given time step.\nThe prompt plus the response generated so far (the current token context).\n\n\nReward (\\(r\\))\nA scalar signal telling the agent how good or bad an outcome was.\nA score from the reward model (trained on preference data) that judges how good or bad a response is.\n\n\nPolicy (\\(\\pi\\))\nA stochastic mapping from states to a distribution over actions.\nThe model‚Äôs probability distribution over the next token given the context.\n\n\nGoal\nFind an optimal policy \\(\\pi^*\\) that maximizes expected cumulative reward over time.\nUpdate (align) the model so it tends to generate responses with higher reward-model scores."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#ii-reward-model-in-rlhf-for-llms",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#ii-reward-model-in-rlhf-for-llms",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "II: Reward Model in RLHF for LLMs",
    "text": "II: Reward Model in RLHF for LLMs\nA Reward Model (RM) is a neural network that takes a prompt \\(x\\) and a response \\(y\\) as input and outputs a scalar reward \\(r_\\phi(x, y) \\in \\mathbb{R}\\) indicating how ‚Äúgood‚Äù or ‚Äúaligned‚Äù that response is according to human preferences.\nPolicy-gradient methods (including PPO) require a scalar objective to update the policy parameters. In standard RL, the environment provides this signal. However for language generation, there is no natural environment giving us rewards for ‚Äúgood‚Äù responses. Having humans rate every output is impractical and for gradient-based optimization, we need a differentiable scalar signal to backpropagate through. Thus, we require a cheap, differentiable proxy for human preferences during RL training. A learned RM provides exactly this.\n\nHow is the Reward Model Trained?\nThe standard procedure for training the reward model is:\n\nSample prompts (\\(x\\))\nGenerate multiple candidate completions (\\(y_1, y_2, \\ldots, y_K\\)) from a baseline policy (often an SFT model).\nAsk humans to compare candidates (pairwise preferences are easier than absolute scoring).\nTrain the RM (\\(r_\\phi\\)) to predict those preferences.\n\nArchitecturally, the reward model is typically: - Initialized from a pretrained language model (often the SFT model itself) - The final non-embedding layer (which projects to vocabulary) is removed - Replaced it with a linear layer that projects the hidden state of the last token to a single scalar output\n\n\nReward Model Loss Function\nThe reward model is trained using the Bradley-Terry model for pairwise comparisons. The probability that response \\(y_w\\) (preferred) is preferred over \\(y_l\\) (less preferred) for any prompt \\(x\\) is modeled as:\n\\[\nP(y_w \\succ y_l | x) = \\sigma\\left(r_\\theta(x, y_w) - r_\\theta(x, y_l)\\right) \\tag{II.I}\n\\]\nwhere \\(\\sigma\\) is the sigmoid function: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\nThe negative log-likelihood loss is:\n\\[\n\\mathcal{L}_{\\text{RM}}(\\phi) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left(r_\\phi(x, y_w) - r_\\phi(x, y_l)\\right) \\right]\n\\]\nOne can verify that this loss forces the reward model to assign higher rewards to preferred responses (see InstructGPT paper or Umar Jamil‚Äôs video for a detailed walkthrough).\nThere are two key insights here: 1. We don‚Äôt need absolute scores, we only need the reward model to correctly rank responses. 2. The loss depends only on differences (\\(r_\\phi(x, y_w) - r_\\phi(x, y_l)\\)), so it is invariant to adding a constant to all rewards. This will be useful later when we discuss the PPO loss.\nThe reward model serves as a learned proxy for human preferences, converting the intractable problem of getting human feedback on every generation into a tractable supervised learning problem. Once trained, it provides the scalar signal \\(r_\\phi(x, y)\\) needed to optimize our policy (LLM) using rl algorithms like PPO."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#iii-trajectories-and-returns",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#iii-trajectories-and-returns",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "III: Trajectories and Returns",
    "text": "III: Trajectories and Returns\n\nTrajectory\nA trajectory (also called a rollout or episode) is a sequence of states (\\(s\\)), actions (\\(a\\)), and rewards (\\(r\\)) generated by an agent interacting with an environment:\n\\[\n\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\ldots, s_T, a_T, r_T)\n\\]\nIn the context of LLMs, a trajectory corresponds to the entire sequence of token generations. It is the prompt followed by all generated tokens until the end-of-sequence token.\nNote that the states are always stochastically modeled, and \\(s_{t+1}\\) can be represented as \\(s_{t+1} \\sim P(s_{t+1} | s_t, a_t)\\). Given a stochastic policy \\(\\pi_\\theta (a_t | s_t)\\), the probability of a trajectory \\(\\tau\\) is the product of: 1. The initial state distribution \\(\\rho_0(s_0)\\) 2. The stochastic policy \\(\\pi_\\theta (a_t | s_t)\\) 3. The environment transition dynamics \\(P(s_{t+1} | s_t, a_t)\\)\n\\[\nP(\\tau | \\pi_\\theta) = \\rho_0(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t | s_t) \\cdot P(s_{t+1} | s_t, a_t) \\tag{III.I}\n\\]\n\n\nReturn\nThe return is the cumulative reward collected over the full trajectory (\\(\\tau\\)). The simplest form is the undiscounted return:\n\\[\nR(\\tau) = \\sum_{t=0}^{T} r_t\n\\]\nMore generally, we use the discounted return:\n\\[\nR(\\tau) = \\sum_{k=0}^{\\infty} \\gamma^k r_{k} = r_0 + \\gamma r_{1} + \\gamma^2 r_{2} + \\cdots \\tag{III.II}\n\\]\nwhere \\(\\gamma \\in [0, 1]\\) is the discount factor. The discount factor \\(\\gamma\\) serves a couple of purposes: 1. It ensures the return is finite for infinite-horizon tasks (\\(T\\to\\infty\\)). 2. It prioritizes immediate rewards over distant ones."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#iv-policy-gradient-optimization-and-reinforce-algorithm",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#iv-policy-gradient-optimization-and-reinforce-algorithm",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "IV: Policy Gradient Optimization and REINFORCE Algorithm",
    "text": "IV: Policy Gradient Optimization and REINFORCE Algorithm\nThe goal of reinforcement learning is to find a policy \\(\\pi_\\theta\\) that maximizes the expected return over all possible trajectories:\n\\[\n\\boxed{J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]} \\tag{IV.I}\n\\]\nThis is our objective function and we want to find parameters \\(\\theta^*\\) such that:\n\\[\n\\theta^* = \\arg\\max_\\theta J(\\theta)\n\\]\nTo maximize \\(J(\\theta)\\) using gradient-based methods, we need to compute \\(\\nabla_\\theta J(\\theta)\\) and perform gradient ascent:\n\\[\n\\boxed{\\theta_{k+1} = \\theta_k + \\alpha \\left. \\nabla_\\theta J(\\pi_\\theta) \\right|_{\\theta_k}} \\tag{IV.II}\n\\]\nThis policy gradient looks simple in equation form but it is intractable to compute. The expectation is over trajectories sampled from \\(\\pi_\\theta\\), which itself depends on \\(\\theta\\). We can‚Äôt simply enumerate all possible trajectories. This is computationally intractable for any reasonably sized state-action space (and certainly not possible for LLMs!).\nThus, as a next step we need to derive some sort of reasonable and tractable approximation for \\(\\nabla_\\theta J(\\theta)\\). We do this by using the log-derivative trick.\n\\[\n\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]\n\\]\nThis expectation can be written as an integral: \\[\n= \\nabla_\\theta \\int_\\tau P(\\tau | \\theta) R(\\tau) \\, d\\tau\n\\]\nBringing the gradient inside the integral: \\[\n= \\int_\\tau \\nabla_\\theta P(\\tau | \\theta) R(\\tau) \\, d\\tau\n\\]\nNow we apply the log-derivative trick: \\[\n\\nabla_\\theta \\log P(\\tau | \\theta) = \\frac{\\nabla_\\theta P(\\tau | \\theta)}{P(\\tau | \\theta)}\n\\]\nRearranging: \\(\\nabla_\\theta P(\\tau | \\theta) = P(\\tau | \\theta) \\nabla_\\theta \\log P(\\tau | \\theta)\\) and substituting back, we get: \\[\n= \\int_\\tau P(\\tau | \\theta) \\nabla_\\theta \\log P(\\tau | \\theta) R(\\tau) \\, d\\tau\n\\]\nwhich can also be written as the following expectation:\n\\[\n\\boxed{\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[ \\nabla_\\theta \\log P(\\tau | \\theta) \\cdot R(\\tau) \\right]} \\tag{IV.III}\n\\]\nNote, here the gradient is now the expectation of the gradient of the log-probability of the trajectory. This can further be simplified by using the trajectory probability expression (III.I): \\[\nP(\\tau | \\theta) = \\rho_0(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t | s_t) \\cdot P(s_{t+1} | s_t, a_t)\n\\]\nTaking the log:\n\\[\n\\log P(\\tau | \\theta) = \\log \\rho_0(s_0) + \\sum_{t=0}^{T-1} \\log \\pi_\\theta(a_t | s_t) + \\sum_{t=0}^{T-1} \\log P(s_{t+1} | s_t, a_t)\n\\]\nWhen we take \\(\\nabla_\\theta\\), only the policy term depends on \\(\\theta\\):\n\\[\n\\nabla_\\theta \\log P(\\tau | \\theta) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n\\]\nThe initial state distribution and transition dynamics are independent of \\(\\theta\\), so their gradients vanish. Substituting back, we obtain the policy gradient theorem:\n\\[\n\\boxed{\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot R(\\tau) \\right]} \\tag{IV.IV}\n\\]\nThis is a remarkable result. We can compute the gradient of our objective without differentiating through the environment dynamics and only need gradients of the log-probabilities of our policy.\nSince we cannot compute the expectation exactly, we approximate it with a sample mean by sampling \\(N\\) trajectories:\n\\[\n\\boxed{\\nabla_\\theta J(\\theta) \\approx \\hat{g} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\right) R(\\tau_i)} \\tag{IV.V}\n\\]\nThis gives us the REINFORCE algorithm:\n\nInitialize: Start with a pretrained or supervised fine-tuned (SFT) language model \\(\\pi_\\theta\\)\nSample prompts: Draw a batch of \\(N\\) prompts \\(\\{x_1, x_2, \\ldots, x_N\\}\\) from a dataset\nGenerate trajectories: For each prompt \\(x_i\\), generate a response \\(y_i = (a_0, a_1, \\ldots, a_T)\\) by sampling tokens from the policy \\(\\pi_\\theta\\). Each trajectory is the sequence of states (prompt + generated tokens so far) and actions (selected tokens).\nCompute log-probabilities: For each trajectory, compute the log-probability of each generated token given its context: \\[\\log \\pi_\\theta(a_t | s_t) \\quad \\text{for } t = 0, 1, \\ldots, T\\]\nCompute rewards: Score each complete (prompt, response) pair using the reward model: \\[R(\\tau_i) = r_\\phi(x_i, y_i)\\]\nEstimate policy gradient: Compute the gradient estimate using (IV.V): \\[\\hat{g} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\right) R(\\tau_i)\\]\nUpdate policy: Perform a gradient ascent step: \\[\\theta \\leftarrow \\theta + \\alpha \\hat{g}\\]\nRepeat: Go back to Step 2 and iterate until convergence\n\nWhile REINFORCE provides an unbiased gradient estimate, it suffers from two critical issues that make it impractical for LLM training:\n\nHigh Variance: The gradient estimate \\(\\hat{g}\\) suffers from high variance depending on the sampled trajectories. This variance can be large and can lead to noisy gradients and unstable training. &gt; If you look again at (IV.V), the gradient estimate for each action is weighted by the return of the entire trajectory \\(R(\\tau)\\). This means that even if an action was good, it might receive a negative gradient update simply because other actions in the trajectory led to poor outcomes (or vice versa). Over many samples, the noise introduced by this coupling can be substantial, leading to high variance\nOn-Policy Constraint (Sample Inefficiency): REINFORCE requires trajectories sampled from the current policy \\(\\pi_\\theta\\). Thus after every gradient update, previously collected trajectories must be discarded and new ones need to be sampled from the updated policy. For LLMs, where each trajectory requires a full forward pass through a billion(s)-parameter model, this is prohibitively expensive especially when we need many small gradient steps to train effectively."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#v-reducing-variance-and-the-advantage-function",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#v-reducing-variance-and-the-advantage-function",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "V: Reducing Variance and the Advantage Function",
    "text": "V: Reducing Variance and the Advantage Function\nThe REINFORCE algorithm provides an unbiased gradient estimate (IV.V). However while unbiased, this estimator suffers from high variance.\n\nReplacing Full-Trajectory Return with Reward-to-Go (using causality)\nA first variance reduction comes from noticing that action \\(a_t\\) taken at time \\(t\\) cannot influence rewards that were received before time \\(t\\). This is a fundamental consequence of causality. These past reward terms contribute only noise to the gradient estimate and add variance without contributing any signal. Thus, we can remove them and consider only the rewards-to-go :\n\\[\n\\hat{R}_t = \\sum_{t'=t}^{T} r_{t'}\n\\]\nThis gives us a lower-variance estimator:\n\\[\n\\boxed{\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\cdot \\hat{R}_{i,t}} \\tag{V.I}\n\\]\nwhere \\(\\hat{R}_{i,t} = \\sum_{t'=t}^{T} r_{i,t'}\\) is the rewards-to-go for trajectory \\(i\\) starting from time \\(t\\).\n\n\nSubtracting a Baseline\nA second complementary technique for variance reduction is to subtract a baseline \\(b(s_t)\\) from the rewards. The key insight is that we can subtract any function that does not depend on the action from our reward signal without changing the expected value of the gradient.\nThus we can subtract a state-dependent baseline \\(b(s_t)\\) from our rewards-to-go to yield an unbiased gradient estimator:\n\\[\n\\boxed{\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\cdot \\left(\\hat{R}_{i,t} - b(s_{i,t})\\right)} \\tag{V.II}\n\\]\n\n\nValue Functions: \\(V^\\pi(s)\\) and \\(Q^\\pi(s, a)\\)\nThe baseline is still an arbitrary function. To make it more systematic and concrete, there are two fundamental functions from RL theory.\nState Value Function: The state value function \\(V^\\pi(s)\\) is the expected return when the agent is in state \\(s\\) and acts according to policy \\(\\pi\\):\n\\[\nV^\\pi(s) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\;\\middle|\\; s_0 = s\\right]  \\]\nIntuitively, \\(V^\\pi(s)\\) tells ‚ÄúHow good is this state on average?‚Äù and is used as a baseline \\(b(s) = V^\\pi(s)\\).\nAction Value Function (Q-function): The action value function \\(Q^\\pi(s, a)\\) is the expected return when starting in state \\(s\\) and taking action \\(a\\) and then acting according to policy \\(\\pi\\):\n\\[\nQ^\\pi(s, a) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\;\\middle|\\; s_0 = s, a_0 = a\\right]\n\\]\nIntuitively, \\(Q^\\pi(s, a)\\) tells ‚ÄúHow good is this specific action in this state?‚Äù and in RL, the rewards-to-go is estimated as \\(Q^\\pi(s, a)\\).\nIn the LLM context: - \\(V^\\pi(s)\\) estimates the expected reward for a given prompt + partial response, assuming the model continues generating according to its current policy. - \\(Q^\\pi(s, a)\\) estimates the expected reward if, from the current prompt + partial response, the model generates a specific next token \\(a\\) and then continues according to its policy.\n\n\nAdvantage Function\nThe advantage function \\(A^\\pi(s, a)\\) measures how much better (or worse) a specific action \\(a\\) is compared to the average action under the policy:\n\\[\n\\boxed{A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)} \\tag{V.III}\n\\]\nThe advantage function directly tells us: ‚ÄúHow much better is this particular action compared to what we would typically do in this state?‚Äù This is precisely the signal we want for policy improvement. We want to increase the probability of actions with positive advantage and decrease the probability of actions with negative advantage.\n\nFrom Umar Jamil‚Äôs video:\nIn the LLM context consider a state where the prompt is ‚ÄúWhere is Shanghai?‚Äù and the model has generated ‚ÄúShanghai is‚Äù. From this state: - If the model samples the token ‚Äúin‚Äù (leading toward ‚ÄúShanghai is in China‚Äù), this action likely has positive advantage. This is because it is better than the average token the model might produce. - If the model samples the token ‚Äúdelicious‚Äù (leading toward an incoherent response), this action likely has negative advantage. This is because it is worse than the average token the model might produce.\n\n\n\nAdvantage-Weighted Policy Gradient\nSubstituting the rewards-to-go and the value function as a baseline, we get the following form of the policy gradient: \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot (Q^\\pi(s_t, a_t) - V^\\pi(s_t))\\right]\n\\]\nwhich can be written as: \\[\n\\boxed{\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot A^{\\pi_\\theta}(s_t, a_t)\\right]} \\tag{V.IV}\n\\]\nand for sample-based approximation:\n\\[\n\\boxed{\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\cdot \\hat{A}_{i,t}} \\tag{V.V}\n\\]\nwhere \\(\\hat{A}_{i,t}\\) is an estimate of the advantage function at time \\(t\\) in trajectory \\(i\\). This is the form of the policy gradient often used.\nIn practice, \\(A^\\pi(s_t, a_t)\\) can be estimated as follows:\n\nLearn a value function: Train a neural network \\(V_\\phi(s)\\) (often called the ‚Äúcritic‚Äù or ‚Äúvalue head‚Äù) to approximate \\(V^\\pi(s)\\). In LLM fine-tuning, this is often a linear layer on top of the same transformer backbone used for the policy.\nEstimate \\(Q^\\pi\\) from samples: Given a trajectory, the rewards-to-go \\(\\hat{R}_t = \\sum_{t'=t}^{T} \\gamma^{t'} r_{t'}\\) provides an unbiased (but high-variance) estimate of \\(Q^\\pi(s_t, a_t)\\).\nCompute advantage estimates: \\(\\hat{A}_t = \\hat{R}_t - V_\\phi(s_t)\\)\n\nMore sophisticated methods like Generalized Advantage Estimation (GAE) interpolate between high-variance, low-bias estimates and low-variance, high-bias estimates by using a weighted combination of multi-step returns. See the GAE paper for more details."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#vi-importance-sampling-and-off-policy-policy-gradients",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#vi-importance-sampling-and-off-policy-policy-gradients",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "VI: Importance Sampling and Off-Policy Policy Gradients",
    "text": "VI: Importance Sampling and Off-Policy Policy Gradients\n\nNote: In RL literature, ‚Äúoff-policy‚Äù typically refers to methods where the behavior policy (generating data) is arbitrarily quite different from the target policy (being optimized) say where transitions from policies thousands of updates old are reused. In this section, what we will call ‚Äúoff-policy‚Äù should more precisely be called ‚Äúlocal off-policy‚Äù.\n\nThe advantage-weighted policy gradient (V.IV) requires trajectories sampled from the current policy \\(\\pi_\\theta\\). ‚Ä¶ The advantage-weighted policy gradient (V.IV) requires trajectories sampled from the current policy \\(\\pi_\\theta\\). This creates a fundamental inefficiency i.e., after each gradient update \\(\\theta \\to \\theta'\\) all previously collected trajectories become ‚Äústale‚Äù and we must discard these trajectories and sample new ones from the updated policy.\nFor LLMs, where each trajectory requires a full forward pass through billion(s)-parameter model, this is prohibitively expensive especially when we need many small gradient steps to train effectively.\nWe need a way to reuse the same trajectories for multiple gradient updates. Importance sampling provides the mathematical machinery to do exactly this!\n\nImportance Sampling\nImportance sampling is a technique for estimating expectations under one probability distribution using samples drawn from a different distribution. Consider an expectation for distribution \\(p(x)\\):\n\\[\n\\mathbb{E}_{x \\sim p}[f(x)] = \\int p(x) f(x) \\, dx\n\\]\nWe can rewrite this by multiplying and dividing by another distribution \\(q(x)\\) (with \\(q(x) &gt; 0\\) wherever \\(p(x) &gt; 0\\)):\n\\[\n= \\int q(x) \\frac{p(x)}{q(x)} f(x) \\, dx = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)} f(x)\\right]\n\\]\nThe ratio \\(\\frac{p(x)}{q(x)}\\) is called the importance weight. This identity tells us:\n\\[\n\\boxed{\\mathbb{E}_{x \\sim p}[f(x)] = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)} f(x)\\right]} \\tag{VI.I}\n\\]\nWe can now estimate the expectation under \\(p\\) using samples from \\(q\\) as long as we reweight each sample by the ratio of probabilities.\n\n\nApplying Importance Sampling to Policy Gradients\nWe can apply this technique to the policy gradient setting. The on-policy advantage-weighted gradient (V.IV) is:\n\\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot A^{\\pi_\\theta}(s_t, a_t)\\right]\n\\]\nTo apply importance sampling, we work at time-step level rather than trajectory level (full trajectory importance weights have extremely high variance). For a single timestep: \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s_t, a_t) \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot A^{\\pi_\\theta}(s_t, a_t)\\right]\n\\]\nUsing importance sampling with samples from \\(\\pi_{\\theta_{\\text{old}}}\\):\n\\[\n= \\mathbb{E}_{(s_t, a_t) \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot A^{\\pi_\\theta}(s_t, a_t)\\right]\n\\]\nNow we apply the log-derivative identity \\(\\nabla_\\theta \\log \\pi_\\theta = \\frac{\\nabla_\\theta \\pi_\\theta}{\\pi_\\theta}\\), which gives us a surrogate objective \\(L(\\theta)\\) whose gradient equals this importance-weighted policy gradient:\n\\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s_t, a_t) \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\nabla_\\theta \\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} A^{\\pi_{\\theta_{\\text{old}}}}(s_t, a_t)\\right]\n\\]\nwhere the importance-weighted surrogate objective also known as the Conservative Policy Iteration (CPI) objective is: \\[\nL^{\\text{CPI}}(\\theta) = \\mathbb{E}_{(s_t, a_t) \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} A^{\\pi_{\\theta_{\\text{old}}}}(s_t, a_t)\\right]\n\\]\nWe also define the probability ratio as:\n\\[\nr_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\tag{VI.II}\n\\]\nNote that \\(r_t(\\theta_{\\text{old}}) = 1\\) by construction. Thus, the CPI objective can be written as:\n\\[\n\\boxed{L^{\\text{CPI}}(\\theta) = \\mathbb{E}_t\\left[\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t\\right] = \\mathbb{E}_t\\left[r_t(\\theta) \\hat{A}_t\\right]} \\tag{VI.III}\n\\]\nwhere \\(\\hat{A}_t\\) is the estimated advantage at timestep \\(t\\), and \\(\\mathbb{E}_t[\\cdot]\\) denotes the empirical average over a batch of samples collected under \\(\\pi_{\\theta_{\\text{old}}}\\).\nThis objective has a clear interpretation: - If \\(\\hat{A}_t &gt; 0\\) (action better than average), we want to increase \\(r_t(\\theta)\\), i.e., make the new policy more likely to take this action. - If \\(\\hat{A}_t &lt; 0\\) (action worse than average), we want to decrease \\(r_t(\\theta)\\), i.e., make the new policy less likely to take this action.\nThe corresponding sample-based approximation is:\n\\[\n\\boxed{L^{\\text{CPI}}(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\frac{\\pi_\\theta(a_{i,t} | s_{i,t})}{\\pi_{\\theta_{\\text{old}}}(a_{i,t} | s_{i,t})} \\hat{A}_{i,t}} \\tag{VI.IV}\n\\]\n\n\nOff-Policy Learning: Reusing Trajectories\nThe CPI objective enables off-policy learning: we can sample trajectories from \\(\\pi_{\\theta_{\\text{old}}}\\), store them and then perform multiple gradient updates on \\(\\theta\\) using the same batch of data. The typical workflow becomes:\n\nCollect: Sample trajectories \\(\\{\\tau_i\\}\\) from the current policy \\(\\pi_{\\theta_{\\text{old}}}\\)\nCompute: Calculate advantages \\(\\hat{A}_{i,t}\\) and log-probabilities \\(\\log \\pi_{\\theta_{\\text{old}}}(a_{i,t} | s_{i,t})\\)\nStore: Save the trajectories along with their advantages and old log-probabilities\nOptimize: Perform multiple gradient ascent steps on \\(L^{\\text{CPI}}(\\theta)\\) using mini-batches from the stored data\nRepeat: Set \\(\\theta_{\\text{old}} \\leftarrow \\theta\\) and return to step 1\n\nThis dramatically improves sample efficiency. Instead of discarding trajectories after a single gradient step, we can extract multiple updates from each batch of expensive LLM rollouts.\n\n\nThe Instability Problem\nWhile the CPI objective improves sample efficiency, unconstrained optimization of \\(L^{\\text{CPI}}(\\theta)\\) is unstable. The core issue is that importance sampling becomes unreliable when \\(\\pi_\\theta\\) drifts far from \\(\\pi_{\\theta_{\\text{old}}}\\):\n\nExtreme probability ratios: The ratio \\(r_t(\\theta)\\) can become arbitrarily large or small, destabilizing gradient estimates.\nStale advantages: The estimates \\(\\hat{A}_t\\) were computed under \\(\\pi_{\\theta_{\\text{old}}}\\) and become inaccurate as \\(\\pi_\\theta\\) diverges. The optimizer may exploit these stale estimates, making updates that appear beneficial but are actually harmful.\n\nIn practice, unconstrained maximization of \\(L^{\\text{CPI}}(\\theta)\\) often leads to excessively large policy updates that cause catastrophic performance collapse.\n\nLLM Context (from Umar Jamil): Suppose we have a trajectory where the model generated ‚ÄúShanghai is in China‚Äù with high advantage. Unconstrained optimization might dramatically upweight ‚ÄúChina‚Äù as the next token given ‚ÄúShanghai is in‚Äù‚Äîbut this could simultaneously cause unintended probability shifts elsewhere, perhaps making the model overly likely to say ‚ÄúChina‚Äù in completely unrelated contexts, or disrupting the probability mass across the entire vocabulary in unpredictable ways.\n\nWe need a mechanism to constrain \\(\\pi_\\theta\\) from deviating too far from \\(\\pi_{\\theta_{\\text{old}}}\\) and keeping the ratio \\(r_t(\\theta)\\) close to 1 while still allowing meaningful policy improvement."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#vii-trust-region-policy-optimization-trpo",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#vii-trust-region-policy-optimization-trpo",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "VII: Trust Region Policy Optimization (TRPO)",
    "text": "VII: Trust Region Policy Optimization (TRPO)\nThe CPI objective is attractive because it lets us reuse data via importance ratios, but unconstrained optimization is unstable. When \\(\\pi_\\theta\\) drifts far from \\(\\pi_{\\theta_{\\text{old}}}\\), the probability ratios \\(r_t(\\theta)\\) become extreme and the advantage estimates \\(\\hat{A}_t\\) become stale and can be exploited by the optimizer.\nThe key insight of Trust Region Policy Optimization (TRPO) is that the surrogate objective \\(L^{\\text{CPI}}(\\theta)\\) is only a valid approximation to the true objective within a local neighborhood of \\(\\theta_{\\text{old}}\\). TRPO paper formalized this by proving policy performance is guaranteed to improve as long as the KL divergence between consecutive policies remains bounded. This theoretical result motivates constraining the policy update to stay within a ‚Äútrust region‚Äù where the surrogate objective remains reliable. See the TRPO paper for the formal proof.\nTRPO converts this insight into a constrained optimization problem that ensures the policy update stays within a ‚Äútrust region‚Äù where the surrogate objective remains reliable.\n\\[\n\\boxed{\n\\begin{aligned}\n\\max_\\theta \\quad & L^{\\text{CPI}}(\\theta) = \\mathbb{E}_t\\left[\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t\\right] \\\\[6pt]\n\\text{subject to} \\quad & \\mathbb{E}_t\\left[D_{\\text{KL}}\\left(\\pi_{\\theta_{\\text{old}}}(\\cdot|s_t) \\| \\pi_\\theta(\\cdot|s_t)\\right)\\right] \\leq \\delta\n\\end{aligned}\n} \\tag{VII.I}\n\\]\nThe hyperparameter \\(\\delta\\) defines the trust region size, the maximum allowed divergence between consecutive policies. This constraint ensures that \\(r_t(\\theta)\\) remains close to 1, keeping our importance-weighted estimates reliable.\nSolving (VII.I) requires second-order optimization. TRPO approximates the objective linearly and the KL constraint quadratically (using the Fisher Information Matrix) and then solves the resulting problem via the conjugate gradient algorithm followed by a line search to ensure constraints are satisfied.\nFor large-scale LLM training, this approach is impractical:\n\nComputational overhead: Each policy update requires multiple conjugate gradient iterations and line search steps, significantly more expensive than standard gradient descent.\nMemory requirements: Computing Fisher-vector products adds substantial memory overhead for billion(s)-parameter models\n\nThe theory behind TRPO also suggests using a KL penalty rather than a hard constraint. It is easier to implement and more computationally efficient.\n\\[\n\\max_\\theta \\; \\mathbb{E}_t\\left[r_t(\\theta) \\hat{A}_t - \\beta \\cdot D_{\\text{KL}}\\left(\\pi_{\\theta_{\\text{old}}}(\\cdot|s_t) \\| \\pi_\\theta(\\cdot|s_t)\\right)\\right] \\tag{VII.II}\n\\]\nHowever, choosing a penalty coefficient \\(\\beta\\) that works across different problems or even across different training stages is notoriously difficult. This motivates Proximal Policy Optimization (PPO): a first-order method that achieves TRPO‚Äôs stability through a clipped surrogate objective rather than explicit constraints."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#viii-proximal-policy-optimization-ppo",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#viii-proximal-policy-optimization-ppo",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "VIII: Proximal Policy Optimization (PPO)",
    "text": "VIII: Proximal Policy Optimization (PPO)\nProximal Policy Optimization (PPO) achieves TRPO‚Äôs stability guarantees using only first-order optimization. Instead of explicitly constraining the KL divergence, PPO modifies the objective function itself to discourage large policy updates through a clipping mechanism. It implicitly limits how far the policy can move, providing a ‚Äúsoft‚Äù trust region using only standard gradient descent.\n\nClipped Surrogate Objective\nCPI objective and probability ratio from Section VI:\n\\[\nL^{\\text{CPI}}(\\theta) = \\mathbb{E}_t\\left[r_t(\\theta) \\hat{A}_t\\right] \\quad \\text{where} \\quad r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}\n\\]\nThe problem with \\(L^{\\text{CPI}}\\) is that nothing prevents \\(r_t(\\theta)\\) from becoming arbitrarily large or small. PPO addresses this by clipping the probability ratio to stay within \\([1-\\epsilon, 1+\\epsilon]\\):\n\\[\n\\boxed{L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\hat{A}_t, \\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\cdot \\hat{A}_t\\right)\\right]} \\tag{VIII.I}\n\\]\nwhere \\(\\epsilon\\) is a hyperparameter (\\(\\epsilon = 0.2\\) from the PPO paper) and the clip function is defined as:\n\\[\n\\text{clip}(r, 1-\\epsilon, 1+\\epsilon) = \\begin{cases}\n1-\\epsilon & \\text{if } r &lt; 1-\\epsilon \\\\\nr & \\text{if } 1-\\epsilon \\leq r \\leq 1+\\epsilon \\\\\n1+\\epsilon & \\text{if } r &gt; 1+\\epsilon\n\\end{cases}\n\\]\nThe \\(\\min\\) operator in (VIII.I) is important. It ensures we take the more pessimistic (lower) estimate between the clipped and unclipped objectives. This creates different behavior depending on the sign of the advantage:\nCase 1: Positive Advantage (\\(\\hat{A}_t &gt; 0\\))\nWhen an action is better than average, we want to increase its probability, which means increasing \\(r_t(\\theta)\\). The objective becomes:\n\\[\nL^{\\text{CLIP}}_t = \\min\\left(r_t(\\theta), 1+\\epsilon\\right) \\cdot \\hat{A}_t\n\\]\n\nIf \\(r_t(\\theta) \\leq 1+\\epsilon\\): The objective is \\(r_t(\\theta) \\hat{A}_t\\), so gradient ascent increases \\(r_t(\\theta)\\)\nIf \\(r_t(\\theta) &gt; 1+\\epsilon\\): The objective becomes \\((1+\\epsilon)\\hat{A}_t\\)\n\nThe clipping removes the incentive to increase \\(r_t(\\theta)\\) beyond \\(1+\\epsilon\\).\nCase 2: Negative Advantage (\\(\\hat{A}_t &lt; 0\\))\nWhen an action is worse than average, we want to decrease its probability, which means decreasing \\(r_t(\\theta)\\). Since \\(\\hat{A}_t &lt; 0\\), multiplying by a smaller \\(r_t\\) makes the product less negative (larger). The objective becomes:\n\\[\nL^{\\text{CLIP}}_t = \\max\\left(r_t(\\theta), 1-\\epsilon\\right) \\cdot \\hat{A}_t\n\\]\n(The \\(\\min\\) with negative values becomes a \\(\\max\\) in terms of which \\(r_t\\) is selected.)\n\nIf \\(r_t(\\theta) \\geq 1-\\epsilon\\): The objective is \\(r_t(\\theta) \\hat{A}_t\\), so gradient ascent decreases \\(r_t(\\theta)\\)\nIf \\(r_t(\\theta) &lt; 1-\\epsilon\\): The objective becomes \\((1-\\epsilon)\\hat{A}_t\\)\n\nThe clipping removes the incentive to decrease \\(r_t(\\theta)\\) beyond \\(1-\\epsilon\\).\nThe takeaway here is that PPO provides a pessimistic lower bound on \\(L^{\\text{CPI}}\\). We ignore updates when they would make things ‚Äútoo good to be true.‚Äù\n\nLLM Context (from Umar Jamil Video): In language model fine-tuning, the policy \\(\\pi_\\theta(a_t|s_t)\\) is the probability the model assigns to token \\(a_t\\) given the context \\(s_t\\) (prompt + previously generated tokens). The probability ratio \\(r_t(\\theta)\\) measures how much more or less likely the fine-tuned model is to generate a particular token compared to the reference policy. Clipping ensures that no single token‚Äôs probability can change by more than a factor of \\((1 \\pm \\epsilon)\\) in a single update iteration, preventing the model from ‚Äúoverreacting‚Äù to high-advantage tokens.\n\n\n\nPPO Objective\nIn practice, PPO combines the clipped policy objective with two additional terms:\n\\[\n\\boxed{L^{\\text{PPO}}(\\theta) = \\mathbb{E}_t\\left[L^{\\text{CLIP}}_t(\\theta) - c_1 L^{\\text{VF}}_t(\\theta) + c_2 S[\\pi_\\theta](s_t)\\right]} \\tag{VIII.II}\n\\]\n1. Value Function Loss (\\(L^{\\text{VF}}\\)): Recall from Section V that we need a value function \\(V_\\phi(s)\\) to compute advantage estimates. The value function is trained to minimize the squared error between its predictions and the actual returns:\n\\[\nL^{\\text{VF}}_t(\\theta) = \\left(V_\\theta(s_t) - V_t^{\\text{target}}\\right)^2\n\\]\nwhere \\(V_t^{\\text{target}}\\) is typically the discounted return-to-go. When the policy and value function share parameters (common in LLM fine-tuning where both use the same transformer backbone), this loss is subtracted from the objective (hence the negative sign, since we maximize \\(L^{\\text{PPO}}\\) but minimize \\(L^{\\text{VF}}\\)).\n2. Entropy Bonus (\\(S[\\pi_\\theta]\\)): To encourage exploration and prevent premature convergence to deterministic policies, PPO adds an entropy loss:\n\\[\nS[\\pi_\\theta](s_t) = -\\sum_a \\pi_\\theta(a|s_t) \\log \\pi_\\theta(a|s_t)\n\\]\nHere, the coefficients \\(c_1, c_2 &gt; 0\\) control the regularization strength."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#ix-complete-ppo-objective-with-kl-penalty",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#ix-complete-ppo-objective-with-kl-penalty",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "IX: Complete PPO Objective with KL Penalty",
    "text": "IX: Complete PPO Objective with KL Penalty\nWhen fine-tuning an LLM with ‚Äúvanilla‚Äù PPO, the policy learns to maximize rewards from the reward model. However, the reward model is an imperfect proxy for human preferences. It is a neural network trained on limited data that can be exploited. Without constraints, the policy may discover adversarial outputs that achieve high reward scores while producing text that:\n\nDegenerates into repetitive or nonsensical patterns that ‚Äúfool‚Äù the reward model\nDrifts far from natural language, losing fluency and coherence\nExploits spurious correlations learned by the reward model\n\nThis phenomenon is called reward hacking. The policy finds a way to ‚Äúgame‚Äù the reward model rather than genuinely improving response quality.\nTo prevent reward hacking, the InstructGPT paper adds a KL divergence penalty that regularizes the policy to stay close to a reference model \\(\\pi_{\\text{ref}}\\) (typically the SFT model before RL fine-tuning).\nFrom Section VIII, the PPO objective (to be maximized via gradient ascent) consists of three terms:\n\\[\nL^{\\text{PPO}}(\\theta) = \\underbrace{L^{\\text{CLIP}}(\\theta)}_{\\text{Clipped Policy Objective}} - \\underbrace{c_1 L^{\\text{VF}}(\\theta)}_{\\text{Value Function Loss}} + \\underbrace{c_2 S[\\pi_\\theta]}_{\\text{Entropy Bonus}}\n\\]\nNow, we don‚Äôt use raw reward model scores directly. Instead, we define a KL-penalized reward that regularizes the policy to stay close to a reference model \\(\\pi_{\\text{ref}}\\):\n\\[\n\\boxed{r_{\\text{total}}(s_t, a_t) = r_{\\text{RM}}(s_t, a_t) - \\beta \\cdot D_{\\text{KL}}\\left(\\pi_\\theta(\\cdot|s_t) \\| \\pi_{\\text{ref}}(\\cdot|s_t)\\right)} \\tag{IX.I}\n\\]\nwhere: - \\(r_{\\text{RM}}(s_t, a_t)\\) is the reward signal at timestep \\(t\\) - \\(\\beta\\) is the KL penalty coefficient - \\(\\pi_{\\text{ref}}\\) is the frozen reference model\nAt each token position, the KL divergence simplifies to:\n\\[\nD_{\\text{KL}}\\left(\\pi_\\theta(\\cdot|s_t) \\| \\pi_{\\text{ref}}(\\cdot|s_t)\\right) = \\mathbb{E}_{a \\sim \\pi_\\theta}\\left[\\log \\frac{\\pi_\\theta(a|s_t)}{\\pi_{\\text{ref}}(a|s_t)}\\right]\n\\]\nIn practice we estimate this expectation with the sampled token \\(a_t\\), yielding: \\[\n\\hat d_t=\\log \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\mathrm{ref}}(a_t|s_t)}\n\\]\nNote that the reward model \\(r_\\phi(x, y)\\) produces a single scalar for the complete response \\((x, y)\\). This score is assigned only at the final token \\(T\\), while the KL penalty applies at every token. \\[\n\\tilde{r}_\\phi = \\begin{cases}\n-\\beta \\cdot \\log \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\text{ref}}(a_t | s_t)} & \\text{if } t &lt; T \\\\[8pt]\nr_\\phi(x, y) - \\beta \\cdot \\log \\frac{\\pi_\\theta(a_T | s_T)}{\\pi_{\\text{ref}}(a_T | s_T)} & \\text{if } t = T\n\\end{cases}\n\\]\nThe KL penalty serves two purposes: 1. Prevents reward hacking: The policy cannot drift arbitrarily far from natural language 2. Maintains fluency: Outputs remain similar in distribution to the well-trained SFT model\nIt modifies the advantage estimates \\(\\hat{A}_t\\) used in PPO through the modified per-token rewards. However, it is mathematically equivalent (and more efficient in implementation) to add the KL term directly to the objective. The PPO objective with KL penalty is:\n\\[\nJ(\\theta) = \\underbrace{\\mathbb{E}_{a \\sim \\pi_\\theta}\\left[r_{\\text{RM}}(s, a)\\right]}_{\\text{Vanilla PPO objective}} - \\underbrace{\\beta \\cdot D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}})}_{\\text{KL penalty term}}\n\\]\nThe first term is exactly what vanilla PPO optimizes using the clipped surrogate. The KL penalty term appears as a separate additive component that penalizes divergence from the reference model. Substituting the PPO clipped surrogate for the first term:\n\\[\nJ_{\\text{c}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t\\right)\\right] - \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}})\n\\]\nCombining all components, the complete PPO objective with KL penalty (to be maximized) is:\n\\[\n\\boxed{L^{\\text{RLHF}}(\\theta) = \\underbrace{L^{\\text{CLIP}}(\\theta)}_{\\text{Policy Objective}} - \\underbrace{c_1 L^{\\text{VF}}(\\theta)}_{\\text{Value Loss}} + \\underbrace{c_2 S[\\pi_\\theta]}_{\\text{Entropy Bonus}} - \\underbrace{\\beta \\cdot D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}})}_{\\text{KL Penalty}}} \\tag{IX.II}\n\\]\nHere, each term serves a distinct purpose:\n\n\n\n\n\n\n\nTerm\nRole\n\n\n\n\nPolicy Objective \\(L^{\\text{CLIP}}\\)\nImproves the policy while preventing destructive updates via clipping\n\n\nValue Loss \\(c_1 L^{\\text{VF}}\\)\nTrains the critic for accurate advantage estimation (subtracted to minimize)\n\n\nEntropy Bonus \\(c_2 S[\\pi_\\theta]\\)\nEncourages exploration, prevents premature convergence\n\n\nKL Penalty \\(\\beta D_{\\text{KL}}\\)\nPrevents reward hacking, maintains language quality (subtracted to penalize drift)\n\n\n\nIt is important to distinguish the two KL-related mechanisms in the complete loss. The PPO clipping mechanism acts as a short-term anchor that constrains how much the policy can change in a single update, while the KL penalty is a long-term anchor that constrains how far the policy can drift from its starting point across all of training."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#finally-done",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#finally-done",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "Finally done‚Ä¶",
    "text": "Finally done‚Ä¶\nAnd that‚Äôs the full derivation! What I find satisfying is that every term in the final loss has a specific purpose. Each one exists because we ran into a specific problem along the way and needed to fix it. I will admit it was not easy to understand all the math and concepts behind the loss. I still do not fully understand every detail but I understand it far better than I did a few days ago.\nI hope this was useful. If you spot any errors in derivation (which I‚Äôm sure there are) or have suggestions, feel free to reach out."
  },
  {
    "objectID": "posts/2025-12-25-deriving-ppo-loss.html#references",
    "href": "posts/2025-12-25-deriving-ppo-loss.html#references",
    "title": "Deriving the PPO Loss from First Principles",
    "section": "References",
    "text": "References\n\nVideo:\n\nUmar Jamil‚Äôs video on RLHF and PPO: A comprehensive and must-watch video covering RLHF and PPO concepts.\n\nPapers:\n\nProximal Policy Optimization Algorithms: The foundational PPO paper introducing the clipped surrogate objective.\nTraining language models to follow instructions with human feedback: The InstructGPT paper demonstrating PPO with KL penalty to mitigate reward hacking in LLM fine-tuning.\nTrust Region Policy Optimization: The TRPO paper that motivates the trust region constraints used in PPO.\nHigh-Dimensional Continuous Control Using Generalized Advantage Estimation: GAE paper introducing the exponentially-weighted advantage estimator for variance reduction in policy gradients."
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html",
    "href": "posts/2025-12-30-deriving-dpo-loss.html",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "",
    "text": "In my previous post, I worked through the derivation of the PPO loss used in RLHF for LLMs. By the end, we arrived at a fairly daunting objective function with multiple components: clipped surrogate, value function loss, entropy bonus and KL penalty. It is not just that the final objective is intimidating but the entire RLHF pipeline is complex and multi-step. You first train a separate reward model to reflect human preferences then fine-tune the LLM using RL with PPO.\nThat brings us to Direct Preference Optimization (DPO). DPO is a computationally lightweight alternative that directly optimizes LLMs to adhere to human preferences without explicit reward modeling or reinforcement learning. The key insight is that DPO implicitly optimizes the same objective as PPO-based RLHF (reward maximization with a KL-divergence constraint) but it replaces the entire reward model + PPO loop with a single supervised objective on preference pairs. There is no sampling during training, no value function, no clipping, just a classification loss!\nHere I derive the DPO loss showing exactly how this simplification is possible. I will assume familiarity with concepts from the PPO post, particularly the reward model and the KL-constrained RLHF objective."
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#i-the-rlhf-objective",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#i-the-rlhf-objective",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "I: The RLHF Objective",
    "text": "I: The RLHF Objective\nLet‚Äôs recall the RLHF objective from the PPO blog. The goal of RLHF is to find a policy \\(\\pi_\\theta\\) that maximizes expected reward while staying close to a reference model \\(\\pi_{\\text{ref}}\\):\n\\[\nJ_{\\text{RLHF}}(\\theta) = \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y|x)}\\left[r_\\phi(x, y)\\right] - \\beta \\cdot D_{\\text{KL}}\\left(\\pi_\\theta(y|x) \\| \\pi_{\\text{ref}}(y|x)\\right)\n\\]\nThe first term encourages the model to generate high-reward responses. The second term (KL penalty) prevents the model from drifting too far from the reference which helps avoid reward hacking and maintains language quality.\nAs we saw in the PPO blog, we can‚Äôt optimize this objective directly with gradient descent because the expectation \\(\\mathbb{E}_{y \\sim \\pi_\\theta(y|x)}[\\cdot]\\) requires sampling from the policy and sampling is non-differentiable. This is why we needed reinforcement learning algorithms like REINFORCE and PPO. They provide ways to estimate policy gradients without differentiating through the sampling process.\n\nWhat if we could reformulate the problem so that we don‚Äôt need to sample from the policy during training? This is exactly what DPO will achieve."
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#ii-the-bradley-terry-model-for-preference-learning",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#ii-the-bradley-terry-model-for-preference-learning",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "II: The Bradley-Terry Model for Preference Learning",
    "text": "II: The Bradley-Terry Model for Preference Learning\nWe also need to understand the Bradley-Terry model for reward model training in a bit more detail with focus on why it works the way it does.\nTraining a reward model requires human-labeled preference data that compares pairs of responses. \\[\n\\mathcal{D} = \\left\\{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\\right\\}_{i=1}^{N}\n\\]\nwhere: - \\(x\\) is the prompt - \\(y_w\\) is the preferred (winning) response - \\(y_l\\) is the dispreferred (losing) response\n\nThe Bradley-Terry Probability Model\nThe Bradley-Terry model provides a principled way to convert comparison data (defined above) into a probabilistic model. It assumes there exists some latent reward function \\(r^*(x, y)\\) that captures true response quality and models the probability that response \\(y_w\\) is preferred over \\(y_l\\) as:\n\\[\nP(y_w \\succ y_l | x) = \\frac{e^{r^*(x, y_w)}}{e^{r^*(x, y_w)} + e^{r^*(x, y_l)}} \\tag{II.I}\n\\]\nThe intuition is straightforward that the responses with higher reward are exponentially more likely to be preferred.\nA key step is recognizing that this ratio of exponentials can be written as a sigmoid function. This is important because it connects Bradley-Terry to standard binary classification.\nLet \\(A = r(x, y_w)\\) and \\(B = r(x, y_l)\\). We want to show:\n\\[\n\\frac{e^A}{e^A + e^B} = \\sigma(A - B)\n\\]\nwhere \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function.\nStarting with the left-hand side:\n\\[\n\\frac{e^A}{e^A + e^B}\n\\]\nwe can rewrite it as: \\[\n= \\frac{e^A / e^A}{(e^A + e^B) / e^A} = \\frac{1}{1 + e^B / e^A} = \\frac{1}{1 + e^{B-A}}\n\\]\nwhich is the sigmoid function of \\((A - B)\\): \\[\n= \\frac{1}{1 + e^{-(A-B)}} = \\sigma(A - B)\n\\]\nTherefore, the Bradley-Terry model can be written as:\n\\[\n\\boxed{P(y_w \\succ y_l | x) = \\sigma\\left(r(x, y_w) - r(x, y_l)\\right)} \\tag{II.II}\n\\]\n\n\nReward Model Loss\nGiven a dataset of preferences \\(\\mathcal{D}\\), we can train a parameterized reward model \\(r_\\phi(x, y)\\) using maximum likelihood estimation. We want to maximize the probability of observing the preferences in our dataset:\n\\[\n\\max_\\phi \\prod_{(x, y_w, y_l) \\in \\mathcal{D}} P(y_w \\succ y_l | x)\n\\]\nTaking the log and negating (to turn maximization into minimization), we get the negative log-likelihood loss:\n\\[\n\\boxed{\\mathcal{L}_{\\text{RM}}(\\phi) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(r_\\phi(x, y_w) - r_\\phi(x, y_l)\\right)\\right]} \\tag{II.III}\n\\]\nThis is just binary cross-entropy objective which helps the reward model learn to assign higher rewards to preferred responses. Notice that the Bradley-Terry model depends only on the difference of rewards: \\(r(x, y_w) - r(x, y_l)\\). The absolute values dont matter, it is only their relative ordering. This means:\n\nIf we add any constant \\(c\\) or any function \\(f(x)\\) that depends only on the prompt (not the response), the preference probabilities dont change. This invariance property will be the key to deriving DPO."
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#iii-optimal-policy-in-closed-form",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#iii-optimal-policy-in-closed-form",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "III: Optimal Policy in Closed Form",
    "text": "III: Optimal Policy in Closed Form\nHere, we will find the exact analytical optimal policy solution to the optimization problem. We want to find the policy that maximizes expected reward while keeping the KL divergence from the reference policy bounded:\n\\[\n\\max_\\pi \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi(y|x)}\\left[r(x, y)\\right] - \\beta \\cdot D_{\\text{KL}}\\left(\\pi(y|x) \\| \\pi_{\\text{ref}}(y|x)\\right) \\tag{III.I}\n\\]\nNote, I am writing \\(\\pi\\) instead of \\(\\pi_\\theta\\) to emphasize that we are looking for the optimal policy in general not just the parameterized version.\nExpanding the KL divergence:\n\\[\nD_{\\text{KL}}\\left(\\pi(y|x) \\| \\pi_{\\text{ref}}(y|x)\\right) = \\mathbb{E}_{y \\sim \\pi(y|x)}\\left[\\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\n\\]\nSo our objective becomes:\n\\[\n\\max_\\pi \\mathbb{E}_{x \\sim \\mathcal{D}} \\mathbb{E}_{y \\sim \\pi(y|x)}\\left[r(x, y) - \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\n\\]\nFor a fixed prompt \\(x\\), we want to find the distribution \\(\\pi(\\cdot|x)\\) that maximizes:\n\\[\n\\mathbb{E}_{y \\sim \\pi(y|x)}\\left[r(x, y) - \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right]\n\\]\nThis is a constrained optimization problem over probability distributions. We can solve it using the method of Lagrange multipliers, enforcing that \\(\\pi(y|x)\\) sums to 1. For discrete \\(y\\):\n\\[\n\\mathcal{L}(\\pi, \\lambda) = \\sum_y \\pi(y|x) \\left[r(x, y) - \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right] + \\lambda \\left(1 - \\sum_y \\pi(y|x)\\right)\n\\]\nTaking the derivative with respect to \\(\\pi(y|x)\\) and setting it to zero (stationary point):\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi(y|x)} = r(x, y) - \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\beta - \\lambda = 0\n\\]\nNow, solving for \\(\\pi(y|x)\\):\n\\[\n\\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} = \\frac{1}{\\beta}\\left(r(x, y) - \\beta - \\lambda\\right)\n\\]\n\\[\n\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} = \\exp\\left(\\frac{1}{\\beta}r(x, y)\\right) \\cdot \\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right)\n\\]\n\\[\n\\pi(y|x) = \\pi_{\\text{ref}}(y|x) \\cdot \\exp\\left(\\frac{1}{\\beta}r(x, y)\\right) \\cdot \\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right)\n\\]\nThe term \\(\\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right)\\) is a constant (with respect to \\(y\\)) that ensures normalization. To find its value, we enforce that \\(\\pi(y|x)\\) must be a valid probability distribution and sum to 1:\n\\[\n\\sum_y \\pi(y|x) = 1\n\\]\nSubstituting our expression for \\(\\pi(y|x)\\):\n\\[\n\\sum_y \\pi_{\\text{ref}}(y|x) \\cdot \\exp\\left(\\frac{1}{\\beta}r(x, y)\\right) \\cdot \\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right) = 1\n\\]\nSince \\(\\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right)\\) doesn‚Äôt depend on \\(y\\), we can factor it out of the sum:\n\\[\n\\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right) \\cdot \\sum_y \\pi_{\\text{ref}}(y|x) \\cdot \\exp\\left(\\frac{1}{\\beta}r(x, y)\\right) = 1\n\\]\nSolving for the constant:\n\\[\n\\exp\\left(-1 - \\frac{\\lambda}{\\beta}\\right) = \\frac{1}{\\sum_y \\pi_{\\text{ref}}(y|x) \\cdot \\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)}\n\\]\nWe define this normalizing sum as the partition function \\(Z(x)\\):\n\\[\nZ(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta}r(x, y)\\right) \\tag{III.II}\n\\]\nSubstituting back, we get the optimal policy:\n\\[\n\\boxed{\\pi_r(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)} \\tag{III.III}\n\\]\nWe have an exact closed-form expression for the optimal policy. However, we cannot compute it directly because \\(Z(x)\\) is intractable. To compute it, we need to sum over all possible responses \\(y\\) which not possible."
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#iv-the-reparameterization-trick",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#iv-the-reparameterization-trick",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "IV: The Reparameterization Trick",
    "text": "IV: The Reparameterization Trick\nThe key insight of DPO is to flip the relationship between reward and policy. Now, we frame the problem as: ‚Äúgiven an optimal policy what reward function does it correspond to?‚Äù\nStarting from the optimal policy equation (III.III):\n\\[\n\\pi_r(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\n\\]\nWe solve for the reward \\(r(x, y)\\) by first taking the log of both sides:\n\\[\n\\log \\pi_r(y|x) = \\log \\pi_{\\text{ref}}(y|x) + \\frac{1}{\\beta}r(x, y) - \\log Z(x)\n\\]\nNow rearrange to get \\(r(x, y)\\) on left-side:\n\\[\n\\frac{1}{\\beta}r(x, y) = \\log \\pi_r(y|x) - \\log \\pi_{\\text{ref}}(y|x) + \\log Z(x)\n\\]\n\\[\nr(x, y) = \\beta \\log \\pi_r(y|x) - \\beta \\log \\pi_{\\text{ref}}(y|x) + \\beta \\log Z(x)\n\\]\nThis can be written more compactly as:\n\\[\n\\boxed{r(x, y) = \\beta \\log \\frac{\\pi_r(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)} \\tag{IV.I}\n\\]\nThe reward is expressed as: - A term involving the log-ratio of the optimal policy to the reference policy - \\(\\beta \\log Z(x)\\) which depends only on \\(x\\) (not on \\(y\\))"
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#v-deriving-the-dpo-loss",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#v-deriving-the-dpo-loss",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "V: Deriving the DPO Loss",
    "text": "V: Deriving the DPO Loss\nFinally, we have all the pieces to derive the DPO loss. From Section II, the Bradley-Terry preference model is:\n\\[\nP(y_w \\succ y_l | x) = \\sigma\\left(r(x, y_w) - r(x, y_l)\\right)\n\\]\nFrom Section IV, assuming we have access to an optimal policy \\(\\pi^*\\), the reward can be written as:\n\\[\nr(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)\n\\]\nSubstituting this into Bradley-Terry:\n\\[\nP(y_w \\succ y_l | x) = \\sigma\\left(\\left[\\beta \\log \\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} + \\beta \\log Z(x)\\right] - \\left[\\beta \\log \\frac{\\pi^*(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} + \\beta \\log Z(x)\\right]\\right)\n\\]\nSimplifying the expression inside the sigmoid:\n\\[\n= \\sigma\\left(\\beta \\log \\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} + \\beta \\log Z(x) - \\beta \\log \\frac{\\pi^*(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} - \\beta \\log Z(x)\\right)\n\\]\nThe \\(\\beta \\log Z(x)\\) terms cancel:\n\\[\n= \\sigma\\left(\\beta \\log \\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi^*(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\n\\]\n\nNow recall the critical insight from Section II where we mentioned that the Bradley-Terry model depends only on reward differences. Thus, when we compute \\(r(x, y_w) - r(x, y_l)\\) the intractable partition function \\(Z(x)\\) cancels out. This is what makes DPO possible.\n\nWe can write this more cleanly by defining the implicit reward in terms of the optimal policy:\n\\[\n\\hat{r}(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)}\n\\]\nThus:\n\\[\nP(y_w \\succ y_l | x) = \\sigma\\left(\\hat{r}(x, y_w) - \\hat{r}(x, y_l)\\right)\n\\]\nWe dont actually have access to the optimal policy \\(\\pi^*\\). But we can parameterize a policy \\(\\pi_\\theta\\) and optimize it to maximize the likelihood of the observed preferences. This is exactly what the reward model loss (II.III) does except now our reward is implicitly defined by the policy itself.\nThe DPO loss is the negative log-likelihood:\n\\[\n\\boxed{\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right]} \\tag{V.I}\n\\]\nfor implicit reward notation, it can be written as:\n\\[\n\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\hat{r}_\\theta(x, y_w) - \\hat{r}_\\theta(x, y_l)\\right)\\right] \\tag{V.II}\n\\]\nSome key insights from the above DPO loss:\n\nThe policy implicitly defines its own reward via the log-ratio with the reference. There is no separate reward model.\nThis is just a supervised classification loss on preference pairs with no RL.\nDPO uses the fixed preference dataset \\(\\mathcal{D}\\). Thus, no sampling during training.\nNo value function needed since we‚Äôre not doing policy gradients\nDPO still optimizes the KL-constrained reward maximization objective but in a different way"
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#vi-building-intuition-for-dpo",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#vi-building-intuition-for-dpo",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "VI: Building Intuition for DPO",
    "text": "VI: Building Intuition for DPO\nNow that we have the DPO loss we can build some intuition around the implicit reward model and its gradient updates.\n\nImplicit Reward Model\nThe DPO paper subtitle is ‚ÄúYour Language Model is Secretly a Reward Model‚Äù and this captures the key insight as we are using the LLM for implicit reward. The policy \\(\\pi_\\theta\\) defines an implicit reward function:\n\\[\n\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}\n\\]\nThis reward measures how much more likely the current policy is to generate response \\(y\\) compared to the reference policy, scaled by \\(\\beta\\).\n\nIf \\(\\pi_\\theta(y|x) &gt; \\pi_{\\text{ref}}(y|x)\\): The implicit reward is positive (the policy ‚Äúlikes‚Äù this response more than reference)\nIf \\(\\pi_\\theta(y|x) &lt; \\pi_{\\text{ref}}(y|x)\\): The implicit reward is negative (the policy ‚Äúlikes‚Äù this response less than reference)\n\n\n\nAnalyzing the Gradient Update\nWe can flex our brain muscles one more time and compute the gradient for the DPO objective:\n\\[\n\\mathcal{L}_{\\text{DPO}} = -\\mathbb{E}\\left[\\log \\sigma\\left(\\hat{r}_\\theta(x, y_w) - \\hat{r}_\\theta(x, y_l)\\right)\\right]\n\\]\nLet \\(u = \\hat{r}_\\theta(x, y_w) - \\hat{r}_\\theta(x, y_l)\\). Using the chain rule:\n\\[\n\\nabla_\\theta \\mathcal{L}_{\\text{DPO}} = -\\mathbb{E}\\left[\\frac{\\sigma'(u)}{\\sigma(u)} \\nabla_\\theta u\\right]\n\\]\nUsing the property \\(\\sigma'(u) = \\sigma(u)(1 - \\sigma(u))\\):\n\\[\n= -\\mathbb{E}\\left[\\frac{\\sigma(u)(1-\\sigma(u))}{\\sigma(u)} \\nabla_\\theta u\\right] = -\\mathbb{E}\\left[(1 - \\sigma(u)) \\nabla_\\theta u\\right]\n\\]\nUsing \\(1 - \\sigma(u) = \\sigma(-u)\\):\n\\[\n= -\\mathbb{E}\\left[\\sigma(-u) \\nabla_\\theta u\\right]\n\\]\nNow, \\(-u = \\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta(x, y_w)\\), and:\n\\[\n\\nabla_\\theta u = \\nabla_\\theta \\hat{r}_\\theta(x, y_w) - \\nabla_\\theta \\hat{r}_\\theta(x, y_l) = \\beta \\nabla_\\theta \\log \\pi_\\theta(y_w|x) - \\beta \\nabla_\\theta \\log \\pi_\\theta(y_l|x)\n\\]\nPutting it together:\n\\[\n\\boxed{\\nabla_\\theta \\mathcal{L}_{\\text{DPO}} = -\\beta \\mathbb{E}\\left[\\underbrace{\\sigma\\left(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta(x, y_w)\\right)}_{\\text{weight}}\\left(\\underbrace{\\nabla_\\theta \\log \\pi_\\theta(y_w|x)}_{\\text{increase } y_w} - \\underbrace{\\nabla_\\theta \\log \\pi_\\theta(y_l|x)}_{\\text{decrease } y_l}\\right)\\right]} \\tag{VI.I}\n\\]\n\n\\(\\nabla_\\theta \\log \\pi_\\theta(y_w|x)\\) points in the direction that increases probability of the preferred response\n\\(-\\nabla_\\theta \\log \\pi_\\theta(y_l|x)\\) points in the direction that decreases probability of the dispreferred response\nThe weight term is high when \\(\\hat{r}_\\theta(x, y_l) &gt; \\hat{r}_\\theta(x, y_w)\\), i.e.¬†when the model currently assigns higher implicit reward to the losing response than the winning response. In other words:\n\nWhen the model is wrong (ranks \\(y_l\\) above \\(y_w\\)), we get large gradient updates\nWhen the model is right (ranks \\(y_w\\) above \\(y_l\\)), we get small gradient updates\n\n\nThis dynamic sigmoid weighting is crucial. It naturally focuses learning on the examples the model currently gets wrong."
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#vii-computing-log-probabilities-in-practice",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#vii-computing-log-probabilities-in-practice",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "VII: Computing Log Probabilities in Practice",
    "text": "VII: Computing Log Probabilities in Practice\n\nThis section is fully adapted from Umar Jamil‚Äôs video. I think it is essential to understand how log probabilities are computed in practice.\n\nThe DPO loss requires computing \\(\\log \\pi_\\theta(y|x)\\), the log probability of a complete response \\(y\\) given a prompt \\(x\\). Let‚Äôs see how this works in practice with LLMs.\nLanguage models are autoregressive: they generate text one token at a time, conditioning on all previous tokens. For a response \\(y = (y_1, y_2, \\ldots, y_T)\\), the probability factorizes as:\n\\[\n\\pi_\\theta(y|x) = \\prod_{t=1}^{T} \\pi_\\theta(y_t | x, y_1, \\ldots, y_{t-1}) = \\prod_{t=1}^{T} \\pi_\\theta(y_t | x, y_{&lt;t})\n\\]\nTaking the logarithm:\n\\[\n\\boxed{\\log \\pi_\\theta(y|x) = \\sum_{t=1}^{T} \\log \\pi_\\theta(y_t | x, y_{&lt;t})} \\tag{VII.I}\n\\]\nThe log probability of the full response is the sum of log probabilities at each position.\nHere‚Äôs how to compute \\(\\log \\pi_\\theta(y|x)\\):\n\nPrepare input: Concatenate the prompt and response into a single sequence\n\\[\\text{input} = [x_1, x_2, \\ldots, x_n, y_1, y_2, \\ldots, y_T]\\]\nForward pass: Run the transformer to get hidden states at each position\nProject to logits: Apply the language model head (typically a linear layer) to get vocabulary logits at each position\nLog softmax: Convert logits to log probabilities over the vocabulary using logsoftmax\nGather relevant log probs: For each position \\(t\\) in the response extract the log probability of the actual next token \\(y_t\\) (since we know the output)\nSum with masking: Sum the log probabilities but only for response tokens (not prompt tokens)\n\\[\\log \\pi_\\theta(y|x) = \\sum_{t \\in \\text{res pos}} \\log \\pi_\\theta(y_t | x, y_{&lt;t})\\]\n\nThis gives us \\(\\log \\pi_\\theta(y|x)\\) for one response. We do this for both the preferred response \\(y_w\\) and the dispreferred response \\(y_l\\) and we also do it for both the policy model \\(\\pi_\\theta\\) and the frozen reference model \\(\\pi_{\\text{ref}}\\). With these four log probabilities in hand, we can compute the DPO loss."
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#conclusion",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#conclusion",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "Conclusion",
    "text": "Conclusion\nOnce you derive the DPO loss, you start appreciating the simplicity and elegance of the solution especially when compared to PPO. The derivation hinges on one observation that the Bradley-Terry model only cares about reward differences and this causes the intractable partition function from analytical solution to cancel out completely. In turn, what remains is a straightforward classification loss."
  },
  {
    "objectID": "posts/2025-12-30-deriving-dpo-loss.html#references",
    "href": "posts/2025-12-30-deriving-dpo-loss.html#references",
    "title": "Deriving the DPO Loss from First Principles",
    "section": "References",
    "text": "References\n\nPapers:\n\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model: The original DPO paper\nTraining language models to follow instructions with human feedback: The InstructGPT paper that established PPO-based RLHF\n\nVideos:\n\nUmar Jamil‚Äôs video on DPO: Excellent walkthrough of the DPO derivation"
  }
]