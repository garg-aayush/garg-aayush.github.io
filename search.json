[
  {
    "objectID": "posts/longer-version.html",
    "href": "posts/longer-version.html",
    "title": "What I Learned in Lecture 1: LLM Evaluation Lifecycle",
    "section": "",
    "text": "Recently I enrolled in AI Evals for Engineers and PMs, a course by Hamel and Shreya that‚Äôs refreshingly LLM application-centric. Instead of abstract benchmarks, it teaches you how to systematically evaluate and improve LLM-powered products. As the they put it, the goal is to:\nIn this first blog of what I hope will be a multi-part series ü§û, I‚Äôll walk through my key takeaways from Lecture 1."
  },
  {
    "objectID": "posts/longer-version.html#evaluation-isnt-optional-but-fundamental",
    "href": "posts/longer-version.html#evaluation-isnt-optional-but-fundamental",
    "title": "What I Learned in Lecture 1: LLM Evaluation Lifecycle",
    "section": "1. Evaluation isn‚Äôt Optional but Fundamental",
    "text": "1. Evaluation isn‚Äôt Optional but Fundamental\nAnyone who has built or worked with LLM pipelines knows that their outputs are inherently open-ended, subjective, and unstructured. This is unlike traditional software or classical ML models, where a loss curve(s) can reliably indicate when performance is improving. Relying on ad hoc checks which I have done, leads to knee-jerk fixes that may yield short-term gains but overlook the long-term need of continuously tracking and enhancing reliability, safety, and usefulness.\nThat is why evaluation‚Äîthe systematic measurement of an LLM pipeline quality‚Äîis absolutely critical! It isn‚Äôt a single score, but a blend of quantitative metrics and structured qualitative reports. Embracing this comprehensive framework is how I diagnose failures, guide improvements, and build truly robust, ever-evolving LLM applications.\nWe need a systematic manner a ## 2. Bridge the Three Gulfs in Pipeline Development ## 3. Iterate with Analyze ‚Üí Measure ‚Üí Improve"
  },
  {
    "objectID": "posts/longer-version.html#always-remember-llms-core-strengths-and-weaknesses",
    "href": "posts/longer-version.html#always-remember-llms-core-strengths-and-weaknesses",
    "title": "What I Learned in Lecture 1: LLM Evaluation Lifecycle",
    "section": "4. Always Remember LLMs Core Strengths and Weaknesses",
    "text": "4. Always Remember LLMs Core Strengths and Weaknesses\n‚ÄúLLMs are powerful but imperfect components. Leverage strengths, anticipate weaknesses.‚Äù  Whenever you‚Äôre iterating on prompts or evaluating your pipeline, remember to capitalize on LLMs‚Äô strengths and put safeguards in place for their weakest spots. Moreover, your evaluations must explicitly address these weaknesses."
  },
  {
    "objectID": "posts/longer-version.html#craft-precise-prompts-and-refine-iteratively",
    "href": "posts/longer-version.html#craft-precise-prompts-and-refine-iteratively",
    "title": "What I Learned in Lecture 1: LLM Evaluation Lifecycle",
    "section": "5. Craft Precise Prompts and Refine Iteratively",
    "text": "5. Craft Precise Prompts and Refine Iteratively"
  },
  {
    "objectID": "posts/longer-version.html#define-good-vs.-bad-behavior-upfront",
    "href": "posts/longer-version.html#define-good-vs.-bad-behavior-upfront",
    "title": "What I Learned in Lecture 1: LLM Evaluation Lifecycle",
    "section": "6. Define Good vs.¬†Bad Behavior Upfront",
    "text": "6. Define Good vs.¬†Bad Behavior Upfront"
  },
  {
    "objectID": "posts/longer-version.html#use-reference-based-and-reference-free-metrics",
    "href": "posts/longer-version.html#use-reference-based-and-reference-free-metrics",
    "title": "What I Learned in Lecture 1: LLM Evaluation Lifecycle",
    "section": "7. Use Reference-Based and Reference-Free Metrics",
    "text": "7. Use Reference-Based and Reference-Free Metrics"
  },
  {
    "objectID": "posts/longer-version.html#clarify-specifications-first-with-error-analysis",
    "href": "posts/longer-version.html#clarify-specifications-first-with-error-analysis",
    "title": "What I Learned in Lecture 1: LLM Evaluation Lifecycle",
    "section": "8. Clarify Specifications First with Error Analysis",
    "text": "8. Clarify Specifications First with Error Analysis"
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "",
    "text": "ComfyUI is by far my favorite open-source software right now. Its intuitive node-based interface has transformed the way we build AI image and video generation workflows.\nWhat I really appreciate about ComfyUI is its flexibility. You can easily extend it with your own custom nodes. Here, I‚Äôll show you how to create custom nodes that let you add exactly the tools you need. I‚Äôll use parts from my Svg2Raster nodes as the running example for this purpose."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#svg2raster",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#svg2raster",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Svg2Raster",
    "text": "Svg2Raster\nComfyUI does not natively support vector graphics like SVGs. I often work with them and needed lightweight nodes to load (SVG-&gt;JPEGs/PNGs) and manipulate SVGs in ComfyUI.\nThus, I built Svg2Raster, a small custom node package that makes it easy to use SVGs with other nodes."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-1-validate-the-core-logic-first",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-1-validate-the-core-logic-first",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 1: Validate the core logic first",
    "text": "Step 1: Validate the core logic first\nI prefer not to start with the ComfyUI node API. First, I like to write a simple Python notebook to test the functionalities I actually need. This validates your core code logic and packages in isolation.\nIn my case, I needed a way to read, rasterize and manipulate the SVGs. Thus, I tested all the relevant operations using the core packages CairoSVG and Pillow.\nFor example:\n# Simple SVG read and conversion check\nimport cairosvg\nfrom PIL import Image, ImageOps\nimport io\n\n# Read SVG file\nwith open('logo.svg', 'r', encoding='utf-8') as f:\n    svg_text = f.read()\n\n# Basic conversion to PNG\nimg_bytes = cairosvg.svg2png(bytestring=svg_text.encode('utf-8'), \n                              output_width=600)\nimg = Image.open(io.BytesIO(img_bytes)).convert('RGBA')\nprint(f\"Image size: {img.size}\")\nIf you want to see all the code snippets (width/height controls, color and border manipulations etc.), please check out the full notebook.\nOnce you have a working standalone script or code, wrapping it as a ComfyUI node is mostly boilerplate."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-2-understand-the-anatomy-of-a-custom-node",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-2-understand-the-anatomy-of-a-custom-node",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 2: Understand the Anatomy of a Custom Node",
    "text": "Step 2: Understand the Anatomy of a Custom Node\nEvery ComfyUI node is a Python class with specific methods that ComfyUI expects. Here are the essential components:\n\n\n\nComponent\nDescription\n\n\n\n\nINPUT_TYPES\nWhat inputs your node accepts\n\n\nRETURN_TYPES\nWhat it outputs to other nodes\n\n\nRETURN_NAMES\nOptional labels for outputs\n\n\nFUNCTION\nThe method name that runs your logic\n\n\nCATEGORY\nWhere it appears in ComfyUI‚Äôs node menu\n\n\n\nComfyUI handles the rest of UI, connections and execution order. For example, this is how a simple custom node class will looks like:\nclass LoadSVG:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"svg_file\": (\"STRING\", {\"default\": \"file.svg\"}),\n            }\n        }\n    \n    RETURN_TYPES = (\"IMAGE\", \"STRING\")\n    RETURN_NAMES = (\"image\", \"svg_text\")\n    FUNCTION = \"load_svg\"\n    CATEGORY = \"Svg2Raster\"\n    \n    def load_svg(self, svg_file):\n        # Your actual logic here\n        return (image_tensor, svg_text)\nThis is a minimal ComfyUI node class explanation that I believe is good enough to start writing your own nodes. If you want more details, check the official ComfyUI custom node documentation."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-3-implementing-the-loadsvgimage-node",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-3-implementing-the-loadsvgimage-node",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 3: Implementing the LoadSVGImage Node",
    "text": "Step 3: Implementing the LoadSVGImage Node\nFirst, I set up the file structure in ComfyUI‚Äôs custom_nodes folder for my nodes package:\ncd ComfyUI/custom_nodes\nmkdir svg2raster\ncd svg2raster\nThen I create two essential files:\n__init__.py: it allows ComfyUI to import your custom nodes.\nfrom .svg2raster_node import *\n\n__all__ = [ \"NODE_CLASS_MAPPINGS\",\n            \"NODE_DISPLAY_NAME_MAPPINGS\"]\nsvg2raster_node.py: this is where the actual nodes code is written.\nYou can find the complete code for these nodes here: svg2raster_node.py. Here‚Äôs the boilerplate structure of LoadSVGImage node:\nclass LoadSVGImage:\n    @classmethod\n    def INPUT_TYPES(cls):\n        \"\"\"Define what inputs this node accepts\"\"\"\n        # Use `folder_paths` to access ComfyUI's input directory\n        input_dir = folder_paths.get_input_directory()\n        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f)) and f.lower().endswith('.svg')]\n        return {\n            \"required\": {\n                \"svg\": (sorted(files), {\"image_upload\": True}),\n            }\n        }\n    \n    # Output configuration\n    RETURN_TYPES = (\"STRING\", \"IMAGE\")\n    RETURN_NAMES = (\"svg_text\", \"preview_image\")\n    FUNCTION = \"load_svg\"  # Method name to execute\n    CATEGORY = \"FromSVG/Tools\"  # Menu location\n    \n    def load_svg(self, svg, background=\"#FFFFFF\"):\n        \"\"\"Main execution method - does the actual work\"\"\"\n        # Your logic here\n        return (svg_text, image_tensor)\n    \n    @classmethod\n    def IS_CHANGED(cls, svg):\n        # Returns file hash or modification time\n        pass\n    \n    @classmethod\n    def VALIDATE_INPUTS(cls, svg):\n        # Check if file exists, return error string if invalid\n        return True\nHere, the helper methods serve crucial purposes: - IS_CHANGED: Tells ComfyUI when to re-execute the node - VALIDATE_INPUTS: Prevents crashes by validating inputs before execution\nComfyUI expects images as tensors in BHWC format (batch, height, width, channels) with values normalized to 0-1. Thus, you need to have a pil to tensor function.\ndef _pil_to_tensor(pil_img: Image.Image):\n    \"\"\"Convert PIL image to ComfyUI IMAGE tensor: (B, H, W, C) in [0,1]\"\"\"\n    # conversion logic\nFinally, you need the mappings for ComfyUI to discover your nodes:\nNODE_CLASS_MAPPINGS = {\n    \"LoadSVGImage\": LoadSVGImage,\n}\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"LoadSVGImage\": \"Load SVG Image\",\n}\nWithout these mappings, ComfyUI won‚Äôt find your nodes even if the code is correct.\nSimilarly, I also wrote a RasterizeSVG class for manipulating the loaded SVG. It takes the SVG text from LoadSVGImage and lets you adjust scale, dimensions, borders, and more. You will see the pattern is identical: define inputs, process with CairoSVG/PIL, convert to tensor, return.\nThat‚Äôs how you implement a node. Write a standalone functionality script, wrap it in the ComfyUI class structure, handle the tensor conversions, add the helper methods, and register it with the mappings."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-4-testing-your-custom-node-in-comfyui",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-4-testing-your-custom-node-in-comfyui",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 4: Testing Your Custom Node in ComfyUI",
    "text": "Step 4: Testing Your Custom Node in ComfyUI\nOnce you are done implementing, testing is straightforward.\n\nEnsure all your dependencies are installed, including CairoSVG:\nRestart ComfyUI for it to detect the new node.\nFind your nodes under the defined category\n\nNote: I have also added the installation steps here."
  },
  {
    "objectID": "posts/2025-09-10-build-custom-comfyui-node.html#step-5-sharing-and-publishing-your-node",
    "href": "posts/2025-09-10-build-custom-comfyui-node.html#step-5-sharing-and-publishing-your-node",
    "title": "A Guide to Building Custom Nodes in ComfyUI",
    "section": "Step 5: Sharing and Publishing Your Node",
    "text": "Step 5: Sharing and Publishing Your Node\nOnce everything worked, I created a GitHub repo for the nodes. You can see how I structured mine in the Svg2Raster repo.\nSome Essential files in the repo are:\n\n\n\nFile\nDescription\n\n\n\n\nREADME.md\nClear installation instructions and usage examples\n\n\nrequirements.txt\nPython dependencies (cairosvg in my case)\n\n\npyproject.toml\nRequired if you plan to publish to ComfyUI Registry\n\n\nexamples/\nOptional Sample SVG files and workflow JSON files\n\n\n\nNote: Having a good Readme and examples makes a huge difference for users trying to understand and use your nodes.\nOnce your repo is ready, you can even publish it to the ComfyUI Registry. There‚Äôs an excellent guide on publishing to ComfyUI Registry - just follow those steps.\nI also set up a GitHub Actions workflow that automatically publishes updates to the ComfyUI Registry whenever I push changes to my repo. This ensures the registry always has the latest version. You can check out my workflow file to see how I did it."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "",
    "text": "Last week, I published the second blog in my LLM fine-tuning series, comparing various models performance in functional representation extraction.\nIn this third part of the series, I discuss the first steps toward fine-tuning an (open-source)-LLM for functional representation extraction. My aim is to give you all a sneak peek at the kind of performance you can expect from fine-tuning an LLM for a custom task. To streamline this step (and to satisfy my own curiosity üòä), I will use Predibase. It is a fast, cheap, and efficient open-source LLM fine-tuning and deployment platform."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#task-and-dataset",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#task-and-dataset",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Task and Dataset",
    "text": "Task and Dataset\nSimilar to my previous blogs, the custom task is to predict the structured functional representation from the given text video game opinions of the ViGGO validation dataset.\nTo make this exercise interesting and challenging for future experiments, I will use a maximum of 1000 examples for fine-tuning any LLM model, instead of the full ~5K train dataset.\nBelow is an example from the randomly selected 1K train dataset:\nText                      : I remember you saying that you loved The Room. Do you tend to enjoy PC games from 2012?\nfunctional_representation : verify_attribute(name[The Room], release_year[2012], rating[excellent], platforms[PC])\nAs shown in the graph below, the selected 1K dataset is a fairly representative sample of the full ViGGO train dataset.\n\n\n\nUnderstanding Data Distribution"
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#upload-the-dataset-to-predibase",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#upload-the-dataset-to-predibase",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Upload the dataset to Predibase",
    "text": "Upload the dataset to Predibase\nPredibase requires you to upload the instruction fine-tuning dataset in particular format. This is from Predibase docs:\n\nFor instruction fine-tuning, your dataset must contain two columns named prompt and completion: - prompt: Your input prompt. It serves as the starting point or the guiding information for the model. - completion: The expected response that corresponds to the input provided in the ‚Äúprompt‚Äù column. - split (optional): Should be either train or evaluation. To learn more, check out this section.\n\nMake sure to add the prompt template to the examples and convert them to the correct format. For this exercise, I use the following prompt template:\nprompt_template = \"\"\"Given a target sentence convert it structured functional representation.\n\n### Target sentence: {text}\n\n### Output Functional representation:\n\"\"\"\nYou can connect your dataset to Predibase via the UI or Python SDK. Here, I will upload the dataset using SDK.\n# Initialize Predibase client\npb = Predibase(api_token=os.environ[\"PREDIBASE_API_TOKEN\"])\n\n# Upload the dataset\ndataset = pb.datasets.from_file(\"viggo_train_val_dataset_1K.csv\", \n                                name=\"viggo_train_val_dataset_1K\")\nOnce uploade, you can check the uploaded dataset on the Predibase UI.  \nFor detailed steps on uploading the dataset to Predibase, please refer to the companion blog notebook."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#setup-and-finetune",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#setup-and-finetune",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Setup and Finetune",
    "text": "Setup and Finetune\nOnce you have uploaded the dataset, running the fine-tuning process is refreshingly simple. For this example, I fine-tune the base llama-3-8b model with the following parameters: epochs=3, rank=16, and learning_rate=2e-4.\n# Create an adapter repository\nrepo = pb.repos.create(name=\"viggo-finetune-1K\", \n                description=\"Llama-3-8b adapter repository for viggo 1K examples\"\n                )\n\n# Create and run the fine-tuning job\nadapter = pb.adapters.create(\n   config=FinetuningConfig(\n       base_model=\"llama-3-8b\",\n       epochs=3,\n       rank=16,\n       learning_rate=0.0002,\n   ),\n   dataset=dataset,\n   repo=repo,\n   description=\"baseline-llama-3-8b\",\n)\nThat‚Äôs all you need to do to submit a job! Once completed, it will be available on the Predibase platform.\n \nYou can always tweak multiple hyperparameters (see Finetuning Config) and run the fine-tune job again. All your fine-tune jobs will be available on the Predibase platform."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#evaluate-the-fine-tuned-model",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#evaluate-the-fine-tuned-model",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Evaluate the Fine-tuned Model",
    "text": "Evaluate the Fine-tuned Model\nPredibase provides both popular Serverless endpoints and Dedicated deployments options for opens-source LLMs and their fine-tuned LORA checkpoints. I will create serverless endpoint for this case.\n\nNote, atleast for now, serverless deployments are available for free.\n\n\nGenerate the responses for validation dataset\nSimilar to my previous blogs, I will evaluate the finetuned model on ViGGO validation dataset and calculate custom performance metrics metrics for a better understanding of finetuned model performance.\nFirst, I generate the responses for the validation dataset:\n# Initialize the Predibase deployment client\nlorax_client = pb.deployments.client(\"llama-3-8b\")\n\n# Load the validation dataset\nviggo_dataset = load_dataset(\"GEM/viggo\")\nval_dataset = viggo_dataset['validation']\n\n# finetuned adapter id\nadapter_id = \"viggo-finetune-1K/2\" \n\nresponses_dict = {}\nfor idx in range(len(val_dataset)):\n    if idx % 50 == 0: print(f\"Processing {idx}/{len(val_dataset)}\")\n    output = lorax_client.generate(prompt_template.format(text=val_dataset[\"target\"][idx]), adapter_id=\"viggo-finetune-1K/2\", max_new_tokens=150).generated_text\n    ground_truth = val_dataset[\"meaning_representation\"][idx]\n    text = val_dataset[\"target\"][idx]\n    responses_dict[idx] = {\"output\": output, \"ground_truth\": ground_truth, \"text\": text}\nNote: Remember to replace ‚Äúviggo-finetune-1K/2‚Äù with the correct adapter ID. You can find the adapter ID in the Predibase dashboard.\nNow, I can generate the evaluation scores using custom evaluation metrics and compare them with previously calculated GPT-4 and Claude 3.5 Sonnet scores:\n\n\n\nplot-finetuned-model\n\n\nThe initial finetuning of LLaMA-3-8B using 1,000 random examples from the ViGGO dataset, while not surpassing GPT-4 and Claude 3.5 Sonnet, shows promising results and outperforms several models from our previous blog. Notably, the exact_match score is even better than that of the two best-performing models."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#improved-performance-with-updated-prompt-template",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#improved-performance-with-updated-prompt-template",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Improved Performance with Updated Prompt Template",
    "text": "Improved Performance with Updated Prompt Template\nA simple yet effective way to enhance the model‚Äôs performance is by refining the prompt template. By providing clearer instructions that convey the structure of the functional representation, we can guide the model to produce more accurate outputs.\nI updated the prompt template as follows:\nprompt_template = \"\"\"Given a target sentence construct the underlying meaningful functional representation of the input sentence as a single function with attributes and attribute values.\n\n### Target sentence: {text}\n\n### Output Functional representation:\n\"\"\"\nAfter uploading this new dataset, finetuning the model, and evaluating it with the new adapter viggo-finetune-1K/3, there is significantly improved evaluation metrics. Notably, the model now surpasses GPT-4o‚Äôs scores for exact_match and function_name_match.\n\n\n\nplot-finetuned-model\n\n\nThis improvement highlights the importance of clear and specific instructions in prompt engineering, even when working with finetuned models."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#conclusions..",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#conclusions..",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Conclusions..",
    "text": "Conclusions..\n\nFirst of all, My overall experience with Predibase has been positive, particularly in terms of rapid finetuning of models. While there are some limitations such as restricted hyperparameter tuning, standardized dataset format, and inability to download adapters in the developer tier, it offers a user-friendly platform for fine-tuning (LORA) large language models. I was able to quickly upload, setut, finetune and infer the llm models.\nI achieve out-of-the-box performance using only random 1K examples. Although the fine-tuned llama-3-8b model doesn‚Äôt match the performance of GPT-4 and Sonnet 3.5 on all metrics. This demonstrates the potential of fine-tuning with limited data, highlighting the efficiency of the approach for task-specific model adaptation."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#next-steps",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#next-steps",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "Next steps‚Ä¶",
    "text": "Next steps‚Ä¶\n\nMy next goal is to further enhance the model‚Äôs performance on evaluation metrics while maintaining a limit of 1,000 training examples. LIMA: Less Is More for Alignment paper has demonstrated in the past that even 1,000 well-curated examples can lead to strong finetuning performance.\nCareful curated selection of examples and hyerparameters will definitely improve the performance benchmarks on evaluation metrics.\nIn addition to it, I will deep dive into one of my favorite LLM fine-tuning tools, Axolotl."
  },
  {
    "objectID": "posts/2024-07-15-finetune-llama3-8B-predibase.html#references",
    "href": "posts/2024-07-15-finetune-llama3-8B-predibase.html#references",
    "title": "Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction",
    "section": "References",
    "text": "References\n\nPart II blog post of series\nNotebook I: Analyze_Viggo_Dataset.ipynb\nNotebook II: Finetune_llama-3-8b_Predibase.ipynb\nPredibase Platform\nLORA blog I: Finetuning Large Language Models (LLMs)\nLORA blog II: LLM Research Insights: Instruction Tuning & Training Paradigms\nLlama 3\nLIMA: Less Is More for Alignment\nAxolotl: A Framework for Fine-tuning LLMs\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "posts/2024-07-03-baseline-gpt4o-structured-data.html",
    "href": "posts/2024-07-03-baseline-gpt4o-structured-data.html",
    "title": "Part I: Baseline Evaluation of GPT-4o for Functional Representation Extraction",
    "section": "",
    "text": "Introduction\nExtracting structured data from unstructured texts allow us to condense the information present in the text. This representation then can be used for efficient indexing and other downstream RAG applications.¬†\nI want to evaluate GPT-4o‚Äôs performance in extracting structural data, specifically, functional representation, from the unstructured domain-specific text. I will use ViGGO dataset to evaluate it on custom evaluation criteria and will set it as baseline performance for that can be used for comparison in future work with other models such as Claude, Gemini, and custom fine-tuned open-source models.\n\n\nViGGO Dataset\nThis is a dataset for generating text opinions in the video game domain. Strictly speaking, it is intended to generate coherent conversational responses based on input functional representations (set of attributes and values).\nHowever, I use the reverse task, where I generate structured functional representations from the given text input. A typical ViGGO dataset example has the output structured functional representation consisting of a single function with attributes and attribute values.\nText:\nYou said that you liked Crysis. Do you often play first person games from Crytek Frankfurt?\n\nFunctional Representation:\nverify_attribute(name[Crysis], developer[Crytek Frankfurt], rating[good], player_perspective[first person])\nThe function and attributes must be one of the following, respectively:\nFunction:\n['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', \n'suggest', 'request_explanation', 'recommend', 'request_attribute']\n\nAttributes:\n['name', 'release_year', 'esrb', 'genres', 'platforms', 'available_on_steam',\n'has_linux_release', 'has_mac_release', 'specifier', 'rating', \n'player_perspective', 'has_multiplayer', 'developer', 'exp_release_date']\nNote: Since I am not training/fine-tuning any model, I will only consider the ViGGO validation datasetfor this exercise.\n\n\nPrompt for generating the functional representation\nI use modified version of prompt template used in Anyscale‚Äôs blog for the ViGGO dataset. The prompt template is a few-shot prompt with examples from each function category to assist the model in understanding the intended output response representation.\nPROMPT_TEMPLATE = \"\"\"\nGiven a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. \nThis function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n\nThe attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']. The order your list the attributes within the function must follow the order listed above. For example the 'name' attribute must always come before the 'exp_release_date' attribute, and so forth.\n\nFor each attribute, fill in the corresponding value of the attribute within brackets. A couple of examples are below. Note: you are to output the string after \"Output: \". Do not include \"Output: \" in your answer.\n\nExample 1)\nSentence: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\nOutput: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n\nExample 2) \nSentence: Were there even any terrible games in 2014?\nOutput: request(release_year[2014], specifier[terrible])\n\nExample 3)\nSentence: Adventure games that combine platforming and puzzles  can be frustrating to play, but the side view perspective is perfect for them. That's why I enjoyed playing Little Nightmares.\nOutput: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\n\nExample 4)\nSentence: Since we're on the subject of games developed by Telltale Games, I'm wondering, have you played The Wolf Among Us?\nOutput: recommend(name[The Wolf Among Us], developer[Telltale Games])\n\nExample 5) \nSentence: Layers of Fear, the indie first person point-and-click adventure game?\nOutput: confirm(name[Layers of Fear], genres[adventure, indie, point-and-click], player_perspective[first person])  \n\nExample 6) \nSentence: I bet you like it when you can play games on Steam, like Worms: Reloaded, right?  \nOutput: suggest(name[Worms: Reloaded], available_on_steam[yes])\n\nExample 7)\nSentence: I recall you saying that you really enjoyed The Legend of Zelda: Ocarina of Time. Are you typically a big fan of games on Nintendo rated E (for Everyone)?    \nOutput: verify_attribute(name[The Legend of Zelda: Ocarina of Time], esrb[E (for Everyone)], rating[excellent], platforms[Nintendo])\n\nExample 8)\nSentence: So what is it about the games that were released in 2005 that you find so excellent?  \nOutput: request_explanation(release_year[2005], rating[excellent])\n\nExample 9)\nSentence: Do you think Mac is a better gaming platform than others?\nOutput: request_attribute(has_mac_release[])\n\nGive the output for the following sentence:\n{input}\n\"\"\"\nThe typical responses for the above template are not perfect but it can be used for generating structured output for the full dataset.\nGround Truth: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[simulation, sport])\nGPT Response: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[sports, simulation])\n\n\nGround Truth: request(player_perspective[side view], specifier[easy])\nGPT Response: Output: request(genres[side view], rating[top], specifier[easy])\n\n\nGround Truth: recommend(name[Need for Speed: The Run], platforms[Xbox])\nGPT Response: confirm(name[Need for Speed: The Run], platforms[Xbox])\n\n\nEvaluation criteria\nOften, you require custom evaluation criteria for custom tasks. For structured functional representation extraction, I define the following binary criteria:\n\nFunction Name Match: The function name must match the ground truth function name.\nFunction and Attributes Match: The generated function name and attributes must match the ground truth function attributes. However, the order of the attributes does not matter.\nFunction, Attributes, and Values Match: The generated function name, attributes, and values must match the ground truth function attributes and values. The order of the attributes and values does not matter.\nExact Match: The generated function must exactly match the ground truth function.\n\nThe above criteria are in order of increasing strictness. The first criterion is the least strict, and the last criterion is the most strict. These criteria will help me evaluate the model‚Äôs performance.\n\n\nEvaluation Strategy\nAlthough not ideal, I ask the model to evaluate its own performance on the given task. This approach has limitations, as it could potentially introduce bias in the evaluation process. However, it serves as a starting point for our analysis. I ask the model to compare the generated function with the ground truth function and provide a boolean score based on the above evaluation criteria.\nSince, I need the evaluation scores in a more structured format to analyze the model‚Äôs performance effectively. I use pydantic and instructor packages to obtain the evaluation scores in a structured format. I use the following prompt and pydantic model to evaluate the GPT-4o performance:\nclass EvaluateFunctionRepresentation(BaseModel):\n    function_match: bool = Field(description=\"The function name is the same but the attributes and values can be different.\")\n    function_attribute_match: bool = Field(description=\"The function and the attributes are the same but the values can be different.\")\n    function_attributes_values_match: bool = Field(description=\"The generated representation has same function and attributes and corresponding values without the same attributes and values order.\")\n    exact_match: bool = Field(description=\"The generated representation is exactly the same as the ground truth representation.\")\n\nPROMPT_TEMPLATE_SCORE_EVAL = \"\"\"I will provide you with two functional representations strings. One will be the ground truth representation (ground_truth) and the other will be a generated representation (generated). You need to compare the generated representation with the ground truth representation and provide the following similarity match in true or false:\n1) function_match\n2) function_attributes_match\n3) function_attributes_values_match\n4) exact_match\n\nA typical functional representation is of this form: function(attribute1[values], attribute2[values], attribute3[values], ...). \n\nGiven the following two functional representation, provide the similarity scores for the following:\n\nground_truth: {ground_truth}\n\ngenerated: {generated}\n\nLet's think step by step.\n\"\"\"\nPlease go through the GPT-4o baseline evaluation notebook for more details on prompt template and evaluation process and responses.\n\n\nEvaluating the performance\nUsing the above evaluation strategy, I generate the average evaluation scores for the full dataset.\n\n\n\nGPT-4o Evaluation Metrics\n\n\nThe task of generating functional representations from natural language sentences is challenging as seen in the above scores. The best the model can do is to provide the exact match only for 30% of the examples. Even the correct function name evaluation score is only about 80%. These results indicate that while GPT-4o shows some capability in extracting functional representations, there‚Äôs significant room for improvement.\n\n\nConclusion\nThe above evaluation exercise provides a good baseline and useful evaluation criteria/metrics that can be used to assess the performance of other models on the same task in the future. Generating functional representations is a complex task and is not easily accomplished using prompt-engineering techniques like few-shot learning as seen in the results.\n\n\nNext steps‚Ä¶\n\nEvaluate other large models like Claude, Gemini, and Llama-3‚Äì70B on the same task to compare their performance. This will provide a broader perspective on how different LLMs handle structured data extraction.\nExplore alternative evaluation methods that don‚Äôt rely on the model evaluating itself. Most importantly, to eliminate potential biases in the evaluation process.\nFine-tune smaller models like Llama-3‚Äì8B or Mistral-7B for this domain-specific structured representation task. Fine-tuning will not only improve the model‚Äôs performance but also enhance latency and reduce the number of input tokens required to generate the output.\nInvestigate the impact of different prompt engineering techniques on the model‚Äôs performance.\n\n\n\nReferences\n\nGPT-4o baseline evaluation notebook\nOpenAI GPT-4o\nViGGO Dataset\nAnyscale Blog on Fine-tuning Llama 2\nPydantic Documentation\nInstructor Package\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "posts/2024-05-19-manage-multiple-cuda.html",
    "href": "posts/2024-05-19-manage-multiple-cuda.html",
    "title": "Managing multiple CUDA versions using environment modules in Ubuntu",
    "section": "",
    "text": "Latest Update: May 19th, 2024\nThis blog contains all the steps required to: - Install multiple CUDA versions (e.g., CUDA 11.8 andCUDA 12.1 - Manage multiple CUDA environments on Ubuntu using the utility called environment modules. - Use this approach to avoid CUDA environment conflicts."
  },
  {
    "objectID": "posts/2024-05-19-manage-multiple-cuda.html#resources-and-helpful-links",
    "href": "posts/2024-05-19-manage-multiple-cuda.html#resources-and-helpful-links",
    "title": "Managing multiple CUDA versions using environment modules in Ubuntu",
    "section": "Resources and helpful links",
    "text": "Resources and helpful links\n\nhttps://ubuntu.com/server/docs/nvidia-drivers-installation\nhttps://developer.nvidia.com/cuda-toolkit-archive\nhttps://developer.nvidia.com/cudnn-downloads\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "pages/jsalt.html",
    "href": "pages/jsalt.html",
    "title": "Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis",
    "section": "",
    "text": "This post describes how to reproduce and extend our pipeline of speech separation, diarization and ASR. We have provided separated audio files along with entire end-to-end reproducible recipes to supplement our SLT 2021 paper.\nHere is a summary of contents of this post:\n\nSummary of our pipeline\nDatasets\nReproducible recipes\nExample use cases (for extending this work)\nCredits\n\n\n\nSummary of our pipeline\nOur speech processing pipeline consists of the following 3 stages in sequence. For each of these stages, we experimented with the methods mentioned below.\n\nSpeech separation\n\nMask-based MVDR\nSequential neural beamforming\n\nSpeaker diarization\n\nClustering: Agglomerative hierarchical clustering, spectral clustering, Variational Bayes based x-vector clustering (VBx)\nRegion proposal networks\nTarget speaker voice activity detection\n\nSpeech recognition (ASR)\n\nHybrid TDNNF-based\nEnd-to-end transformer based\n\n\n\n\n\nDatasets\nIn our paper, we compare the performance of our models on mixed and separated audio, where the separation is performed using the methods mentioned earlier.\n\nDownloading and preparing the mixed (original) LibriCSS data\n$ git clone https://github.com/chenzhuo1011/libri_css.git\n$ conda env create -f conda_env.yml\n$ conda activate libricss_release\n$ cd libri_css && ./dataprep/scripts/dataprep.sh\n\n\nDownloading the separated audio data\nAdditionally, we also provided 2-stream and 3-stream separated audio wav files through Zenodo. You can download them using the following commands:\n$ wget https://zenodo.org/record/4415163/files/libricss_mvdr_2stream.tar.gz\n$ wget https://zenodo.org/record/4415163/files/libricss_sequential_3stream.tar.gz\nOnce the archived files are downloaded, you can extract them and then use the path in the Kaldi or ESPNet recipes.\n\n\n\n\nReproducible recipes\n\nKaldi recipe\nWe have provided Kaldi recipes s5_mono and s5_css here.\n\ns5_mono: This is a single channel diarization + ASR recipe which takes as the input a long single-channel recording containing mixed audio. It then performs SAD, diarization, and ASR on it and outputs speaker-attributed transcriptions, which are then evaluated with cpWER (similar to CHiME6 Track 2).\ns5_css: This pipeline uses a speech separation module at the beginning, so the input is 2-3 separated audio streams. We assume that the separation is window-based, so that the same speaker may be split across different streams in different windows, thus making diarization necessary.\n\ns5_mono evaluates diarization and ASR on mixed audio, while s5_css does the same for separated audio streams.\nNote: Only clustering-based diarization is available in this recipe at the time of making this post, but we are also preparing RPN and TS-VAD setups.\nFor ease of reproduction, we have included training stages in the s5_mono recipe. We also provide pretrained models for both diarization and ASR systems:\n\nSAD: CHiME-6 baseline TDNN-Stats SAD available here.\nSpeaker diarization: CHiME-6 baseline x-vector + AHC diarizer, trained on VoxCeleb with simulated RIRs available here.\nASR: We used the chain model trained on 960h clean LibriSpeech training data available here. It was then additionally fine-tuned for 1 epoch on LibriSpeech + simulated RIRs. For LM, we trained a TDNN-LSTM language model for rescoring. All of these models are available at this Google Drive link.\n\n\n\nESPNet recipe\nThe ESPNet recipe corresponding to the Kaldi s5_mono recipe is available here as asr1. A recipe corresponding to s5_css is not available yet, but it should be simple to extend asr1 similar to the s5_css recipe, since it follows Kaldi-style diarization. For help or other details, please contact Pavel Denisov who created the ESPNet LibriCSS recipe.\n\n\n\n\nExample use cases (for extending this work)\nLet us now look at how this research may be extended through 2 examples.\n\nExample 1: A new window-based separation method\nSuppose you have a new window-based ‚Äúcontinuous‚Äù speech separation model, and you want to evaluate the downstream ASR performance (and compare it with the methods in our paper). This can be done as follows:\n\nDownload and prepare the ‚Äúmixed‚Äù LibriCSS audio data as described here.\nRun your separation method on this data and store the generated audio streams using the following naming convention: overlap_ratio_10.0_sil0.1_1.0_session7_actual10.1_channel_1.wav. It is similar to the naming of the original LibriCSS files, with the addition of *_channel_1* at the end which denotes the stream.\n\nNote: channel here does not refer to the microphone; it refers to the separated stream; so if your model separated the audio into 2 streams, they would have the suffixes *_channel_0* and *_channel_1*.\n\nStore all the output wav files in a directory. They can have any hierarchy within this directory as long as they follow the naming convention.\nDownload and install Kaldi, and navigate to the egs/libri_css/s5_css folder.\nIn the run.sh, replace the paths to the mixed LibriCSS data and your own separated audio files.\nRun the script. It is recommended to run the stages one by one, since the evaluation outputs after the SAD and diarization stage are also printed to the standard output.\nAt the end of decoding, the cpWERs will be printed as follows:\n\nDev WERs:\nbest_wer_session0_CH0_0L %WER 10.98 [ 130 / 1184, 34 ins, 12 del, 84 sub ]\nbest_wer_session0_CH0_0S %WER 15.10 [ 269 / 1782, 67 ins, 23 del, 179 sub ]\nbest_wer_session0_CH0_OV10 %WER 25.12 [ 465 / 1851, 156 ins, 85 del, 224 sub ]\nbest_wer_session0_CH0_OV20 %WER 18.86 [ 342 / 1813, 94 ins, 33 del, 215 sub ]\nbest_wer_session0_CH0_OV30 %WER 20.42 [ 395 / 1934, 117 ins, 40 del, 238 sub ]\nbest_wer_session0_CH0_OV40 %WER 28.47 [ 636 / 2234, 236 ins, 137 del, 263 sub ]\nEval WERs:\n0L %WER 22.36 [ 2446 / 10938, 785 ins, 413 del, 1248 sub ]\n0S %WER 19.81 [ 2970 / 14994, 861 ins, 431 del, 1678 sub ]\nOV10 %WER 21.39 [ 3412 / 15951, 1060 ins, 580 del, 1772 sub ]\nOV20 %WER 23.49 [ 3984 / 16963, 1128 ins, 747 del, 2109 sub ]\nOV30 %WER 26.06 [ 4789 / 18376, 1415 ins, 988 del, 2386 sub ]\nOV40 %WER 25.45 [ 4818 / 18932, 1410 ins, 676 del, 2732 sub ]\nThese can also be found at: exp/chain_cleaned/tdnn_1d_sp/decode_dev${data_affix}_diarized_2stage_rescore/scoring_kaldi_multispeaker/best_wer\nNote: To evaluate performance using end-to-end Transformer based ASR, you would need to first create an s5_css equivalent in ESPNet by extending the asr1 recipe.\n\n\nExample 2: A new cross-stream diarizer\nOne of the observations in our paper was that it is hard to perform good diarization on top of separated audio streams:\n\nMethods such as VBx cannot be used for this purpose because of their time continuity constraint.\nAlthough models like RPN and TS-VAD do well on mixed audio, they fail on separated audio due to a train-test mismatch (they were trained on simulated overlapping mixtures).\n\nYet, this ‚Äúseparation+diarization‚Äù system is very promising, especially considering that such a system from Microsoft obtained the best performance in the recent VoxConverse diarization challenge.\nSuppose you have a new diarization method which works across separated audio streams. To evaluate your method on LibriCSS:\n\nDownload the separated audio data from Zenodo.\nIf your method is implemented in Kaldi, clone and install Kaldi and follow the s5_css recipe until the diarization stage. Otherwise, run your implementation on the separated audio files and compute the final DER.\nYou can compare your performance against the results reported in Table 3 of our paper. The baselines can be reproduced by running the s5_css recipe till stage 3.\n\n\n\n\n\nCredits\nThis work was conducted during JSALT 2020 with support from Microsoft, Amazon, and Google. If you use the data or code in your research, consider citing:\n@article{Raj2021IntegrationOS,\n  title={Integration of speech separation, diarization, and recognition for multi-speaker \n  meetings: System description, comparison, and analysis},\n  author={Desh Raj and Pavel Denisov and Zhuo Chen and Hakan Erdogan and Zili Huang \n  and Maokui He and Shinji Watanabe and Jun Du and Takuya Yoshioka and Yi Luo and \n  Naoyuki Kanda and Jinyu Li and Scott Wisdom and John R. Hershey},\n  journal={2021 IEEE Spoken Language Technology ({SLT}) Workshop},\n  year={2021}}\n}\n\n\nOther resources\n\nPaper: Link\nSlides: Link"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\n\nTitle\n\n\n\n\n\n\n\n\n9/10/25\n\n\nA Guide to Building Custom Nodes in ComfyUI\n\n\n\n\n\n\n9/2/25\n\n\nKey Takeaways from Lecture 1: LLM Evaluation Lifecycle\n\n\n\n\n\n\n7/15/24\n\n\nPart III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction\n\n\n\n\n\n\n7/9/24\n\n\nPart II: Comparison of Model Performances on Structured Functional Representation Extraction\n\n\n\n\n\n\n7/3/24\n\n\nPart I: Baseline Evaluation of GPT-4o for Functional Representation Extraction\n\n\n\n\n\n\n6/30/24\n\n\nStep-by-Step Guide to Setup Your Personal GPU Server\n\n\n\n\n\n\n5/19/24\n\n\nManaging multiple CUDA versions using environment modules in Ubuntu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an applied machine learning engineer passionate about building AI-driven products to solve real-world problems. My motto is ‚ÄúApplied AI should always be product-driven‚Äù.\nCurrently, I am working as a Senior Machine Learning Engineer at Jiffy, where I build image enhancement pipelines, deploy custom in-house and open-source ML models, and develop LLM/VLM applications across different verticals. My work also spans improving search and recommendation systems and designing automated prompt-optimization workflows.\nPreviously at Flixstock, I predominantly developed vision generative AI solutions, with a focus on building virtual try-on and background generation solutions emphasizing identity and pose control.\nUnlike many in this field, my core background is not in computer science. My graduate and Ph.D.¬†research experience is in computational geophysics, and I worked for Shell as a research geophysicist for about three years. I believe this diverse background helps me approach problems from unique perspectives.\nI love connecting, chatting, and collaborating with new people. Whether you are a fellow professional, a new graduate, or someone interested in discussing AI-powered products, please do not hesitate to reach out.\nYou can also book a meeting with me using the Calendly link."
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n\nJiffy\n\n\nJan 2024 - Present\n\n\n\nSenior Machine Learning Engineer\n\n\nBuilding generative AI image pipelines, LLM/VLM applications, and evals across product verticals.\n\n\n\n\n\nFlixstock\n\n\nAug 2022 - Dec 2024\n\n\n\nSoftware Development Engineer III\n\n\nDeveloped vision-based generative AI solutions for virtual try-on and background generation.\n\n\n\n\n\nShell\n\n\nOct 2019 - Jul 2022\n\n\n\nResearch Geophysicist\n\n\nFocused on uncertainty quantification and denoising of subsurface seismic data.\n\n\n\n\n\nTU Delft\n\n\nAug 2015 - Sep 2019\n\n\n\nPhD Researcher\n\n\nWorked on a new imaging approach, JMI-res, for high-resolution subsurface physical properties."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aayush Garg",
    "section": "",
    "text": "I am an applied machine learning engineer focused on building AI-driven products and systems people can rely on. At Jiffy, I build image enhancement pipelines, deploy ML models at scale, and develop LLM/VLM applications across product verticals."
  },
  {
    "objectID": "index.html#current-work",
    "href": "index.html#current-work",
    "title": "Aayush Garg",
    "section": "üíº Current Work",
    "text": "üíº Current Work\nI am currently building an in-house AI design platform, developing evaluation and prompt-optimization workflows, and exploring applied LLM fine-tuning for structured data extraction. My background is in computational geophysics, which keeps me grounded in first principles and pragmatic experimentation.\nIf you‚Äôre interested in my work or want to collaborate, feel free to reach out via email or connect on LinkedIn."
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Aayush Garg",
    "section": "üìÆ Blog",
    "text": "üìÆ Blog\nI write about practical ML engineering, LLMs, and lessons learned from building AI products. Below are my recent posts.\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\n9/10/25\n\n\nA Guide to Building Custom Nodes in ComfyUI\n\n\n\n\n\n\n9/2/25\n\n\nKey Takeaways from Lecture 1: LLM Evaluation Lifecycle\n\n\n\n\n\n\n7/15/24\n\n\nPart III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction\n\n\n\n\n\n\n7/9/24\n\n\nPart II: Comparison of Model Performances on Structured Functional Representation Extraction\n\n\n\n\n\n\n7/3/24\n\n\nPart I: Baseline Evaluation of GPT-4o for Functional Representation Extraction\n\n\n\n\n\n\n6/30/24\n\n\nStep-by-Step Guide to Setup Your Personal GPU Server\n\n\n\n\n\n\n5/19/24\n\n\nManaging multiple CUDA versions using environment modules in Ubuntu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#follow-me",
    "href": "index.html#follow-me",
    "title": "Aayush Garg",
    "section": "üì¨ Follow Me",
    "text": "üì¨ Follow Me\nThe best way to follow my work is on GitHub or Twitter."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Target-oriented elastic parameter models estimation from surface seismic data using JMI-res\n\n\nGarg, A., Verschuur, D. J.\n\n\nInvited speaker for FWI workshop in 82nd EAGE Conference and Exhibition\n\n\n\n\n\n\n\n\n\nFrom surface seismic data to reservoir elastic parameters using a full-wavefield redatuming approach\n\n\nGarg, A., Verschuur, D. J.\n\n\nGeophysical Journal International, Volume 221, Issue 1, April 2020, Pages 115-128\n\n\n\n\nSpatial aliasing removal using deep learning super-resolution\n\n\nGarg, A., Vos, A., Bortych, N., Gupta, D. K., Verschuur, D. J.\n\n\nFirst Break, 37(9), 87-92\n\n\n\n\nSpatial aliasing removal using deep learning super-resolution\n\n\nGarg, A., Verschuur, D. J.\n\n\n89th Annual International Meeting, SEG Technical Program Expanded Abstracts, Pages 644-648\n\n\n\n\nBlock-Krylov methods for multi-dimensional deconvolution\n\n\nLuiken N., Garg, A.\n\n\n89th Annual International Meeting, SEG Technical Program Expanded Abstracts\n\n\n\n\nElastic parameters estimation using reservoir-oriented Joint Migration Inversion for Norwegian Sea field data\n\n\nGarg, A., H. Masoomzadeh, S. Baldock, Verschuur, D. J.\n\n\n81st EAGE Conference and Exhibition 2019, Jun 2019, Volume 2019, Pages 1-5\n\n\n\n\n\n\n\n\n\nIncluding internal multiples in joint migration inversion and redatuming of North Sea field data\n\n\nGarg, A., Verschuur, D. J.\n\n\n88th Annual International Meeting, SEG, Extended abstracts, Pages 4342-4346\n\n\n\n\nReservoir elastic parameters estimation from surface seismic data using JMI-res: A full-wavefield approach\n\n\nGarg, A., S. Sharma, Verschuur, D. J.\n\n\n80th EAGE Conference and Exhibition 2018, Jun 2018, Volume 2018, Pages 1-5\n\n\n\n\nUsing one-way propagators to build a full wavefield inversion process\n\n\nVerschuur, D. J., Davydenko, M., Garg, A.\n\n\n80th EAGE Conference and Exhibition 2018 Workshop Programme, Jun 2018, cp-556-00082\n\n\n\n\n\n\n\n\n\nElastic reflectivity preserving full-wavefield inversion\n\n\nGarg, A., Verschuur, D. J.\n\n\n87th Annual International Meeting, SEG Technical Program Expanded Abstracts, Pages 5561-5566\n\n\n\n\nHigh-resolution reservoir impulse response estimation\n\n\nGarg, A., Verschuur, D. J.\n\n\n87th SEG Technical Program Expanded Abstracts, Pages: 3516-3520\n\n\n\n\nAVP-preserving estimation of reservoir impulse responses\n\n\nGarg, A., Verschuur, D. J.\n\n\n79th EAGE Conference and Exhibition 2017, Jun 2017, Volume 2017, Pages 1-5\n\n\n\n\n\n\n\n\n\nReservoir impulse response estimation using joint migration inversion\n\n\nGarg, A., Verschuur, D. J.\n\n\n78th EAGE Conference and Exhibition 2016, May 2016, Volume 2016, Pages 1-5"
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "",
    "text": "Target-oriented elastic parameter models estimation from surface seismic data using JMI-res\n\n\nGarg, A., Verschuur, D. J.\n\n\nInvited speaker for FWI workshop in 82nd EAGE Conference and Exhibition"
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "",
    "text": "From surface seismic data to reservoir elastic parameters using a full-wavefield redatuming approach\n\n\nGarg, A., Verschuur, D. J.\n\n\nGeophysical Journal International, Volume 221, Issue 1, April 2020, Pages 115-128\n\n\n\n\nSpatial aliasing removal using deep learning super-resolution\n\n\nGarg, A., Vos, A., Bortych, N., Gupta, D. K., Verschuur, D. J.\n\n\nFirst Break, 37(9), 87-92\n\n\n\n\nSpatial aliasing removal using deep learning super-resolution\n\n\nGarg, A., Verschuur, D. J.\n\n\n89th Annual International Meeting, SEG Technical Program Expanded Abstracts, Pages 644-648\n\n\n\n\nBlock-Krylov methods for multi-dimensional deconvolution\n\n\nLuiken N., Garg, A.\n\n\n89th Annual International Meeting, SEG Technical Program Expanded Abstracts\n\n\n\n\nElastic parameters estimation using reservoir-oriented Joint Migration Inversion for Norwegian Sea field data\n\n\nGarg, A., H. Masoomzadeh, S. Baldock, Verschuur, D. J.\n\n\n81st EAGE Conference and Exhibition 2019, Jun 2019, Volume 2019, Pages 1-5"
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "",
    "text": "Including internal multiples in joint migration inversion and redatuming of North Sea field data\n\n\nGarg, A., Verschuur, D. J.\n\n\n88th Annual International Meeting, SEG, Extended abstracts, Pages 4342-4346\n\n\n\n\nReservoir elastic parameters estimation from surface seismic data using JMI-res: A full-wavefield approach\n\n\nGarg, A., S. Sharma, Verschuur, D. J.\n\n\n80th EAGE Conference and Exhibition 2018, Jun 2018, Volume 2018, Pages 1-5\n\n\n\n\nUsing one-way propagators to build a full wavefield inversion process\n\n\nVerschuur, D. J., Davydenko, M., Garg, A.\n\n\n80th EAGE Conference and Exhibition 2018 Workshop Programme, Jun 2018, cp-556-00082"
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications",
    "section": "",
    "text": "Elastic reflectivity preserving full-wavefield inversion\n\n\nGarg, A., Verschuur, D. J.\n\n\n87th Annual International Meeting, SEG Technical Program Expanded Abstracts, Pages 5561-5566\n\n\n\n\nHigh-resolution reservoir impulse response estimation\n\n\nGarg, A., Verschuur, D. J.\n\n\n87th SEG Technical Program Expanded Abstracts, Pages: 3516-3520\n\n\n\n\nAVP-preserving estimation of reservoir impulse responses\n\n\nGarg, A., Verschuur, D. J.\n\n\n79th EAGE Conference and Exhibition 2017, Jun 2017, Volume 2017, Pages 1-5"
  },
  {
    "objectID": "publications.html#section-4",
    "href": "publications.html#section-4",
    "title": "Publications",
    "section": "",
    "text": "Reservoir impulse response estimation using joint migration inversion\n\n\nGarg, A., Verschuur, D. J.\n\n\n78th EAGE Conference and Exhibition 2016, May 2016, Volume 2016, Pages 1-5"
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Speaker profile",
    "section": "",
    "text": "Desh is a PhD student at Johns Hopkins University, working in the Center for Language and Speech Processing (CLSP), advised by Sanjeev Khudanpur and Dan Povey. His research interests lie in the application of machine learning methods for speech and language tasks. He is currently working on speech recognition (a.k.a speech-to-text) and speaker diarization (a.k.a who spoke when) for multi-party conversations. He wants to build systems which can identify the speaker and transcribe their speech in real time, and do this for all of the world‚Äôs languages.\nHe spent summer 2021 building end-to-end multi-talker ASR systems at Microsoft. In summer 2022, he was an intern in the AI Speech team at Meta, where he explored target-speaker ASR models for improving transducer models in the presence of background speech. Desh has several publications at ICASSP, InterSpeech, SLT, and ASRU, with over 450 citations.\nPreviously, he graduated from the Indian Institute of Technology Guwahati in 2017 with a major in Computer Science, where his thesis was on deep learning methods for biomedical text. He has worked on building smart assistants at Samsung Research (India) and was an intern at Microsoft India, devising statistics APIs for the Enterprise Commerce team.\nWhen he is not doing ML, he likes to work out, climb boulders, play guitar, and read fiction."
  },
  {
    "objectID": "pages/doverlap.html",
    "href": "pages/doverlap.html",
    "title": "DOVER-Lap",
    "section": "",
    "text": "The code is publicly available under the MIT license: Github repo"
  },
  {
    "objectID": "pages/overlap-aware-sc.html",
    "href": "pages/overlap-aware-sc.html",
    "title": "Multi-class Spectral Clustering with Overlaps for Speaker Diarization",
    "section": "",
    "text": "This post describes the implementation of our paper ‚ÄúMulti-class spectral clustering with overlaps for speaker diarization‚Äù, accepted for publication at IEEE SLT 2021.\nThe code consists of two parts: overlap detector, and our modified spectral clustering method for overlap-aware diarization.\nWe have implemented the diarization recipe in Kaldi, and modified scikit-learn‚Äôs spectral clustering class for our modification. The entire code to reproduce our results is available at: https://github.com/desh2608/kaldi/tree/slt21_spectral\n\nInstallation and setup\nSince the recipe is implemented using Kaldi, you first need to install Kaldi by following the instructions at: https://github.com/kaldi-asr/kaldi/blob/master/INSTALL\nIn the Kaldi installation, miniconda is not installed by default. To install it, go to tools/ and run:\nextras/install_miniconda.sh\nInstall a modified version of scikit-learn in the miniconda Python installation:\n$HOME/miniconda3/bin/python -m pip install git+https://github.com/desh2608/scikit-learn.git@overlap\n\n\nUsage\nThe recipe containing the overlap detector and the spectral clustering for AMI can be found at egs/ami/s5c. Additionally, the recipe also contains example for different clustering methods, namely AHC, VBx, and spectral clustering, to reproduce the single-speaker baselines in the paper.\nThe run.sh script does not contain stages for training an x-vector extractor, since we used the same extractor from the CHiME-6 baseline system.\nThe key stages in the script are as follows.\n\n--stage 8: Trains the overlap detector.\n--stage 9: Performs decoding with a trained overlap detector.\n--stage 10: Performs spectral clustering informed by the output from stage 9.\n\n\n\nWhere to find the clustering implementation?\nWe use scikit-learn‚Äôs spectral clustering class to implement our modified clustering method. In the default scikit-learn class, the argument assign_labels can take on two values:\n\nkmeans: This performs the conventional spectral clustering using the Ng-Jordan-Weiss method.\ndiscretize: This implements the clustering described in this paper and we modify it for our implementation.\n\nWe modified the discretize() function here by adding an additional argument which specifies the overlap vector. The vector is used in L141-148 to assign a second label to the overlapping segments.\n\n\nPre-trained model\nFor all our clustering-based diarization experiments, we used the x-vector extractor that was provided with the CHiME-6 baseline system, and is available here. To use it, first download the extractor using wget and then extract it using tar -xvzf and copy the contents to your exp directory.\n\n\nCitation\nIf you find this code useful, consider citing our paper:\n@inproceedings{Raj2021MultiSC,\n  title={Multi-class spectral clustering with overlaps for speaker diarization},\n  author={Desh Raj and Zili Huang and Sanjeev Khudanpur},\n  booktitle={IEEE Spoken Language Technology Workshop},\n  year={2021},\n}\n\n\nQuestions\nFor any questions about using the code, you can contact me at draj@cs.jhu.edu."
  },
  {
    "objectID": "posts/2024-06-30-remote-gpu-server.html",
    "href": "posts/2024-06-30-remote-gpu-server.html",
    "title": "Step-by-Step Guide to Setup Your Personal GPU Server",
    "section": "",
    "text": "Github Gist Link\nI‚Äôve been using a GPU workstation with an RTX 4090 for almost a year now, and it‚Äôs been one of the best decisions I‚Äôve made. With a personal GPU server, you no longer need to rely on cloud-based GPU instances from services like RunPod or Vast.ai every time you want to run a job or try new models. The best part? No stress about recurring GPU instance costs! :-)\nHowever, I rarely work directly on my workstation. Instead, I prefer the flexibility of accessing the GPU remotely using my MacBook, whether I‚Äôm working from different locations within my home, from a co-working space, or a cozy cafe in another part of town.\nIn this blog, I will walk you through the steps to configure a personal GPU Ubuntu server.\nFor this guide, I assume you already have a workstation running Ubuntu with a GPU and it is connected to your local network"
  },
  {
    "objectID": "posts/2024-06-30-remote-gpu-server.html#setting-up-local-remote-access",
    "href": "posts/2024-06-30-remote-gpu-server.html#setting-up-local-remote-access",
    "title": "Step-by-Step Guide to Setup Your Personal GPU Server",
    "section": "Setting Up Local Remote Access",
    "text": "Setting Up Local Remote Access\nLet‚Äôs start by setting up local access, which will allow you to ssh into your GPU server when you‚Äôre on the same home Wi-Fi network. This is ideal for a work-from-home (WFH) setup where your workstation is running in a corner of your living space.\n\nInstall the SSH server\nFirst, we need to install an SSH (Secure Shell) server. This will allow you to securely access your GPU machine remotely. Open a terminal on your Ubuntu machine and run the following commands: bash  sudo apt update &&  sudo apt install openssh-server This command updates your package lists and installs the OpenSSH server.\nStart and Enable SSH Service\nNext, enable the SSH service using this command: bash  sudo systemctl enable --now ssh You can verify if the service is enabled by running: bash  sudo systemctl status ssh\nLook for a line starting with Active: active (running) for ssh.service. This indicates that the SSH service is up and running.\n\nNote: The OpenSSH server starts running on boot by default.\n\nConfigure the firewall\nTo allow SSH connections through the system firewall, you need to open the appropriate port. Ubuntu‚Äôs default firewall, UFW (Uncomplicated Firewall), makes this process straightforward: bash  sudo ufw allow ssh This command adds an exception to your firewall rules, permitting incoming SSH connections. You can check the SSH status with: bash  sudo ufw status You should see the output similar to: bash  To                         Action      From  --                         ------      ----  22/tcp                     ALLOW       Anywhere  22/tcp(v6)                 ALLOW       Anywhere (v6)\nConnect to the local server\nNow that your GPU server is set up, it‚Äôs time to test the connection. From your laptop (which should be on the same local network as your GPU machine), open a terminal and use the following command: bash  ssh user@local-ip-address Replace user with your Ubuntu user and local-ip-address with the IP address of your GPU machine on the local network.\n\nTo find your username on the workstation, you can use the whoami command.\nTo find your local IP address, use one of these methods on your workstation:\n\nRun hostname -I and use the first address listed.\nUse ip addr show | grep -w inet for more detailed network information.\nHow to find my IP address on Ubuntu Linux is a great blog on it. It explains multiple commands like ip addr show | grep -w inet or networkctl status to get the local IP address.\n\n\n\nYour local IP address typically starts with 192.168.\nNote: If your router dynamically changes the local IP address of your workstation, it‚Äôs best to log into your router and assign a fixed local IP address to ensure consistent access.\n\nIf everything is configured correctly, you‚Äôll be prompted to enter your password, after which you‚Äôll gain remote access to your GPU server.\nSet Up SSH Keys for Passwordless Login\nIt is recommended to set up key-based authentication for better security and convenience purposes. This allows you to connect to your remote server without entering a password each time.\n\nIt is quite common to setup ssh key-based authentication.\nFor detailed instructions on setting up SSH keys, refer to the DigitalOcean guide on Setting up SSH keys on Ubuntu 20.04."
  },
  {
    "objectID": "posts/2024-06-30-remote-gpu-server.html#setting-up-external-remote-access",
    "href": "posts/2024-06-30-remote-gpu-server.html#setting-up-external-remote-access",
    "title": "Step-by-Step Guide to Setup Your Personal GPU Server",
    "section": "Setting Up External Remote Access",
    "text": "Setting Up External Remote Access\nWhile local access is great for working within your home network, sometimes you need to access your GPU workstation from outside your local network, such as from co-working spaces or a cozy cafe.\nOne simple and secure way to achieve this is by using ngrok.\n\nngrok helps creates secure tunnels from public endpoints to locally running services. It allows you to expose your personal server to the internet, enabling remote access from anywhere without complex network configurations.\n\nHere‚Äôs how to set it up:\n\nInstall ngrok\nFirst, you need to install ngrok on your GPU workstation. Open a terminal and run this command: bash  snap install ngrok\n\nFor more installation options, see https://dashboard.ngrok.com/get-started/setup/linux.\n\nCreate and connect to ngrok Account\nVisit ngrok‚Äôs website and sign up for a free account if you haven‚Äôt already. After signing up, you‚Äôll receive an auth token. On your GPU workstation, run: bash  ngrok config add-authtoken YOUR_AUTH_TOKEN You can get the config file path and edit using ngrok config check and vim &lt;path&gt;, respectively.\nStart the ngrok Tunnel\nNow, you can create a secure tunnel to your SSH service: bash  ngrok tcp 22 This command will display a URL that looks like tcp://X.tcp.ngrok.io:PORT. Note down this URL.\nConnect to Your Workstation\nFrom any external laptop, you can now SSH into your GPU workstation using: bash  ssh -p YYYY user@X.tcp.ngrok.io Replace PORT with the port number and X with the subdomain from the ngrok URL. Replace user with your Ubuntu username.\nThe above steps ensure that you can remotely access the workstation from external network. However, no one is going to manually start the ngrok every time before heading out.\nMake ngrok start automatically on boot\nTo ensure ngrok starts automatically when your workstation boots:\n\nCreate a new service file:\n\nsudo vim /etc/systemd/system/ngrok.service\n\nAdd the following content:\n\n[Unit]\nDescription=start ngrok tunnel on startup\nAfter=network.target\n\n[Service]\nExecStart=/snap/bin/ngrok tcp 22\nRestart=on-failure\nUser=&lt;your_username&gt;\n\n[Install]\nWantedBy=multi-user.target\nReplace &lt;your_username&gt; with your Ubuntu username. Save the file and exit the editor.\n\nEnable and start the service:\n\nsudo systemctl enable ngrok.service\nsudo systemctl start ngrok.service\nNow ngrok will automatically start and create a tunnel when your workstation boots.\n\nNote: With a free account, ngrok assigns a new port (YYYY) each time your workstation boots. You can get the new port from the ngrok dashboard.\n\nPaid ngrok account for dedicated port\nFor a dedicated TCP endpoint port that doesn‚Äôt change on reboot, you need a paid ngrok personal account ($10/month).\n\nReserve a tcp endpoint\n\nOnce you have a paid account, reserve a TCP endpoint at https://dashboard.ngrok.com/cloud-edge/tcp-addresses.\n\nUpdate the ngrok service file\n\nAdd the following content:\n[Unit]\nDescription=start ngrok tunnel on startup\nAfter=network.target\n\n[Service]\nExecStart=/snap/bin/ngrok tcp --region=&lt;region&gt; --remote-addr=&lt;remote-address&gt; 22\nRestart=on-failure\nUser=&lt;your_username&gt;\n\n[Install]\nWantedBy=multi-user.target\nReplace &lt;region&gt;, &lt;remote-address&gt;, and &lt;your_username&gt; with the appropriate values from your reserved TCP endpoint config.\n\nWith this setup, your SSH remote endpoint will remain the same even if the system reboots.\n\nReference Links:\n\nUbuntu SSH Documentation\nDigitalOcean Guide on Setting Up SSH Keys\nngrok Documentation\nIP Address Information for Ubuntu\nUFW (Uncomplicated Firewall) Guide\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "posts/2024-07-09-compare-models-structured-data.html",
    "href": "posts/2024-07-09-compare-models-structured-data.html",
    "title": "Part II: Comparison of Model Performances on Structured Functional Representation Extraction",
    "section": "",
    "text": "In the previous blog post, I established a performance baseline using GPT-4o for generating structured data, particularly functional representations, from text using the ViGGO Dataset.\nBuilding on that foundation, I expand the experiment to include a broader range of models, both open-source and proprietary. This comparison aims to provide insights on how well these models perform out of the box in structured data extraction tasks, which is quite crucial for RAG applications, knowledge base construction, and reasoning systems.\nI evaluate and compare the performance of these six LLM models:\n\nGPT-4o: OpenAI‚Äôs latest iteration of the GPT-4 model, known for its faster generation, advanced natural language understanding and generation capabilities. Currently one of the most popular and capable models.\nClaude Sonnet-3.5: Anthropic‚Äôs refined language model with enhanced reasoning abilities. It aims to provide more context-aware outputs compared to earlier versions and has recently outperformed GPT-4o on many benchmarks.\nGemini-1.5-Flash: Google DeepMind‚Äôs streamlined version of the Gemini model, optimized for faster inference and reduced computational requirements.\nllama-3-70b-instruct: Meta‚Äôs large-scale instruction-tuned language model, part of the latest LLaMA 3 family, with 70 billion parameters. It‚Äôs designed to follow complex instructions and generate high-quality text across diverse domains.\nmixtral-8x7b-instruct-v0.1: Mistral AI‚Äôs instruction-tuned variant of the Mixtral 8x7B model, known for its mixture-of-experts architecture.\nllama-3-8b-instruct: A more compact version of Meta‚Äôs LLaMA 3 family, with 8 billion parameters, optimized for instruction following.\n\n\nNote: I‚Äôve included the smaller Llama-3-8B model as I plan to finetune it‚Äôs base 8B model in coming days. It would help me compare the general instruction finetuned 8B model performance."
  },
  {
    "objectID": "posts/2024-07-09-compare-models-structured-data.html#references",
    "href": "posts/2024-07-09-compare-models-structured-data.html#references",
    "title": "Part II: Comparison of Model Performances on Structured Functional Representation Extraction",
    "section": "References",
    "text": "References\n\nPart I blog post of series\nViGGO Dataset\nNotebook I: Generate_responses_all_llms.ipynb\nNotebook II: Compare_models_performances.ipynb\n\n\nThanks for reading! If you have any questions or feedback, please let me know on Twitter or LinkedIn."
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "",
    "text": "A couple of months back, I enrolled in AI Evals for Engineers and PMs, a course by Hamel and Shreya. The live cohort for ot ran from July to mid-August, but due to work commitments I couldn‚Äôt follow along in real time.\nI have now started following it as a self-paced course and plans to write a blog for each lesson as I progress. This will be my way to capture what I learn and to reflect on the material. In this first blog ü§û, I‚Äôll walk through my key takeaways from introductory Lecture 1."
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#evaluation-isnt-optional-but-fundamental",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#evaluation-isnt-optional-but-fundamental",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "1. Evaluation isn‚Äôt Optional but Fundamental",
    "text": "1. Evaluation isn‚Äôt Optional but Fundamental\nAnyone who has built or worked with LLM pipelines knows that their outputs are open-ended, subjective, and unstructured (unless you enforce it). If you rely on ad-hoc checks which I have been guilty of, it often leads to knee-jerk fixes. Moreover, it completely miss the long-term need of continuous tracking which is essential for improving your pipeline reliability and usefulness. This is why Evaluation‚Äîthe systematic measurement of an LLM pipeline quality‚Äîis critical!"
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#the-three-gulfs",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#the-three-gulfs",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "2. The Three Gulfs",
    "text": "2. The Three Gulfs\nThe below image beautifully captures and categorizes the challenges associated with any LLM application: \n\nGulf of Comprehension: This is a result of limited understanding of the input data (user queries) and the pipeline‚Äôs outputs (behavior). Bridging it requires examining examples to identify common failure modes. This brings it own challenge: ‚ÄúHow to manually review every input or output to identify failure modes?‚Äù\nGulf of Specification: It refers to the difficulty of translating a user‚Äôs high-level intent into unambiguous precise instructions for the LLM. Bridging it requires writing detailed prompts that captures ‚Äútrue intent‚Äù which in itself is challenging due to ambiguous nature of natural language.\nGulf of Generalizaton: This is due to LLMs unexpected and inconsistent behavior on new or unusual (out of distribution) inputs. Bridging it requires a good understanding of your LLM model capabilities. This leads to the question: ‚ÄúHow to improve LLM model?‚Äù"
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#analyze-measure-improve-lifecycle",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#analyze-measure-improve-lifecycle",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "3. Analyze ‚Üí Measure ‚Üí Improve Lifecycle",
    "text": "3. Analyze ‚Üí Measure ‚Üí Improve Lifecycle\nHamel and Shreya introduced a structured way to bridge the above gulfs: Analyze ‚Üí Measure ‚Üí Improve lifecycle.\n\n\n\nAnalyze ‚Üí Measure ‚Üí Improve Lifecycle\n\n\nHowever, the most important takeaways for me was not what each phase means but the pitfalls that often derail them:\n\n\n\n\n\n\n\n\nPhase\nPitfalls\nNotes\n\n\n\n\nAnalyze\nOutsourcing annotation; looking at too few examples and forming shaky hypotheses\nThis is where you learn the most. Spend ~75‚Äì80% of your time here‚Äîgood analysis sets up everything else.\n\n\nMeasure\nMisaligned or poorly designed LLM judges; ‚Äúoverfitting‚Äù by testing judges on the same examples used in the judge prompt\nIn this phase, you need the rigor of data science. NEVER leak test data into judge prompts.\n\n\nImprove\nPrematurely jumping to fixes; defaulting to the most complex solution first (fine-tuning, bigger models)\nStart simple. Prompt tweaks and improvements often go a long way before heavier changes are needed."
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#llms-are-imperfectprompt-iteratively",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#llms-are-imperfectprompt-iteratively",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "4. LLMs are Imperfect‚ÄîPrompt Iteratively",
    "text": "4. LLMs are Imperfect‚ÄîPrompt Iteratively\nWhen we write prompts it‚Äôs easy to ignore that LLMs are non-deterministic, prompt-sensitive and can confidently hallucinate. Thus, always remember: ‚ÄúLLMs are powerful but imperfect components. Leverage strengths, anticipate weaknesses.‚Äù\n\n\n\nLLM Strengths vs.¬†Weaknesses\n\n\nEffective prompting starts with you. You should not delegate the prompting to an LLM or you will miss important failure modes. Instead, write your own draft prompt and if needed, use an LLM only to polish clarity.\nFrom there on, treat prompting as an iterative process where the first draft is a starting point which you refine based on observed outputs."
  },
  {
    "objectID": "posts/2025-09-02-llm-evaluation-lifecycle.html#reference-based-vs-reference-free-metrics",
    "href": "posts/2025-09-02-llm-evaluation-lifecycle.html#reference-based-vs-reference-free-metrics",
    "title": "Key Takeaways from Lecture 1: LLM Evaluation Lifecycle",
    "section": "5. Reference-based vs Reference-free Metrics",
    "text": "5. Reference-based vs Reference-free Metrics\nThe evaluation metrics broadly fall into two categories: reference-free and reference-based. Both of them are useful but in different contexts.\n\n\n\n\n\n\n\n\n\nReference-Free\nReference-Based\n\n\n\n\nWhat it means\nEvaluates properties of the output itself (no golden answer required)\nCompares output against a golden reference or ground truth\n\n\nWhen to use\nCreative or open-ended tasks, formatting/structure checks, validity tests\nTasks with clearly defined correct answers (e.g., factual QA, deterministic outputs)\n\n\nExamples\n- Does the output follow the JSON format?- Does generated code/SQL run without errors?\n- Exact match against a gold SQL query- ROUGE/BLEU score for text generation"
  },
  {
    "objectID": "posts/help.html",
    "href": "posts/help.html",
    "title": "Aayush Garg",
    "section": "",
    "text": "In this point I will focus on why LLM pipelines evaluations is hard and matters. Then what is evaluation in terms Shreya and Hamel and why it is important to avoid knee jerk fixes.Evaluation is the systematic measurement of an LLM pipeline‚Äôs quality‚Äîencompassing safety, reliability, and usefulness‚Äîrather than relying on a single metric \nIn this I will talk about the three challenges with LLM application development in form of three gulfs.Developers must bridge three ‚Äúgulfs‚Äù in LLM pipeline development‚ÄîComprehension (understanding data and model failures), Specification (translating intent into precise prompts), and Generalization (ensuring robust behavior on new inputs) \nIn this i will talk Analyze ‚Üí Measure ‚Üí Improve about Effective LLM evaluation follows an iterative Analyze ‚Üí Measure ‚Üí Improve lifecycle to systematically uncover, quantify, and address application-specific failures \nWhat LLM strengths and weaknesses are. LLMs bring strengths (fluent text generation, summarization, few-shot learning) but also weaknesses (non-determinism, hallucination, limited algorithmic reasoning) that evaluation must explicitly address \nWhy it is important to write a good initial prompt. Why iterative refinement matters. A well-specified initial prompt must include: role and objective, clear ‚Äúdo‚Äôs and don‚Äôts,‚Äù relevant context, few-shot examples, reasoning directives, output formatting constraints, and delimiters to reduce ambiguity * Iterative prompt refinement is crucial‚Äîfirst drafts are starting points that must be tested and improved before deep error analysis \nWhat is good and bad behavior as per them. Defining ‚Äúgood‚Äù vs.¬†‚Äúbad‚Äù behavior begins by brainstorming user pain points to derive concrete specifications (what the bot must and must not do) \nDifferent evaluation metrics.Evaluation metrics split into reference-based checks (comparing outputs to known ‚Äúgold‚Äù answers) and reference-free checks (verifying inherent output properties), each suited to different failure modes \nFinish with the first most important step in evaluation is Clarifying specification failures (ambiguous prompts) should precede measuring generalization failures, ensuring that metrics focus on true model limitations rather than prompt misunderstandings THis is where error analysis becomes your super-power. Robust error analysis‚Äîbootstrapping representative traces, open coding to identify failures, and axial coding to cluster them‚Äîlays the foundation for meaningful measurement and improvement"
  },
  {
    "objectID": "posts/help.html#key-takeaways",
    "href": "posts/help.html#key-takeaways",
    "title": "Aayush Garg",
    "section": "",
    "text": "In this point I will focus on why LLM pipelines evaluations is hard and matters. Then what is evaluation in terms Shreya and Hamel and why it is important to avoid knee jerk fixes.Evaluation is the systematic measurement of an LLM pipeline‚Äôs quality‚Äîencompassing safety, reliability, and usefulness‚Äîrather than relying on a single metric \nIn this I will talk about the three challenges with LLM application development in form of three gulfs.Developers must bridge three ‚Äúgulfs‚Äù in LLM pipeline development‚ÄîComprehension (understanding data and model failures), Specification (translating intent into precise prompts), and Generalization (ensuring robust behavior on new inputs) \nIn this i will talk Analyze ‚Üí Measure ‚Üí Improve about Effective LLM evaluation follows an iterative Analyze ‚Üí Measure ‚Üí Improve lifecycle to systematically uncover, quantify, and address application-specific failures \nWhat LLM strengths and weaknesses are. LLMs bring strengths (fluent text generation, summarization, few-shot learning) but also weaknesses (non-determinism, hallucination, limited algorithmic reasoning) that evaluation must explicitly address \nWhy it is important to write a good initial prompt. Why iterative refinement matters. A well-specified initial prompt must include: role and objective, clear ‚Äúdo‚Äôs and don‚Äôts,‚Äù relevant context, few-shot examples, reasoning directives, output formatting constraints, and delimiters to reduce ambiguity * Iterative prompt refinement is crucial‚Äîfirst drafts are starting points that must be tested and improved before deep error analysis \nWhat is good and bad behavior as per them. Defining ‚Äúgood‚Äù vs.¬†‚Äúbad‚Äù behavior begins by brainstorming user pain points to derive concrete specifications (what the bot must and must not do) \nDifferent evaluation metrics.Evaluation metrics split into reference-based checks (comparing outputs to known ‚Äúgold‚Äù answers) and reference-free checks (verifying inherent output properties), each suited to different failure modes \nFinish with the first most important step in evaluation is Clarifying specification failures (ambiguous prompts) should precede measuring generalization failures, ensuring that metrics focus on true model limitations rather than prompt misunderstandings THis is where error analysis becomes your super-power. Robust error analysis‚Äîbootstrapping representative traces, open coding to identify failures, and axial coding to cluster them‚Äîlays the foundation for meaningful measurement and improvement"
  },
  {
    "objectID": "posts/help.html#points-mentioned-by-hamel-and-shreya",
    "href": "posts/help.html#points-mentioned-by-hamel-and-shreya",
    "title": "Aayush Garg",
    "section": "Points mentioned by Hamel and Shreya",
    "text": "Points mentioned by Hamel and Shreya\n\nHamel\nHamel‚Äôs Key Points:\n\nOn Data Understanding: ‚ÄúBefore jumping to metrics, before jumping to anything else, you have to really look at your data and comprehend it and comprehend your problem specifically enough.‚Äù\nOn Metrics Selection: ‚ÄúOne of the metrics they used in their EVOLS was edit distance‚Ä¶ Actually, that turned out to be a very bad metric.‚Äù\nOn Annotation Importance: ‚ÄúYou don‚Äôt want to outsource your annotation‚Ä¶ That‚Äôs just an engineering thing. This EV is an engineering thing only, that‚Äôs a really big mistake.‚Äù\nOn Analysis Phase: ‚ÄúThe analyzed phase is probably the most important phase of them all‚Ä¶ That‚Äôs the thing that most people skip, that‚Äôs the thing where there‚Äôs probably the least guidance out there in the industry.‚Äù\nOn Prompt Development: ‚ÄúYou should definitely‚Ä¶ pay attention to your prompt‚Ä¶ a lot of times I‚Äôve seen prompts that are clearly that a human hasn‚Äôt looked at.‚Äù\nOn User Understanding: ‚ÄúIf you‚Äôre developing some kind of medical application and you‚Äôre not a doctor, then you might not really understand what is good and bad. In that case, it‚Äôs really important that you understand your user quite intimately.‚Äù\nOn Error Analysis: ‚ÄúError analysis is going to be one of your superpowers. It‚Äôs something that is not really taught anywhere else.‚Äù\n\nI cannot provide insights from ‚ÄúShreya‚Äù as this person is not identified as a speaker in the provided transcript.\n\n\nShreya\n\nOn Understanding Data & Queries: ‚ÄúPeople are not aware of what are the different types of queries that the products that they‚Äôre building are going to seem‚Ä¶ If you‚Äôre blind to this, you haven‚Äôt seen the full variety of user requests, then you can‚Äôt build an effective product.‚Äù\nOn Data Volume Challenges: ‚ÄúIf you‚Äôre building recipe bot, you might get thousands and tens of thousands‚Ä¶ of queries that are coming every single day. You can‚Äôt read all of that text.‚Äù\nOn Prompt Development Strategy: ‚ÄúWhat I like to do when writing a prompt is I like to write a draft of it myself, specifying bullet lists of dos and don‚Äôts‚Ä¶ then you can use an LLM to maybe improve the clarity of that.‚Äù\nOn Generalization Challenges: ‚ÄúI commonly test queries that the LLM gives a good output, but then it‚Äôs in production or some other user is working with it and then suddenly that output is wrong‚Ä¶ I don‚Äôt have a great mental model of when the LLM can and can‚Äôt generalize.‚Äù\nOn Time Investment in Analysis: ‚ÄúI ballpark like 75% to 80% of my time I spend in the analyze.‚Äù\nOn Components of Good Prompts:\n\n\n‚ÄúFirst, I‚Äôd like to start a prompt with the role and the objective‚Äù\n‚ÄúBe very specific, be very unambiguous in what to do and what not to do‚Äù\n‚ÄúSometimes that could be pulling in recipes from the internet, or maybe you want a personalized experience for the user‚Äù\n\n\nOn Specifications: ‚ÄúSpecifications are the only place to insert taste. And a model that is trained to solve everybody‚Äôs problems on every domain‚Ä¶ is not going to be able to know everybody‚Äôs specifications.‚Äù\nOn Moving Slowly: ‚ÄúIt‚Äôs very tempting to try to move as quickly as possible with large language models‚Ä¶ but it‚Äôs okay to move slowly. I find that every time I move too fast, I regret it later on.‚Äù"
  }
]