<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Aayush Garg</title>
<link>https://garg-aayush.github.io/</link>
<atom:link href="https://garg-aayush.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Notes on applied ML engineering, LLMs, and building AI products.</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Thu, 25 Dec 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Deriving the PPO Loss from First Principles</title>
  <link>https://garg-aayush.github.io/posts/2025-12-25-deriving-ppo-loss.html</link>
  <description><![CDATA[ 




<p>I have been trying to wrap my head around reinforcement learning methods like <a href="https://arxiv.org/abs/2305.18290">DPO</a>, <a href="https://arxiv.org/pdf/2402.03300">GRPO</a>, and <a href="https://arxiv.org/abs/2506.14245">RLVR</a> for a while now, especially with all the recent work showing how effective they can be for LLM post-training. Since I amm still pretty new to RL, I figured the best place to start was Proximal Policy Optimization (PPO), the algorithm OpenAI used to show how reinforcement learning could meaningfully improve LLM alignment (<a href="https://arxiv.org/pdf/2203.02155">InstructGPT</a> paper). My hope is that getting comfortable with PPO will give me the right mental model for the policy-gradient side of things and make it easier to understand the newer LLM-specific RL methods built on similar ideas.</p>
<p>If you start learning RL, you quickly realize it involves a lot of math! So I decided to lean into that and do a few (possibly annoying) derivation sessions to really understand the PPO objective by building it up from first principles, similar to how Umar Jamil does in his video.</p>
<blockquote class="blockquote">
<p>A huge shoutout to Umar Jamil’s <a href="https://www.youtube.com/watch?v=qGyFrqc34yc">video on RLHF and PPO</a>: it was incredibly helpful for building intuition and understanding the math behind the PPO loss.</p>
</blockquote>
<p>Below is my attempt at the derivation based on the original PPO and InstructGPT papers and Umar Jamil’s video.</p>
<section id="i-reinforcement-learning-core-definitions" class="level2">
<h2 class="anchored" data-anchor-id="i-reinforcement-learning-core-definitions">I: Reinforcement Learning: Core Definitions</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 39%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>General RL Definition</th>
<th>LLM Context (RLHF)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Reinforcement Learning</strong></td>
<td>A learning setup where an agent learns to act in an environment to maximize expected cumulative reward.</td>
<td>Fine-tuning a language model to generate responses that better match human preferences using reward-based feedback.</td>
</tr>
<tr class="even">
<td><strong>Environment</strong></td>
<td>Everything outside the agent that it interacts with and that produces observations and rewards.</td>
<td>The prompt distribution and interaction loop and the reward signal from a reward model evaluating generated responses.</td>
</tr>
<tr class="odd">
<td><strong>Agent</strong></td>
<td>The learner/decision-maker that observes states, takes actions, and receives rewards.</td>
<td>The language model generating text token by token.</td>
</tr>
<tr class="even">
<td><strong>Action (<img src="https://latex.codecogs.com/png.latex?a">)</strong></td>
<td>A choice made by the agent, usually conditioned on the state <img src="https://latex.codecogs.com/png.latex?s">.</td>
<td>Picking the next token at each step of generation.</td>
</tr>
<tr class="odd">
<td><strong>State (<img src="https://latex.codecogs.com/png.latex?s">)</strong></td>
<td>The information available to the agent at a given time step.</td>
<td>The prompt plus the response generated so far (the current token context).</td>
</tr>
<tr class="even">
<td><strong>Reward (<img src="https://latex.codecogs.com/png.latex?r">)</strong></td>
<td>A scalar signal telling the agent how good or bad an outcome was.</td>
<td>A score from the reward model (trained on preference data) that judges how good or bad a response is.</td>
</tr>
<tr class="odd">
<td><strong>Policy (<img src="https://latex.codecogs.com/png.latex?%5Cpi">)</strong></td>
<td>A stochastic mapping from states to a distribution over actions.</td>
<td>The model’s probability distribution over the next token given the context.</td>
</tr>
<tr class="even">
<td><strong>Goal</strong></td>
<td>Find an optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*"> that maximizes expected cumulative reward over time.</td>
<td>Update (align) the model so it tends to generate responses with higher reward-model scores.</td>
</tr>
</tbody>
</table>
</section>
<section id="ii-reward-model-in-rlhf-for-llms" class="level2">
<h2 class="anchored" data-anchor-id="ii-reward-model-in-rlhf-for-llms">II: Reward Model in RLHF for LLMs</h2>
<p>A <strong>Reward Model (RM)</strong> is a neural network that takes a prompt <img src="https://latex.codecogs.com/png.latex?x"> and a response <img src="https://latex.codecogs.com/png.latex?y"> as input and outputs a <strong>scalar reward</strong> <img src="https://latex.codecogs.com/png.latex?r_%5Cphi(x,%20y)%20%5Cin%20%5Cmathbb%7BR%7D"> indicating how “good” or “aligned” that response is according to human preferences.</p>
<p>Policy-gradient methods (including PPO) require a scalar objective to update the policy parameters. In standard RL, the environment provides this signal. However for language generation, there is no natural environment giving us rewards for “good” responses. Having humans rate every output is impractical and for gradient-based optimization, we need a differentiable scalar signal to backpropagate through. Thus, we require a cheap, differentiable proxy for human preferences during RL training. A learned RM provides exactly this.</p>
<section id="how-is-the-reward-model-trained" class="level3">
<h3 class="anchored" data-anchor-id="how-is-the-reward-model-trained">How is the Reward Model Trained?</h3>
<p>The standard procedure for training the reward model is:</p>
<ol type="1">
<li>Sample prompts (<img src="https://latex.codecogs.com/png.latex?x">)</li>
<li>Generate multiple candidate completions (<img src="https://latex.codecogs.com/png.latex?y_1,%20y_2,%20%5Cldots,%20y_K">) from a baseline policy (often an SFT model).</li>
<li>Ask humans to <strong>compare</strong> candidates (pairwise preferences are easier than absolute scoring).</li>
<li>Train the RM (<img src="https://latex.codecogs.com/png.latex?r_%5Cphi">) to predict those preferences.</li>
</ol>
<p>Architecturally, the reward model is typically: - Initialized from a pretrained language model (often the SFT model itself) - The final non-embedding layer (which projects to vocabulary) is <strong>removed</strong> - Replaced it with a <strong>linear layer</strong> that projects the hidden state of the last token to a single scalar output</p>
</section>
<section id="reward-model-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="reward-model-loss-function">Reward Model Loss Function</h3>
<p>The reward model is trained using the <strong>Bradley-Terry model</strong> for pairwise comparisons. The probability that response <img src="https://latex.codecogs.com/png.latex?y_w"> (preferred) is preferred over <img src="https://latex.codecogs.com/png.latex?y_l"> (less preferred) for any prompt <img src="https://latex.codecogs.com/png.latex?x"> is modeled as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y_w%20%5Csucc%20y_l%20%7C%20x)%20=%20%5Csigma%5Cleft(r_%5Ctheta(x,%20y_w)%20-%20r_%5Ctheta(x,%20y_l)%5Cright)%20%5Ctag%7BII.I%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is the sigmoid function: <img src="https://latex.codecogs.com/png.latex?%5Csigma(z)%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-z%7D%7D"></p>
<p>The <strong>negative log-likelihood loss</strong> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7B%5Ctext%7BRM%7D%7D(%5Cphi)%20=%20-%5Cmathbb%7BE%7D_%7B(x,%20y_w,%20y_l)%20%5Csim%20%5Cmathcal%7BD%7D%7D%20%5Cleft%5B%20%5Clog%20%5Csigma%5Cleft(r_%5Cphi(x,%20y_w)%20-%20r_%5Cphi(x,%20y_l)%5Cright)%20%5Cright%5D%0A"></p>
<p>One can verify that this loss forces the reward model to assign higher rewards to preferred responses (see <a href="https://arxiv.org/pdf/2203.02155">InstructGPT paper</a> or Umar Jamil’s video for a detailed walkthrough).</p>
<p>There are two key insights here: 1. We don’t need absolute scores, we only need the reward model to <strong>correctly rank</strong> responses. 2. The loss depends only on <strong>differences</strong> (<img src="https://latex.codecogs.com/png.latex?r_%5Cphi(x,%20y_w)%20-%20r_%5Cphi(x,%20y_l)">), so it is invariant to adding a constant to all rewards. This will be useful later when we discuss the PPO loss.</p>
<p>The reward model serves as a <strong>learned proxy for human preferences</strong>, converting the intractable problem of getting human feedback on every generation into a tractable supervised learning problem. Once trained, it provides the scalar signal <img src="https://latex.codecogs.com/png.latex?r_%5Cphi(x,%20y)"> needed to optimize our policy (LLM) using rl algorithms like PPO.</p>
</section>
</section>
<section id="iii-trajectories-and-returns" class="level2">
<h2 class="anchored" data-anchor-id="iii-trajectories-and-returns">III: Trajectories and Returns</h2>
<section id="trajectory" class="level3">
<h3 class="anchored" data-anchor-id="trajectory">Trajectory</h3>
<p>A <strong>trajectory</strong> (also called a rollout or episode) is a sequence of states (<img src="https://latex.codecogs.com/png.latex?s">), actions (<img src="https://latex.codecogs.com/png.latex?a">), and rewards (<img src="https://latex.codecogs.com/png.latex?r">) generated by an agent interacting with an environment:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctau%20=%20(s_0,%20a_0,%20r_0,%20s_1,%20a_1,%20r_1,%20%5Cldots,%20s_T,%20a_T,%20r_T)%0A"></p>
<p>In the context of LLMs, a trajectory corresponds to the entire sequence of token generations. It is the prompt followed by all generated tokens until the end-of-sequence token.</p>
<p>Note that the states are always stochastically modeled, and <img src="https://latex.codecogs.com/png.latex?s_%7Bt+1%7D"> can be represented as <img src="https://latex.codecogs.com/png.latex?s_%7Bt+1%7D%20%5Csim%20P(s_%7Bt+1%7D%20%7C%20s_t,%20a_t)">. Given a stochastic policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20(a_t%20%7C%20s_t)">, the probability of a trajectory <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is the product of: 1. The initial state distribution <img src="https://latex.codecogs.com/png.latex?%5Crho_0(s_0)"> 2. The stochastic policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20(a_t%20%7C%20s_t)"> 3. The environment transition dynamics <img src="https://latex.codecogs.com/png.latex?P(s_%7Bt+1%7D%20%7C%20s_t,%20a_t)"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(%5Ctau%20%7C%20%5Cpi_%5Ctheta)%20=%20%5Crho_0(s_0)%20%5Cprod_%7Bt=0%7D%5E%7BT-1%7D%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Ccdot%20P(s_%7Bt+1%7D%20%7C%20s_t,%20a_t)%20%5Ctag%7BIII.I%7D%0A"></p>
</section>
<section id="return" class="level3">
<h3 class="anchored" data-anchor-id="return">Return</h3>
<p>The <strong>return</strong> is the cumulative reward collected over the full trajectory (<img src="https://latex.codecogs.com/png.latex?%5Ctau">). The simplest form is the <strong>undiscounted return</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR(%5Ctau)%20=%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20r_t%0A"></p>
<p>More generally, we use the <strong>discounted return</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR(%5Ctau)%20=%20%5Csum_%7Bk=0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20r_%7Bk%7D%20=%20r_0%20+%20%5Cgamma%20r_%7B1%7D%20+%20%5Cgamma%5E2%20r_%7B2%7D%20+%20%5Ccdots%20%5Ctag%7BIII.II%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cin%20%5B0,%201%5D"> is the <strong>discount factor</strong>. The discount factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> serves a couple of purposes: 1. It ensures the return is finite for infinite-horizon tasks (<img src="https://latex.codecogs.com/png.latex?T%5Cto%5Cinfty">). 2. It prioritizes immediate rewards over distant ones.</p>
</section>
</section>
<section id="iv-policy-gradient-optimization-and-reinforce-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="iv-policy-gradient-optimization-and-reinforce-algorithm">IV: Policy Gradient Optimization and REINFORCE Algorithm</h2>
<p>The goal of reinforcement learning is to find a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> that maximizes the <strong>expected return</strong> over all possible trajectories:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7BJ(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi_%5Ctheta%7D%5BR(%5Ctau)%5D%7D%20%5Ctag%7BIV.I%7D%0A"></p>
<p>This is our objective function and we want to find parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E*"> such that:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%5E*%20=%20%5Carg%5Cmax_%5Ctheta%20J(%5Ctheta)%0A"></p>
<p>To maximize <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)"> using gradient-based methods, we need to compute <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20J(%5Ctheta)"> and perform gradient ascent:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Ctheta_%7Bk+1%7D%20=%20%5Ctheta_k%20+%20%5Calpha%20%5Cleft.%20%5Cnabla_%5Ctheta%20J(%5Cpi_%5Ctheta)%20%5Cright%7C_%7B%5Ctheta_k%7D%7D%20%5Ctag%7BIV.II%7D%0A"></p>
<p>This policy gradient looks simple in equation form but it is intractable to compute. The expectation is over trajectories sampled from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">, which itself depends on <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. We can’t simply enumerate all possible trajectories. This is computationally intractable for any reasonably sized state-action space (and certainly not possible for LLMs!).</p>
<p>Thus, as a next step we need to derive some sort of reasonable and tractable approximation for <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20J(%5Ctheta)">. We do this by using the <strong>log-derivative trick</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20=%20%5Cnabla_%5Ctheta%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi_%5Ctheta%7D%5BR(%5Ctau)%5D%0A"></p>
<p>This expectation can be written as an integral: <img src="https://latex.codecogs.com/png.latex?%0A=%20%5Cnabla_%5Ctheta%20%5Cint_%5Ctau%20P(%5Ctau%20%7C%20%5Ctheta)%20R(%5Ctau)%20%5C,%20d%5Ctau%0A"></p>
<p>Bringing the gradient inside the integral: <img src="https://latex.codecogs.com/png.latex?%0A=%20%5Cint_%5Ctau%20%5Cnabla_%5Ctheta%20P(%5Ctau%20%7C%20%5Ctheta)%20R(%5Ctau)%20%5C,%20d%5Ctau%0A"></p>
<p>Now we apply the <strong>log-derivative trick</strong>: <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20%5Clog%20P(%5Ctau%20%7C%20%5Ctheta)%20=%20%5Cfrac%7B%5Cnabla_%5Ctheta%20P(%5Ctau%20%7C%20%5Ctheta)%7D%7BP(%5Ctau%20%7C%20%5Ctheta)%7D%0A"></p>
<p>Rearranging: <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20P(%5Ctau%20%7C%20%5Ctheta)%20=%20P(%5Ctau%20%7C%20%5Ctheta)%20%5Cnabla_%5Ctheta%20%5Clog%20P(%5Ctau%20%7C%20%5Ctheta)"> and substituting back, we get: <img src="https://latex.codecogs.com/png.latex?%0A=%20%5Cint_%5Ctau%20P(%5Ctau%20%7C%20%5Ctheta)%20%5Cnabla_%5Ctheta%20%5Clog%20P(%5Ctau%20%7C%20%5Ctheta)%20R(%5Ctau)%20%5C,%20d%5Ctau%0A"></p>
<p>which can also be written as the following expectation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Cnabla_%5Ctheta%20J(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi_%5Ctheta%7D%5Cleft%5B%20%5Cnabla_%5Ctheta%20%5Clog%20P(%5Ctau%20%7C%20%5Ctheta)%20%5Ccdot%20R(%5Ctau)%20%5Cright%5D%7D%20%5Ctag%7BIV.III%7D%0A"></p>
<p>Note, here the gradient is now the expectation of the gradient of the log-probability of the trajectory. This can further be simplified by using the trajectory probability expression (III.I): <img src="https://latex.codecogs.com/png.latex?%0AP(%5Ctau%20%7C%20%5Ctheta)%20=%20%5Crho_0(s_0)%20%5Cprod_%7Bt=0%7D%5E%7BT-1%7D%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Ccdot%20P(s_%7Bt+1%7D%20%7C%20s_t,%20a_t)%0A"></p>
<p>Taking the log:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20P(%5Ctau%20%7C%20%5Ctheta)%20=%20%5Clog%20%5Crho_0(s_0)%20+%20%5Csum_%7Bt=0%7D%5E%7BT-1%7D%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20+%20%5Csum_%7Bt=0%7D%5E%7BT-1%7D%20%5Clog%20P(s_%7Bt+1%7D%20%7C%20s_t,%20a_t)%0A"></p>
<p>When we take <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta">, only the policy term depends on <img src="https://latex.codecogs.com/png.latex?%5Ctheta">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20%5Clog%20P(%5Ctau%20%7C%20%5Ctheta)%20=%20%5Csum_%7Bt=0%7D%5E%7BT-1%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%0A"></p>
<p>The initial state distribution and transition dynamics are independent of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, so their gradients vanish. Substituting back, we obtain the <strong>policy gradient theorem</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Cnabla_%5Ctheta%20J(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi_%5Ctheta%7D%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Ccdot%20R(%5Ctau)%20%5Cright%5D%7D%20%5Ctag%7BIV.IV%7D%0A"></p>
<p>This is a remarkable result. We can compute the gradient of our objective without differentiating through the environment dynamics and <strong>only need gradients of the log-probabilities of our policy</strong>.</p>
<p>Since we cannot compute the expectation exactly, we approximate it with a sample mean by sampling <img src="https://latex.codecogs.com/png.latex?N"> trajectories:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Capprox%20%5Chat%7Bg%7D%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_%7Bi,t%7D%20%7C%20s_%7Bi,t%7D)%20%5Cright)%20R(%5Ctau_i)%7D%20%5Ctag%7BIV.V%7D%0A"></p>
<p>This gives us the <strong>REINFORCE algorithm</strong>:</p>
<ol type="1">
<li><p><strong>Initialize</strong>: Start with a pretrained or supervised fine-tuned (SFT) language model <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"></p></li>
<li><p><strong>Sample prompts</strong>: Draw a batch of <img src="https://latex.codecogs.com/png.latex?N"> prompts <img src="https://latex.codecogs.com/png.latex?%5C%7Bx_1,%20x_2,%20%5Cldots,%20x_N%5C%7D"> from a dataset</p></li>
<li><p><strong>Generate trajectories</strong>: For each prompt <img src="https://latex.codecogs.com/png.latex?x_i">, generate a response <img src="https://latex.codecogs.com/png.latex?y_i%20=%20(a_0,%20a_1,%20%5Cldots,%20a_T)"> by sampling tokens from the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">. Each trajectory is the sequence of states (prompt + generated tokens so far) and actions (selected tokens).</p></li>
<li><p><strong>Compute log-probabilities</strong>: For each trajectory, compute the log-probability of each generated token given its context: <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Cquad%20%5Ctext%7Bfor%20%7D%20t%20=%200,%201,%20%5Cldots,%20T"></p></li>
<li><p><strong>Compute rewards</strong>: Score each complete (prompt, response) pair using the reward model: <img src="https://latex.codecogs.com/png.latex?R(%5Ctau_i)%20=%20r_%5Cphi(x_i,%20y_i)"></p></li>
<li><p><strong>Estimate policy gradient</strong>: Compute the gradient estimate using (IV.V): <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bg%7D%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_%7Bi,t%7D%20%7C%20s_%7Bi,t%7D)%20%5Cright)%20R(%5Ctau_i)"></p></li>
<li><p><strong>Update policy</strong>: Perform a gradient ascent step: <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cleftarrow%20%5Ctheta%20+%20%5Calpha%20%5Chat%7Bg%7D"></p></li>
<li><p><strong>Repeat</strong>: Go back to Step 2 and iterate until convergence</p></li>
</ol>
<p>While REINFORCE provides an unbiased gradient estimate, it suffers from two critical issues that make it impractical for LLM training:</p>
<ol type="1">
<li><p><strong>High Variance</strong>: The gradient estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bg%7D"> suffers from high variance depending on the sampled trajectories. This variance can be large and can lead to noisy gradients and unstable training. &gt; If you look again at (IV.V), the gradient estimate for each action is weighted by the return of the <em>entire</em> trajectory <img src="https://latex.codecogs.com/png.latex?R(%5Ctau)">. This means that even if an action was good, it might receive a negative gradient update simply because other actions in the trajectory led to poor outcomes (or vice versa). Over many samples, the noise introduced by this coupling can be substantial, leading to high variance</p></li>
<li><p><strong>On-Policy Constraint (Sample Inefficiency)</strong>: REINFORCE requires trajectories sampled from the <em>current</em> policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">. Thus after every gradient update, previously collected trajectories must be discarded and new ones need to be sampled from the updated policy. For LLMs, where each trajectory requires a full forward pass through a billion(s)-parameter model, this is prohibitively expensive especially when we need many small gradient steps to train effectively.</p></li>
</ol>
</section>
<section id="v-reducing-variance-and-the-advantage-function" class="level2">
<h2 class="anchored" data-anchor-id="v-reducing-variance-and-the-advantage-function">V: Reducing Variance and the Advantage Function</h2>
<p>The REINFORCE algorithm provides an unbiased gradient estimate (IV.V). However while unbiased, this estimator suffers from <strong>high variance</strong>.</p>
<section id="replacing-full-trajectory-return-with-reward-to-go-using-causality" class="level3">
<h3 class="anchored" data-anchor-id="replacing-full-trajectory-return-with-reward-to-go-using-causality">Replacing Full-Trajectory Return with Reward-to-Go (using causality)</h3>
<p>A first variance reduction comes from noticing that action <img src="https://latex.codecogs.com/png.latex?a_t"> taken at time <img src="https://latex.codecogs.com/png.latex?t"> <strong>cannot influence rewards that were received before time <img src="https://latex.codecogs.com/png.latex?t"></strong>. This is a fundamental consequence of causality. These past reward terms contribute only noise to the gradient estimate and add variance without contributing any signal. Thus, we can remove them and consider only the <strong>rewards-to-go</strong> :</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BR%7D_t%20=%20%5Csum_%7Bt'=t%7D%5E%7BT%7D%20r_%7Bt'%7D%0A"></p>
<p>This gives us a lower-variance estimator:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_%7Bi,t%7D%20%7C%20s_%7Bi,t%7D)%20%5Ccdot%20%5Chat%7BR%7D_%7Bi,t%7D%7D%20%5Ctag%7BV.I%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D_%7Bi,t%7D%20=%20%5Csum_%7Bt'=t%7D%5E%7BT%7D%20r_%7Bi,t'%7D"> is the rewards-to-go for trajectory <img src="https://latex.codecogs.com/png.latex?i"> starting from time <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="subtracting-a-baseline" class="level3">
<h3 class="anchored" data-anchor-id="subtracting-a-baseline">Subtracting a Baseline</h3>
<p>A second complementary technique for variance reduction is to subtract a <strong>baseline</strong> <img src="https://latex.codecogs.com/png.latex?b(s_t)"> from the rewards. The key insight is that we can subtract <strong>any function that does not depend on the action</strong> from our reward signal without changing the expected value of the gradient.</p>
<p>Thus we can subtract a state-dependent baseline <img src="https://latex.codecogs.com/png.latex?b(s_t)"> from our rewards-to-go to yield an <strong>unbiased</strong> gradient estimator:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_%7Bi,t%7D%20%7C%20s_%7Bi,t%7D)%20%5Ccdot%20%5Cleft(%5Chat%7BR%7D_%7Bi,t%7D%20-%20b(s_%7Bi,t%7D)%5Cright)%7D%20%5Ctag%7BV.II%7D%0A"></p>
</section>
<section id="value-functions-vpis-and-qpis-a" class="level3">
<h3 class="anchored" data-anchor-id="value-functions-vpis-and-qpis-a">Value Functions: <img src="https://latex.codecogs.com/png.latex?V%5E%5Cpi(s)"> and <img src="https://latex.codecogs.com/png.latex?Q%5E%5Cpi(s,%20a)"></h3>
<p>The baseline is still an arbitrary function. To make it more systematic and concrete, there are two fundamental functions from RL theory.</p>
<p><strong>State Value Function:</strong> The <strong>state value function</strong> <img src="https://latex.codecogs.com/png.latex?V%5E%5Cpi(s)"> is the expected return when the agent is in state <img src="https://latex.codecogs.com/png.latex?s"> and acts according to policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV%5E%5Cpi(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi%7D%5Cleft%5B%5Csum_%7Bt=0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Et%20r_t%20%5C;%5Cmiddle%7C%5C;%20s_0%20=%20s%5Cright%5D%20%20"></p>
<p>Intuitively, <img src="https://latex.codecogs.com/png.latex?V%5E%5Cpi(s)"> tells <strong>“How good is this state on average?”</strong> and is used as a baseline <img src="https://latex.codecogs.com/png.latex?b(s)%20=%20V%5E%5Cpi(s)">.</p>
<p><strong>Action Value Function (Q-function):</strong> The <strong>action value function</strong> <img src="https://latex.codecogs.com/png.latex?Q%5E%5Cpi(s,%20a)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s"> and taking action <img src="https://latex.codecogs.com/png.latex?a"> and then acting according to policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AQ%5E%5Cpi(s,%20a)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi%7D%5Cleft%5B%5Csum_%7Bt=0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Et%20r_t%20%5C;%5Cmiddle%7C%5C;%20s_0%20=%20s,%20a_0%20=%20a%5Cright%5D%0A"></p>
<p>Intuitively, <img src="https://latex.codecogs.com/png.latex?Q%5E%5Cpi(s,%20a)"> tells <strong>“How good is this specific action in this state?”</strong> and in RL, the rewards-to-go is estimated as <img src="https://latex.codecogs.com/png.latex?Q%5E%5Cpi(s,%20a)">.</p>
<p>In the LLM context: - <img src="https://latex.codecogs.com/png.latex?V%5E%5Cpi(s)"> estimates the expected reward for a given prompt + partial response, assuming the model continues generating according to its current policy. - <img src="https://latex.codecogs.com/png.latex?Q%5E%5Cpi(s,%20a)"> estimates the expected reward if, from the current prompt + partial response, the model generates a specific next token <img src="https://latex.codecogs.com/png.latex?a"> and then continues according to its policy.</p>
</section>
<section id="advantage-function" class="level3">
<h3 class="anchored" data-anchor-id="advantage-function">Advantage Function</h3>
<p>The <strong>advantage function</strong> <img src="https://latex.codecogs.com/png.latex?A%5E%5Cpi(s,%20a)"> measures how much better (or worse) a specific action <img src="https://latex.codecogs.com/png.latex?a"> is compared to the average action under the policy:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7BA%5E%5Cpi(s,%20a)%20=%20Q%5E%5Cpi(s,%20a)%20-%20V%5E%5Cpi(s)%7D%20%5Ctag%7BV.III%7D%0A"></p>
<p>The advantage function directly tells us: <strong>“How much better is this particular action compared to what we would typically do in this state?”</strong> This is precisely the signal we want for policy improvement. We want to increase the probability of actions with positive advantage and decrease the probability of actions with negative advantage.</p>
<blockquote class="blockquote">
<p>From Umar Jamil’s video:<br>
In the LLM context consider a state where the prompt is “Where is Shanghai?” and the model has generated “Shanghai is”. From this state: - If the model samples the token “in” (leading toward “Shanghai is in China”), this action likely has <strong>positive advantage</strong>. This is because it is better than the average token the model might produce. - If the model samples the token “delicious” (leading toward an incoherent response), this action likely has <strong>negative advantage</strong>. This is because it is worse than the average token the model might produce.</p>
</blockquote>
</section>
<section id="advantage-weighted-policy-gradient" class="level3">
<h3 class="anchored" data-anchor-id="advantage-weighted-policy-gradient">Advantage-Weighted Policy Gradient</h3>
<p>Substituting the rewards-to-go and the value function as a baseline, we get the following form of the policy gradient: <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Ccdot%20(Q%5E%5Cpi(s_t,%20a_t)%20-%20V%5E%5Cpi(s_t))%5Cright%5D%0A"></p>
<p>which can be written as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Cnabla_%5Ctheta%20J(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Ccdot%20A%5E%7B%5Cpi_%5Ctheta%7D(s_t,%20a_t)%5Cright%5D%7D%20%5Ctag%7BV.IV%7D%0A"></p>
<p>and for sample-based approximation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_%7Bi,t%7D%20%7C%20s_%7Bi,t%7D)%20%5Ccdot%20%5Chat%7BA%7D_%7Bi,t%7D%7D%20%5Ctag%7BV.V%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_%7Bi,t%7D"> is an estimate of the advantage function at time <img src="https://latex.codecogs.com/png.latex?t"> in trajectory <img src="https://latex.codecogs.com/png.latex?i">. This is the form of the policy gradient often used.</p>
<p>In practice, <img src="https://latex.codecogs.com/png.latex?A%5E%5Cpi(s_t,%20a_t)"> can be estimated as follows:</p>
<ol type="1">
<li><p><strong>Learn a value function:</strong> Train a neural network <img src="https://latex.codecogs.com/png.latex?V_%5Cphi(s)"> (often called the “critic” or “value head”) to approximate <img src="https://latex.codecogs.com/png.latex?V%5E%5Cpi(s)">. In LLM fine-tuning, this is often a linear layer on top of the same transformer backbone used for the policy.</p></li>
<li><p><strong>Estimate <img src="https://latex.codecogs.com/png.latex?Q%5E%5Cpi"> from samples:</strong> Given a trajectory, the rewards-to-go <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D_t%20=%20%5Csum_%7Bt'=t%7D%5E%7BT%7D%20%5Cgamma%5E%7Bt'%7D%20r_%7Bt'%7D"> provides an unbiased (but high-variance) estimate of <img src="https://latex.codecogs.com/png.latex?Q%5E%5Cpi(s_t,%20a_t)">.</p></li>
<li><p><strong>Compute advantage estimates:</strong> <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t%20=%20%5Chat%7BR%7D_t%20-%20V_%5Cphi(s_t)"></p></li>
</ol>
<p>More sophisticated methods like <strong>Generalized Advantage Estimation (GAE)</strong> interpolate between high-variance, low-bias estimates and low-variance, high-bias estimates by using a weighted combination of multi-step returns. See the <a href="https://arxiv.org/abs/1506.02438">GAE paper</a> for more details.</p>
</section>
</section>
<section id="vi-importance-sampling-and-off-policy-policy-gradients" class="level2">
<h2 class="anchored" data-anchor-id="vi-importance-sampling-and-off-policy-policy-gradients">VI: Importance Sampling and Off-Policy Policy Gradients</h2>
<blockquote class="blockquote">
<p><strong>Note:</strong> In RL literature, “off-policy” typically refers to methods where the <em>behavior policy</em> (generating data) is arbitrarily quite different from the <em>target policy</em> (being optimized) say where transitions from policies thousands of updates old are reused. In this section, what we will call “off-policy” should more precisely be called “local off-policy”.</p>
</blockquote>
<p>The advantage-weighted policy gradient (V.IV) requires trajectories sampled from the current policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">. … The advantage-weighted policy gradient (V.IV) requires trajectories sampled from the current policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">. This creates a fundamental <strong>inefficiency</strong> i.e., after each gradient update <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cto%20%5Ctheta'"> all previously collected trajectories become “stale” and we must discard these trajectories and sample new ones from the updated policy.</p>
<p>For LLMs, where each trajectory requires a full forward pass through billion(s)-parameter model, this is prohibitively expensive especially when we need many small gradient steps to train effectively.</p>
<p>We need a way to reuse the same trajectories for multiple gradient updates. <strong>Importance sampling</strong> provides the mathematical machinery to do exactly this!</p>
<section id="importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="importance-sampling">Importance Sampling</h3>
<p>Importance sampling is a technique for estimating expectations under one probability distribution using samples drawn from a different distribution. Consider an expectation for distribution <img src="https://latex.codecogs.com/png.latex?p(x)">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20p%7D%5Bf(x)%5D%20=%20%5Cint%20p(x)%20f(x)%20%5C,%20dx%0A"></p>
<p>We can rewrite this by multiplying and dividing by another distribution <img src="https://latex.codecogs.com/png.latex?q(x)"> (with <img src="https://latex.codecogs.com/png.latex?q(x)%20%3E%200"> wherever <img src="https://latex.codecogs.com/png.latex?p(x)%20%3E%200">):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A=%20%5Cint%20q(x)%20%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%20f(x)%20%5C,%20dx%20=%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20q%7D%5Cleft%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%20f(x)%5Cright%5D%0A"></p>
<p>The ratio <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp(x)%7D%7Bq(x)%7D"> is called the <strong>importance weight</strong>. This identity tells us:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20p%7D%5Bf(x)%5D%20=%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20q%7D%5Cleft%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%20f(x)%5Cright%5D%7D%20%5Ctag%7BVI.I%7D%0A"></p>
<p>We can now estimate the expectation under <img src="https://latex.codecogs.com/png.latex?p"> using samples from <img src="https://latex.codecogs.com/png.latex?q"> as long as we reweight each sample by the ratio of probabilities.</p>
</section>
<section id="applying-importance-sampling-to-policy-gradients" class="level3">
<h3 class="anchored" data-anchor-id="applying-importance-sampling-to-policy-gradients">Applying Importance Sampling to Policy Gradients</h3>
<p>We can apply this technique to the policy gradient setting. The on-policy advantage-weighted gradient (V.IV) is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%20%5Csim%20%5Cpi_%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Ccdot%20A%5E%7B%5Cpi_%5Ctheta%7D(s_t,%20a_t)%5Cright%5D%0A"></p>
<p>To apply importance sampling, we work at time-step level rather than trajectory level (full trajectory importance weights have extremely high variance). For a single timestep: <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B(s_t,%20a_t)%20%5Csim%20%5Cpi_%5Ctheta%7D%5Cleft%5B%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Ccdot%20A%5E%7B%5Cpi_%5Ctheta%7D(s_t,%20a_t)%5Cright%5D%0A"></p>
<p>Using importance sampling with samples from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A=%20%5Cmathbb%7BE%7D_%7B(s_t,%20a_t)%20%5Csim%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_t%20%7C%20s_t)%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%20%5Ccdot%20A%5E%7B%5Cpi_%5Ctheta%7D(s_t,%20a_t)%5Cright%5D%0A"></p>
<p>Now we apply the log-derivative identity <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%20=%20%5Cfrac%7B%5Cnabla_%5Ctheta%20%5Cpi_%5Ctheta%7D%7B%5Cpi_%5Ctheta%7D">, which gives us a <strong>surrogate objective <img src="https://latex.codecogs.com/png.latex?L(%5Ctheta)"></strong> whose gradient equals this importance-weighted policy gradient:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B(s_t,%20a_t)%20%5Csim%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%5Cleft%5B%5Cfrac%7B%5Cnabla_%5Ctheta%20%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_t%20%7C%20s_t)%7D%20A%5E%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D(s_t,%20a_t)%5Cright%5D%0A"></p>
<p>where the importance-weighted surrogate objective also known as the <strong>Conservative Policy Iteration (CPI)</strong> objective is: <img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B(s_t,%20a_t)%20%5Csim%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_t%20%7C%20s_t)%7D%20A%5E%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D%7D(s_t,%20a_t)%5Cright%5D%0A"></p>
<p>We also define the <strong>probability ratio</strong> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar_t(%5Ctheta)%20=%20%5Cfrac%7B%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_t%20%7C%20s_t)%7D%20%5Ctag%7BVI.II%7D%0A"></p>
<p>Note that <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta_%7B%5Ctext%7Bold%7D%7D)%20=%201"> by construction. Thus, the CPI objective can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7BL%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_t%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_t%20%7C%20s_t)%7D%20%5Chat%7BA%7D_t%5Cright%5D%20=%20%5Cmathbb%7BE%7D_t%5Cleft%5Br_t(%5Ctheta)%20%5Chat%7BA%7D_t%5Cright%5D%7D%20%5Ctag%7BVI.III%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t"> is the estimated advantage at timestep <img src="https://latex.codecogs.com/png.latex?t">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_t%5B%5Ccdot%5D"> denotes the empirical average over a batch of samples collected under <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">.</p>
<p>This objective has a clear interpretation: - If <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t%20%3E%200"> (action better than average), we want to <strong>increase</strong> <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)">, i.e., make the new policy more likely to take this action. - If <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t%20%3C%200"> (action worse than average), we want to <strong>decrease</strong> <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)">, i.e., make the new policy less likely to take this action.</p>
<p>The corresponding sample-based approximation is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7BL%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)%20%5Capprox%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cfrac%7B%5Cpi_%5Ctheta(a_%7Bi,t%7D%20%7C%20s_%7Bi,t%7D)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_%7Bi,t%7D%20%7C%20s_%7Bi,t%7D)%7D%20%5Chat%7BA%7D_%7Bi,t%7D%7D%20%5Ctag%7BVI.IV%7D%0A"></p>
</section>
<section id="off-policy-learning-reusing-trajectories" class="level3">
<h3 class="anchored" data-anchor-id="off-policy-learning-reusing-trajectories">Off-Policy Learning: Reusing Trajectories</h3>
<p>The CPI objective enables <strong>off-policy learning</strong>: we can sample trajectories from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">, store them and then perform multiple gradient updates on <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> using the same batch of data. The typical workflow becomes:</p>
<ol type="1">
<li><strong>Collect</strong>: Sample trajectories <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Ctau_i%5C%7D"> from the current policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"></li>
<li><strong>Compute</strong>: Calculate advantages <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_%7Bi,t%7D"> and log-probabilities <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_%7Bi,t%7D%20%7C%20s_%7Bi,t%7D)"></li>
<li><strong>Store</strong>: Save the trajectories along with their advantages and old log-probabilities</li>
<li><strong>Optimize</strong>: Perform multiple gradient ascent steps on <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)"> using mini-batches from the stored data</li>
<li><strong>Repeat</strong>: Set <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Ctext%7Bold%7D%7D%20%5Cleftarrow%20%5Ctheta"> and return to step 1</li>
</ol>
<p>This dramatically improves sample efficiency. Instead of discarding trajectories after a single gradient step, we can extract multiple updates from each batch of expensive LLM rollouts.</p>
</section>
<section id="the-instability-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-instability-problem">The Instability Problem</h3>
<p>While the CPI objective improves sample efficiency, <strong>unconstrained optimization of <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)"> is unstable</strong>. The core issue is that importance sampling becomes unreliable when <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> drifts far from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">:</p>
<ul>
<li><strong>Extreme probability ratios</strong>: The ratio <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> can become arbitrarily large or small, destabilizing gradient estimates.</li>
<li><strong>Stale advantages</strong>: The estimates <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t"> were computed under <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> and become inaccurate as <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> diverges. The optimizer may exploit these stale estimates, making updates that appear beneficial but are actually harmful.</li>
</ul>
<p>In practice, unconstrained maximization of <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)"> often leads to excessively large policy updates that cause catastrophic performance collapse.</p>
<blockquote class="blockquote">
<p><strong>LLM Context (from Umar Jamil):</strong> Suppose we have a trajectory where the model generated “Shanghai is in China” with high advantage. Unconstrained optimization might dramatically upweight “China” as the next token given “Shanghai is in”—but this could simultaneously cause unintended probability shifts elsewhere, perhaps making the model overly likely to say “China” in completely unrelated contexts, or disrupting the probability mass across the entire vocabulary in unpredictable ways.</p>
</blockquote>
<p>We need a mechanism to constrain <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> from deviating too far from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D"> and keeping the ratio <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> close to 1 while still allowing meaningful policy improvement.</p>
</section>
</section>
<section id="vii-trust-region-policy-optimization-trpo" class="level2">
<h2 class="anchored" data-anchor-id="vii-trust-region-policy-optimization-trpo">VII: Trust Region Policy Optimization (TRPO)</h2>
<p>The CPI objective is attractive because it lets us reuse data via importance ratios, but <strong>unconstrained optimization is unstable</strong>. When <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> drifts far from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D">, the probability ratios <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> become extreme and the advantage estimates <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t"> become stale and can be exploited by the optimizer.</p>
<p>The key insight of Trust Region Policy Optimization (<a href="https://arxiv.org/abs/1502.05477">TRPO</a>) is that the surrogate objective <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)"> is only a valid approximation to the true objective within a local neighborhood of <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Ctext%7Bold%7D%7D">. TRPO paper formalized this by proving policy performance is guaranteed to improve as long as the KL divergence between consecutive policies remains bounded. This theoretical result motivates constraining the policy update to stay within a “trust region” where the surrogate objective remains reliable. See the <a href="https://arxiv.org/abs/1502.05477">TRPO paper</a> for the formal proof.</p>
<p>TRPO converts this insight into a <strong>constrained optimization problem</strong> that ensures the policy update stays within a “trust region” where the surrogate objective remains reliable.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%0A%5Cbegin%7Baligned%7D%0A%5Cmax_%5Ctheta%20%5Cquad%20&amp;%20L%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_t%5Cleft%5B%5Cfrac%7B%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_t%20%7C%20s_t)%7D%20%5Chat%7BA%7D_t%5Cright%5D%20%5C%5C%5B6pt%5D%0A%5Ctext%7Bsubject%20to%7D%20%5Cquad%20&amp;%20%5Cmathbb%7BE%7D_t%5Cleft%5BD_%7B%5Ctext%7BKL%7D%7D%5Cleft(%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Ccdot%7Cs_t)%20%5C%7C%20%5Cpi_%5Ctheta(%5Ccdot%7Cs_t)%5Cright)%5Cright%5D%20%5Cleq%20%5Cdelta%0A%5Cend%7Baligned%7D%0A%7D%20%5Ctag%7BVII.I%7D%0A"></p>
<p>The hyperparameter <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> defines the trust region size, the maximum allowed divergence between consecutive policies. This constraint ensures that <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> remains close to 1, keeping our importance-weighted estimates reliable.</p>
<p>Solving (VII.I) requires <strong>second-order optimization</strong>. TRPO approximates the objective linearly and the KL constraint quadratically (using the Fisher Information Matrix) and then solves the resulting problem via the <strong>conjugate gradient algorithm</strong> followed by a <strong>line search</strong> to ensure constraints are satisfied.</p>
<p>For large-scale LLM training, this approach is impractical:</p>
<ul>
<li><strong>Computational overhead</strong>: Each policy update requires multiple conjugate gradient iterations and line search steps, significantly more expensive than standard gradient descent.</li>
<li><strong>Memory requirements</strong>: Computing Fisher-vector products adds substantial memory overhead for billion(s)-parameter models</li>
</ul>
<p>The theory behind TRPO also suggests using a <strong>KL penalty</strong> rather than a hard constraint. It is easier to implement and more computationally efficient.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmax_%5Ctheta%20%5C;%20%5Cmathbb%7BE%7D_t%5Cleft%5Br_t(%5Ctheta)%20%5Chat%7BA%7D_t%20-%20%5Cbeta%20%5Ccdot%20D_%7B%5Ctext%7BKL%7D%7D%5Cleft(%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(%5Ccdot%7Cs_t)%20%5C%7C%20%5Cpi_%5Ctheta(%5Ccdot%7Cs_t)%5Cright)%5Cright%5D%20%5Ctag%7BVII.II%7D%0A"></p>
<p>However, choosing a penalty coefficient <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> that works across different problems or even across different training stages is notoriously difficult. This motivates Proximal Policy Optimization (PPO): a <strong>first-order method</strong> that achieves TRPO’s stability through a <strong>clipped surrogate objective</strong> rather than explicit constraints.</p>
</section>
<section id="viii-proximal-policy-optimization-ppo" class="level2">
<h2 class="anchored" data-anchor-id="viii-proximal-policy-optimization-ppo">VIII: Proximal Policy Optimization (PPO)</h2>
<p>Proximal Policy Optimization (PPO) achieves TRPO’s stability guarantees using only <strong>first-order optimization</strong>. Instead of explicitly constraining the KL divergence, PPO modifies the objective function itself to discourage large policy updates through a <strong>clipping mechanism</strong>. It implicitly limits how far the policy can move, providing a “soft” trust region using only standard gradient descent.</p>
<section id="clipped-surrogate-objective" class="level3">
<h3 class="anchored" data-anchor-id="clipped-surrogate-objective">Clipped Surrogate Objective</h3>
<p>CPI objective and probability ratio from Section VI:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BCPI%7D%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_t%5Cleft%5Br_t(%5Ctheta)%20%5Chat%7BA%7D_t%5Cright%5D%20%5Cquad%20%5Ctext%7Bwhere%7D%20%5Cquad%20r_t(%5Ctheta)%20=%20%5Cfrac%7B%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(a_t%20%7C%20s_t)%7D%0A"></p>
<p>The problem with <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCPI%7D%7D"> is that nothing prevents <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> from becoming arbitrarily large or small. PPO addresses this by <strong>clipping</strong> the probability ratio to stay within <img src="https://latex.codecogs.com/png.latex?%5B1-%5Cepsilon,%201+%5Cepsilon%5D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7BL%5E%7B%5Ctext%7BCLIP%7D%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_t%5Cleft%5B%5Cmin%5Cleft(r_t(%5Ctheta)%20%5Chat%7BA%7D_t,%20%5C;%20%5Ctext%7Bclip%7D(r_t(%5Ctheta),%201-%5Cepsilon,%201+%5Cepsilon)%20%5Ccdot%20%5Chat%7BA%7D_t%5Cright)%5Cright%5D%7D%20%5Ctag%7BVIII.I%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> is a hyperparameter (<img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20=%200.2"> from the <a href="https://arxiv.org/abs/1707.06347">PPO paper</a>) and the clip function is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bclip%7D(r,%201-%5Cepsilon,%201+%5Cepsilon)%20=%20%5Cbegin%7Bcases%7D%0A1-%5Cepsilon%20&amp;%20%5Ctext%7Bif%20%7D%20r%20%3C%201-%5Cepsilon%20%5C%5C%0Ar%20&amp;%20%5Ctext%7Bif%20%7D%201-%5Cepsilon%20%5Cleq%20r%20%5Cleq%201+%5Cepsilon%20%5C%5C%0A1+%5Cepsilon%20&amp;%20%5Ctext%7Bif%20%7D%20r%20%3E%201+%5Cepsilon%0A%5Cend%7Bcases%7D%0A"></p>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Cmin"> operator in (VIII.I) is important. It ensures we take the <strong>more pessimistic</strong> (lower) estimate between the clipped and unclipped objectives. This creates different behavior depending on the sign of the advantage:</p>
<p><strong>Case 1: Positive Advantage (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t%20%3E%200">)</strong></p>
<p>When an action is better than average, we want to <strong>increase</strong> its probability, which means increasing <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)">. The objective becomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BCLIP%7D%7D_t%20=%20%5Cmin%5Cleft(r_t(%5Ctheta),%201+%5Cepsilon%5Cright)%20%5Ccdot%20%5Chat%7BA%7D_t%0A"></p>
<ul>
<li>If <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)%20%5Cleq%201+%5Cepsilon">: The objective is <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)%20%5Chat%7BA%7D_t">, so gradient ascent increases <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"></li>
<li>If <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)%20%3E%201+%5Cepsilon">: The objective becomes <img src="https://latex.codecogs.com/png.latex?(1+%5Cepsilon)%5Chat%7BA%7D_t"></li>
</ul>
<p>The clipping <strong>removes the incentive</strong> to increase <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> beyond <img src="https://latex.codecogs.com/png.latex?1+%5Cepsilon">.</p>
<p><strong>Case 2: Negative Advantage (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t%20%3C%200">)</strong></p>
<p>When an action is worse than average, we want to <strong>decrease</strong> its probability, which means decreasing <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)">. Since <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t%20%3C%200">, multiplying by a smaller <img src="https://latex.codecogs.com/png.latex?r_t"> makes the product <em>less negative</em> (larger). The objective becomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BCLIP%7D%7D_t%20=%20%5Cmax%5Cleft(r_t(%5Ctheta),%201-%5Cepsilon%5Cright)%20%5Ccdot%20%5Chat%7BA%7D_t%0A"></p>
<p>(The <img src="https://latex.codecogs.com/png.latex?%5Cmin"> with negative values becomes a <img src="https://latex.codecogs.com/png.latex?%5Cmax"> in terms of which <img src="https://latex.codecogs.com/png.latex?r_t"> is selected.)</p>
<ul>
<li>If <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)%20%5Cgeq%201-%5Cepsilon">: The objective is <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)%20%5Chat%7BA%7D_t">, so gradient ascent decreases <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"></li>
<li>If <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)%20%3C%201-%5Cepsilon">: The objective becomes <img src="https://latex.codecogs.com/png.latex?(1-%5Cepsilon)%5Chat%7BA%7D_t"></li>
</ul>
<p>The clipping <strong>removes the incentive</strong> to decrease <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> beyond <img src="https://latex.codecogs.com/png.latex?1-%5Cepsilon">.</p>
<p>The takeaway here is that PPO provides a <strong>pessimistic lower bound</strong> on <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCPI%7D%7D">. We ignore updates when they would make things “too good to be true.”</p>
<blockquote class="blockquote">
<p><strong>LLM Context (from Umar Jamil Video):</strong> In language model fine-tuning, the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta(a_t%7Cs_t)"> is the probability the model assigns to token <img src="https://latex.codecogs.com/png.latex?a_t"> given the context <img src="https://latex.codecogs.com/png.latex?s_t"> (prompt + previously generated tokens). The probability ratio <img src="https://latex.codecogs.com/png.latex?r_t(%5Ctheta)"> measures how much more or less likely the fine-tuned model is to generate a particular token compared to the reference policy. Clipping ensures that no single token’s probability can change by more than a factor of <img src="https://latex.codecogs.com/png.latex?(1%20%5Cpm%20%5Cepsilon)"> in a single update iteration, preventing the model from “overreacting” to high-advantage tokens.</p>
</blockquote>
</section>
<section id="ppo-objective" class="level3">
<h3 class="anchored" data-anchor-id="ppo-objective">PPO Objective</h3>
<p>In practice, PPO combines the clipped policy objective with two additional terms:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7BL%5E%7B%5Ctext%7BPPO%7D%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_t%5Cleft%5BL%5E%7B%5Ctext%7BCLIP%7D%7D_t(%5Ctheta)%20-%20c_1%20L%5E%7B%5Ctext%7BVF%7D%7D_t(%5Ctheta)%20+%20c_2%20S%5B%5Cpi_%5Ctheta%5D(s_t)%5Cright%5D%7D%20%5Ctag%7BVIII.II%7D%0A"></p>
<p><strong>1. Value Function Loss (<img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BVF%7D%7D">):</strong> Recall from Section V that we need a value function <img src="https://latex.codecogs.com/png.latex?V_%5Cphi(s)"> to compute advantage estimates. The value function is trained to minimize the squared error between its predictions and the actual returns:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BVF%7D%7D_t(%5Ctheta)%20=%20%5Cleft(V_%5Ctheta(s_t)%20-%20V_t%5E%7B%5Ctext%7Btarget%7D%7D%5Cright)%5E2%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?V_t%5E%7B%5Ctext%7Btarget%7D%7D"> is typically the discounted return-to-go. When the policy and value function share parameters (common in LLM fine-tuning where both use the same transformer backbone), this loss is subtracted from the objective (hence the negative sign, since we maximize <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BPPO%7D%7D"> but minimize <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BVF%7D%7D">).</p>
<p><strong>2. Entropy Bonus (<img src="https://latex.codecogs.com/png.latex?S%5B%5Cpi_%5Ctheta%5D">):</strong> To encourage exploration and prevent premature convergence to deterministic policies, PPO adds an entropy loss:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AS%5B%5Cpi_%5Ctheta%5D(s_t)%20=%20-%5Csum_a%20%5Cpi_%5Ctheta(a%7Cs_t)%20%5Clog%20%5Cpi_%5Ctheta(a%7Cs_t)%0A"></p>
<p>Here, the coefficients <img src="https://latex.codecogs.com/png.latex?c_1,%20c_2%20%3E%200"> control the regularization strength.</p>
</section>
</section>
<section id="ix-complete-ppo-objective-with-kl-penalty" class="level2">
<h2 class="anchored" data-anchor-id="ix-complete-ppo-objective-with-kl-penalty">IX: Complete PPO Objective with KL Penalty</h2>
<p>When fine-tuning an LLM with “vanilla” PPO, the policy learns to maximize rewards from the reward model. However, the reward model is an imperfect proxy for human preferences. It is a neural network trained on limited data that can be exploited. Without constraints, the policy may discover adversarial outputs that achieve high reward scores while producing text that:</p>
<ul>
<li>Degenerates into repetitive or nonsensical patterns that “fool” the reward model</li>
<li>Drifts far from natural language, losing fluency and coherence</li>
<li>Exploits spurious correlations learned by the reward model</li>
</ul>
<p>This phenomenon is called <strong>reward hacking</strong>. The policy finds a way to “game” the reward model rather than genuinely improving response quality.</p>
<p>To prevent reward hacking, the <a href="https://arxiv.org/pdf/2203.02155">InstructGPT paper</a> adds a <strong>KL divergence penalty</strong> that regularizes the policy to stay close to a <strong>reference model</strong> <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctext%7Bref%7D%7D"> (typically the SFT model before RL fine-tuning).</p>
<p>From Section VIII, the PPO objective (to be maximized via gradient ascent) consists of three terms:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%5E%7B%5Ctext%7BPPO%7D%7D(%5Ctheta)%20=%20%5Cunderbrace%7BL%5E%7B%5Ctext%7BCLIP%7D%7D(%5Ctheta)%7D_%7B%5Ctext%7BClipped%20Policy%20Objective%7D%7D%20-%20%5Cunderbrace%7Bc_1%20L%5E%7B%5Ctext%7BVF%7D%7D(%5Ctheta)%7D_%7B%5Ctext%7BValue%20Function%20Loss%7D%7D%20+%20%5Cunderbrace%7Bc_2%20S%5B%5Cpi_%5Ctheta%5D%7D_%7B%5Ctext%7BEntropy%20Bonus%7D%7D%0A"></p>
<p>Now, we don’t use raw reward model scores directly. Instead, we define a <strong>KL-penalized reward</strong> that regularizes the policy to stay close to a reference model <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctext%7Bref%7D%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7Br_%7B%5Ctext%7Btotal%7D%7D(s_t,%20a_t)%20=%20r_%7B%5Ctext%7BRM%7D%7D(s_t,%20a_t)%20-%20%5Cbeta%20%5Ccdot%20D_%7B%5Ctext%7BKL%7D%7D%5Cleft(%5Cpi_%5Ctheta(%5Ccdot%7Cs_t)%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Ccdot%7Cs_t)%5Cright)%7D%20%5Ctag%7BIX.I%7D%0A"></p>
<p>where: - <img src="https://latex.codecogs.com/png.latex?r_%7B%5Ctext%7BRM%7D%7D(s_t,%20a_t)"> is the reward signal at timestep <img src="https://latex.codecogs.com/png.latex?t"> - <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is the KL penalty coefficient - <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Ctext%7Bref%7D%7D"> is the frozen reference model</p>
<p>At each token position, the KL divergence simplifies to:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD_%7B%5Ctext%7BKL%7D%7D%5Cleft(%5Cpi_%5Ctheta(%5Ccdot%7Cs_t)%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D(%5Ccdot%7Cs_t)%5Cright)%20=%20%5Cmathbb%7BE%7D_%7Ba%20%5Csim%20%5Cpi_%5Ctheta%7D%5Cleft%5B%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta(a%7Cs_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a%7Cs_t)%7D%5Cright%5D%0A"></p>
<p>In practice we estimate this expectation with the sampled token <img src="https://latex.codecogs.com/png.latex?a_t">, yielding: <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%20d_t=%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta(a_t%7Cs_t)%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D(a_t%7Cs_t)%7D%0A"></p>
<p>Note that the reward model <img src="https://latex.codecogs.com/png.latex?r_%5Cphi(x,%20y)"> produces a single scalar for the complete response <img src="https://latex.codecogs.com/png.latex?(x,%20y)">. This score is assigned only at the <strong>final token</strong> <img src="https://latex.codecogs.com/png.latex?T">, while the KL penalty applies at <strong>every token</strong>. <img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7Br%7D_%5Cphi%20=%20%5Cbegin%7Bcases%7D%0A-%5Cbeta%20%5Ccdot%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta(a_t%20%7C%20s_t)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_t%20%7C%20s_t)%7D%20&amp;%20%5Ctext%7Bif%20%7D%20t%20%3C%20T%20%5C%5C%5B8pt%5D%0Ar_%5Cphi(x,%20y)%20-%20%5Cbeta%20%5Ccdot%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta(a_T%20%7C%20s_T)%7D%7B%5Cpi_%7B%5Ctext%7Bref%7D%7D(a_T%20%7C%20s_T)%7D%20&amp;%20%5Ctext%7Bif%20%7D%20t%20=%20T%0A%5Cend%7Bcases%7D%0A"></p>
<p>The KL penalty serves two purposes: 1. <strong>Prevents reward hacking</strong>: The policy cannot drift arbitrarily far from natural language 2. <strong>Maintains fluency</strong>: Outputs remain similar in distribution to the well-trained SFT model</p>
<p>It modifies the advantage estimates <img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D_t"> used in PPO through the modified per-token rewards. However, it is mathematically equivalent (and more efficient in implementation) to add the KL term directly to the objective. The PPO objective with KL penalty is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AJ(%5Ctheta)%20=%20%5Cunderbrace%7B%5Cmathbb%7BE%7D_%7Ba%20%5Csim%20%5Cpi_%5Ctheta%7D%5Cleft%5Br_%7B%5Ctext%7BRM%7D%7D(s,%20a)%5Cright%5D%7D_%7B%5Ctext%7BVanilla%20PPO%20objective%7D%7D%20-%20%5Cunderbrace%7B%5Cbeta%20%5Ccdot%20D_%7B%5Ctext%7BKL%7D%7D(%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D)%7D_%7B%5Ctext%7BKL%20penalty%20term%7D%7D%0A"></p>
<p>The first term is exactly what vanilla PPO optimizes using the clipped surrogate. The KL penalty term appears as a separate additive component that penalizes divergence from the reference model. Substituting the PPO clipped surrogate for the first term:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AJ_%7B%5Ctext%7Bc%7D%7D(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_t%5Cleft%5B%5Cmin%5Cleft(r_t(%5Ctheta)%20%5Chat%7BA%7D_t,%20%5Ctext%7Bclip%7D(r_t(%5Ctheta),%201-%5Cepsilon,%201+%5Cepsilon)%20%5Chat%7BA%7D_t%5Cright)%5Cright%5D%20-%20%5Cbeta%20%5Ccdot%20D_%7B%5Ctext%7BKL%7D%7D(%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D)%0A"></p>
<p>Combining all components, the <strong>complete PPO objective with KL penalty</strong> (to be maximized) is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7BL%5E%7B%5Ctext%7BRLHF%7D%7D(%5Ctheta)%20=%20%5Cunderbrace%7BL%5E%7B%5Ctext%7BCLIP%7D%7D(%5Ctheta)%7D_%7B%5Ctext%7BPolicy%20Objective%7D%7D%20-%20%5Cunderbrace%7Bc_1%20L%5E%7B%5Ctext%7BVF%7D%7D(%5Ctheta)%7D_%7B%5Ctext%7BValue%20Loss%7D%7D%20+%20%5Cunderbrace%7Bc_2%20S%5B%5Cpi_%5Ctheta%5D%7D_%7B%5Ctext%7BEntropy%20Bonus%7D%7D%20-%20%5Cunderbrace%7B%5Cbeta%20%5Ccdot%20D_%7B%5Ctext%7BKL%7D%7D(%5Cpi_%5Ctheta%20%5C%7C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D)%7D_%7B%5Ctext%7BKL%20Penalty%7D%7D%7D%20%5Ctag%7BIX.II%7D%0A"></p>
<p>Here, each term serves a distinct purpose:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Policy Objective</strong> <img src="https://latex.codecogs.com/png.latex?L%5E%7B%5Ctext%7BCLIP%7D%7D"></td>
<td>Improves the policy while preventing destructive updates via clipping</td>
</tr>
<tr class="even">
<td><strong>Value Loss</strong> <img src="https://latex.codecogs.com/png.latex?c_1%20L%5E%7B%5Ctext%7BVF%7D%7D"></td>
<td>Trains the critic for accurate advantage estimation (subtracted to minimize)</td>
</tr>
<tr class="odd">
<td><strong>Entropy Bonus</strong> <img src="https://latex.codecogs.com/png.latex?c_2%20S%5B%5Cpi_%5Ctheta%5D"></td>
<td>Encourages exploration, prevents premature convergence</td>
</tr>
<tr class="even">
<td><strong>KL Penalty</strong> <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20D_%7B%5Ctext%7BKL%7D%7D"></td>
<td>Prevents reward hacking, maintains language quality (subtracted to penalize drift)</td>
</tr>
</tbody>
</table>
<p>It is important to distinguish the two KL-related mechanisms in the complete loss. The PPO clipping mechanism acts as a <strong>short-term anchor</strong> that constrains how much the policy can change in a single update, while the KL penalty is a <strong>long-term anchor</strong> that constrains how far the policy can drift from its starting point across all of training.</p>
</section>
<section id="finally-done" class="level2">
<h2 class="anchored" data-anchor-id="finally-done">Finally done…</h2>
<p>And that’s the full derivation! What I find satisfying is that every term in the final loss has a specific purpose. Each one exists because we ran into a specific problem along the way and needed to fix it. I will admit it was not easy to understand all the math and concepts behind the loss. I still do not fully understand every detail but I understand it far better than I did a few days ago.</p>
<p>I hope this was useful. If you spot any errors in derivation (which I’m sure there are) or have suggestions, feel free to reach out.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><strong>Video:</strong>
<ul>
<li><a href="https://www.youtube.com/watch?v=qGyFrqc34yc">Umar Jamil’s video on RLHF and PPO</a>: A comprehensive and must-watch video covering RLHF and PPO concepts.</li>
</ul></li>
<li><strong>Papers:</strong>
<ul>
<li><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>: The foundational PPO paper introducing the clipped surrogate objective.</li>
<li><a href="https://arxiv.org/pdf/2203.02155">Training language models to follow instructions with human feedback</a>: The InstructGPT paper demonstrating PPO with KL penalty to mitigate reward hacking in LLM fine-tuning.</li>
<li><a href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>: The TRPO paper that motivates the trust region constraints used in PPO.</li>
<li><a href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>: GAE paper introducing the exponentially-weighted advantage estimator for variance reduction in policy gradients.</li>
</ul></li>
</ul>


</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2025-12-25-deriving-ppo-loss.html</guid>
  <pubDate>Thu, 25 Dec 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>What I Learned Building SFT from the Ground Up</title>
  <link>https://garg-aayush.github.io/posts/2025-12-28-sft-from-scratch.html</link>
  <description><![CDATA[ 




<p>Over the past few weeks, I implemented supervised fine-tuning (SFT) from scratch, continuing a series of projects where I’m building foundational LLM components as a learning exercise from the ground up. Previously, I’ve worked through <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/gpt-2">implementing GPT-2 from scratch</a> and <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/llm-inference">writing LLM inference scripts from the ground up</a>. Naturally, SFT was the next step in this series.</p>
<p>One thing I realized pretty quickly, writing the training scripts from scratch is not the most difficult part. However, making it actually work, producing results that seems reasonable is where the real challenge begins 😅. You run into all sorts of difficulties: debugging annoying errors, dealing with gradient instabilities, getting vLLM to cooperate for intermediate evaluation (especially with limited GPU memory) etc. <strong>These are the things that eat up your time but teach you the most</strong>.</p>
<p>In this post, I want to share not just what I built, but the building and debugging journey that got me there.</p>
<section id="what-i-built" class="level2">
<h2 class="anchored" data-anchor-id="what-i-built">What I Built</h2>
<p>I loosely followed Stanford’s <a href="https://github.com/stanford-cs336/assignment5-alignment">CS336 Assignment 5</a> as a guide, wrote all the SFT core components, and ran two sets of experiments:</p>
<p><strong>1. Reasoning SFT</strong>: Fine-tuned <a href="https://huggingface.co/Qwen/Qwen2.5-Math-1.5B">Qwen2.5-Math-1.5B</a> on math reasoning traces to improve step-by-step problem solving capabilities.</p>
<p align="center">
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/sft_reasoning_summary.png" alt="Reasoning SFT Results" width="600">
</p><p align="center">
<em>Best: <b>53.4% reward accuracy</b> (up from 2.9% baseline) with 99.3% format accuracy</em>
</p>
<p></p>
<strong>2. Instruction SFT</strong>: Fine-tuned <a href="https://huggingface.co/meta-llama/Llama-3.1-8B">Llama-3.1-8B</a> on UltraChat-200K + SafetyLlama for general instruction following and safety.
<p align="center">
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/instruct_finetune_results_nomask.png" alt="Reasoning SFT Results">
</p><p align="center">
<em>Best: GSM8K 16-&gt;<b>33%</b>, Safety 62-&gt;<b>78%</b>, AlpacaEval 1.6-&gt;<b>5.3%</b>, MMLU ~58%</em>
</p>
<p></p>
<p>All experiment code, training scripts, and detailed notes are available in my <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/sft">building-from-scratch</a> repo.</p>
</section>
<section id="part-1-reasoning-sft-with-qwen2.5-math-1.5b" class="level2">
<h2 class="anchored" data-anchor-id="part-1-reasoning-sft-with-qwen2.5-math-1.5b">Part 1: Reasoning SFT with Qwen2.5-Math-1.5B</h2>
<p>The idea behind reasoning SFT is simple. You take a base model that barely outputs correct answers, show it high-quality examples of <em>how</em> to solve problems step-by-step, and train it to replicate/mimic that reasoning process. The model learns to think in a structured format with first generating reasoning inside <code>&lt;think&gt;</code> tags, then outputting the final answer in <code>&lt;answer&gt;</code> tags.</p>
<p>My starting point was <code>Qwen2.5-Math-1.5B</code>, which had quite poor baseline accuracies on the math validation set: <strong>~2.9%</strong> for answers and <strong>~14%</strong> for format.</p>
<section id="creating-the-dataset-first-challenge" class="level3">
<h3 class="anchored" data-anchor-id="creating-the-dataset-first-challenge">Creating the Dataset: First Challenge</h3>
<p>The original CS336 MATH dataset used for SFT training is not publicly available, so I had to create my own. My dataset creation pipeline had three steps:</p>
<ol type="1">
<li><p><strong>Source problems</strong>: I used <a href="https://huggingface.co/datasets/hiyouga/math12k">hiyouga/math12k</a> dataset to create the training set, carefully filtering out any problems that appeared in the validation set to avoid data leakage.</p></li>
<li><p><strong>Generate reasoning traces</strong>: The next and most important step is to <strong>generate the reasoning traces for each problem</strong>. I used <code>gpt-oss-120b</code> model to generate them via Fireworks Batch Inference API. It costed me around ~$4 to generate the reasoning traces.</p></li>
<li><p><strong>Filter for quality</strong>: I also created a subset of around ~3.6K examples by filtering out the reasoning traces that led to wrong answers.</p></li>
</ol>
</section>
<section id="the-training-loop-per-token-vs.-sequence-loss" class="level3">
<h3 class="anchored" data-anchor-id="the-training-loop-per-token-vs.-sequence-loss">The Training Loop: Per-Token vs.&nbsp;Sequence Loss</h3>
<p>The original assignment uses sequence level loss normalization where you sum the loss over all tokens in a sequence and normalize by a constant, not by the variable number of tokens.</p>
<p>While running the initial experiments, I noticed the gradient norms were really large values, and training felt unstable. Even though the loss seemed to be going in the right direction, something didn’t feel right. After some investigation, I realized the issue: with variable-length sequences (my training examples ranged from short to quite long), longer sequences contribute more to the gradient than shorter ones. This creates high variance in gradient updates.</p>
<table align="center">
<tbody><tr>
<td>
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/grad_norm_wo_token_loss.png" alt="Gradient Norm without Per-Token Loss" width="400">
</td>
<td>
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/grad_norm_w_token_loss.png" alt="Gradient Norm with Per-Token Loss" width="400">
</td>
</tr>
</tbody></table>
<p align="center">
<em>Left: Sequence-level loss (high variance gradients) | Right: Per-token loss (stable gradients)</em>
</p>
<p>Thus, I added a <code>per_token_loss</code> flag to my training step which when enabled normalizes the loss by the actual number of response tokens in each sequence. The difference was noticeable with subtle improved accuracy. More importantly, the gradients became much more stable with per-token normalization.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Run</th>
<th>Loss Normalization</th>
<th>Reward Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>run_filtered</td>
<td>Per-token</td>
<td>0.5204</td>
</tr>
<tr class="even">
<td>run_filtered-res-len</td>
<td>Sequence-level</td>
<td>0.5106</td>
</tr>
</tbody>
</table>
</section>
<section id="vllm-integration-the-debugging-nightmare" class="level3">
<h3 class="anchored" data-anchor-id="vllm-integration-the-debugging-nightmare">vLLM Integration: The Debugging Nightmare</h3>
<p>Here’s where things got really tricky and painful. I wanted to run intermediate evaluations during training using vLLM for fast inference. The assignment provided code for this but it was written for an older vLLM version and nothing worked out of the box 😅.</p>
<p><strong>Problem 1: vLLM initialization changed</strong></p>
<p>The assignment’s approach used a separate GPU dedicated to running vLLM as an inference server. I wasn’t keen on this setup anyway as it meant paying for an extra GPU just for inference. But more importantly, the approach broke completely with the vLLM version I was using (0.7+). The initialization logic had changed, and the old code just wouldn’t run.</p>
<p><strong><em>Solution</em></strong>: I switched to the <code>colocate</code> approach, running vLLM on the same device as the training model. I came across this in the excellent <a href="https://huggingface.co/blog/vllm-colocate">HuggingFace blog post on co-located vLLM</a>. Though, this required being more careful about GPU memory (setting appropriate values for <code>gpu_memory_utilization</code>, <code>max_model_len</code>, and <code>max_num_seqs</code>), but it actually works and saves on GPU costs.</p>
<p><strong>Problem 2: Missing <code>model_executor</code> attribute</strong></p>
<p>When I tried to load updated model weights into the vLLM instance during training, I hit this error:</p>
<pre><code>AttributeError: 'LLMEngine' object has no attribute 'model_executor'</code></pre>
<p>This was really annoying because the attribute clearly existed in the vLLM source code. After much debugging, I found two solutions: - Downgrade to vLLM 0.10.2, or - If using vLLM 0.11.0, set the environment variable <code>VLLM_ENABLE_V1_MULTIPROCESSING=0</code> at the start of the script</p>
<p>I went with the environment variable approach since I didn’t want to deal with version conflicts.</p>
<p><strong>Problem 3: The <code>_orig_mod</code> issue</strong></p>
<p>With <code>torch.compile</code> enabled on my model (for faster training), loading weights into vLLM failed with the below error. The issue is that <code>torch.compile</code> wraps the original model and stores the actual weights under <code>_orig_mod</code>. When loading weights into vLLM, you need to access them through this attribute, not directly from the compiled model.</p>
<pre><code>ValueError: There is no module or parameter named '_orig_mod' in Qwen2ForCausalLM</code></pre>
<p><strong><em>Solution</em></strong>: In my <code>load_policy_into_vllm_instance</code> function, I made sure to load from <code>model._orig_mod</code> when the model is compiled.</p>
<p><strong>These three issues cost me almost a day. However, it was worth it because I learned a lot about vLLM and how to integrate it in training run</strong></p>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>After all that debugging, here’s what the training runs achieved:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/sft_train_reasoning_results.png" class="img-fluid figure-img"></p>
<figcaption>Reasoning SFT Results</figcaption>
</figure>
</div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Run</th>
<th>Training Data</th>
<th>Reward Accuracy</th>
<th>Format Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>baseline</td>
<td>-</td>
<td>0.0288</td>
<td>0.1438</td>
</tr>
<tr class="even">
<td>run_all</td>
<td>Full 4.8K (correct + incorrect)</td>
<td>0.4214</td>
<td>0.9924</td>
</tr>
<tr class="odd">
<td>run_filtered</td>
<td>Filtered 3.6K (correct only)</td>
<td>0.5204</td>
<td>0.9906</td>
</tr>
<tr class="even">
<td>run_filtered-2epoch</td>
<td>Filtered 3.6K (2 epochs)</td>
<td>0.5336</td>
<td>0.9926</td>
</tr>
</tbody>
</table>
<p><strong>Key takeaways</strong>: - Filtering out incorrect reasoning traces boosted accuracy from 42% to 52%. Training on wrong traces teaches the model wrong patterns. - The model quickly learned the output format (99%+ format accuracy after training). - Running for 2 epochs gave a boost in accuracy though a marginal one.</p>
</section>
</section>
<section id="part-2-instruction-sft-with-llama-3.1-8b" class="level2">
<h2 class="anchored" data-anchor-id="part-2-instruction-sft-with-llama-3.1-8b">Part 2: Instruction SFT with Llama-3.1-8B</h2>
<p>With the reasoning SFT working, I moved on to the second part: instruction fine-tuning. This loosely follows the <a href="https://github.com/stanford-cs336/assignment5-alignment/blob/main/cs336_spring2025_assignment5_supplement_safety_rlhf.pdf">CS336 Supplementary Assignment 5</a>, where the goal is to build a model that can follow diverse instructions and refuse harmful requests.</p>
<p>Unlike reasoning SFT, instruction fine-tuning uses conversational instruction-response pairs. The training data combines <strong>UltraChat-200K</strong> (diverse multi-turn conversations) and <strong>SafetyLlama</strong> (safety-focused examples) totaling around 200K examples, formatted using the Alpaca prompt template.</p>
<p>For evaluation, I used four benchmarks as specified in the assignment: - <strong>GSM8K</strong>: Grade-school math problems (tests math reasoning) - <strong>MMLU</strong>: Multiple-choice questions across 57 subjects (tests factual knowledge) - <strong>AlpacaEval</strong>: Open-ended instructions judged by LLM-as-judge (tests instruction-following quality)<br>
- <strong>Simple Safety Tests (SST)</strong>: Harmful prompts to test refusal behavior (tests safety)</p>
<section id="the-prompt-masking-implementation-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-prompt-masking-implementation-problem">The Prompt Masking Implementation Problem</h3>
<p>I wanted to experiment with <strong>prompt masking</strong> i.e.&nbsp;masking prompt tokens (labels = -100) so the loss is computed only on response tokens, helping the model focus on generating good responses.</p>
<p><strong>Problem 1: BPE tokenization boundary issues</strong></p>
<p>Implementing this led to an interesting debugging session. When I tokenized the prompt separately (ending with <code>"### Response:\n"</code>) and compared it to the tokens in the full sequence (prompt + response), the boundary tokens didn’t match. This is a known issue of BPE tokenization: subword merging behavior changes based on context.</p>
<p>My first instinct was to try to implement complex boundary detection logic. However, I thought let’s try the simplest fix that works.</p>
<p><strong><em>Solution</em></strong>: I decided to drop the last token from the prompt before masking. This is a bit quick fix. However, I might train on one extra formatting token (likely just a newline) but will never accidentally mask response tokens.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Conservative fix: drop last prompt token to avoid boundary issues</span></span>
<span id="cb3-2">prompt_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(prompt_tokens) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-3">labels[:prompt_length] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span></code></pre></div></div>
<p><strong>Problem 2: Very short or empty responses</strong></p>
<p>Another issue I ran into with prompt masking, some training examples had very short or empty responses. When all tokens are masked leaving only a few response tokens, the cross-entropy loss calculation can produce extreme values or NaNs.</p>
<p><strong><em>Solution</em></strong>: The fix was simple. I filtered out examples with very short responses (0-2 words) from both training and validation sets.</p>
</section>
<section id="setting-up-alpacaeval" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-alpacaeval">Setting Up AlpacaEval</h3>
<p>A quick note on the AlpacaEval evaluation setup. It uses an LLM-as-judge approach where an annotator model compares outputs from your mode against GPT-4 reference responses.</p>
<p>The assignment suggested deploying <code>Llama-3.3-70B-Instruct</code> locally as the annotator, but that requires at least two GPUs which is not cost effective (atleast for my case). Instead, I used <code>Llama-3.3-70B-Instruct</code> via Fireworks API. This required some config tweaking (API key mapping, judge configuration) but works well.</p>
</section>
<section id="results-and-analysis" class="level3">
<h3 class="anchored" data-anchor-id="results-and-analysis">Results and Analysis</h3>
<p>I ran two experiments: one with prompt masking (<code>mask</code>) and one without (<code>no-mask</code>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/instruct_finetune_results_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Instruction Fine-tuning Comparison</figcaption>
</figure>
</div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Baseline</th>
<th>No-Mask</th>
<th>Mask</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>GSM8K</strong></td>
<td>16.4%</td>
<td>29.0%</td>
<td><strong>32.7%</strong></td>
</tr>
<tr class="even">
<td><strong>MMLU</strong></td>
<td>58.1%</td>
<td>58.4%</td>
<td>58.2%</td>
</tr>
<tr class="odd">
<td><strong>SST Safety</strong></td>
<td>62.0%</td>
<td><strong>78.0%</strong></td>
<td>77.0%</td>
</tr>
<tr class="even">
<td><strong>AlpacaEval</strong></td>
<td>1.57%</td>
<td><strong>5.3%</strong></td>
<td>4.5%</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>GSM8K (16% -&gt; 29-33%)</strong>: Both approaches significantly improved math reasoning, but masking helped more (32.7% vs 29.0%).</p></li>
<li><p><strong>Safety (62% -&gt; 78%)</strong>: You see big improvement as expected since the training data includes SafetyLlama examples.</p></li>
<li><p><strong>AlpacaEval (1.6% -&gt; 5.3%)</strong>: The conversational instruction-following improved substantially. Interestingly, no-mask performed slightly better (5.3% vs 4.5%). My guess: training on the full sequence helps the model learn overall conversational patterns and produce more naturally flowing responses that match the prompt style.</p></li>
<li><p><strong>MMLU (~58% -&gt; ~58%)</strong>: This stayed flat and that’s actually good news. MMLU tests factual knowledge which is encoded during pre-training. SFT teaches the model <em>how</em> to respond, not <em>what</em> to know. The fact that MMLU didn’t drop means we avoided catastrophic forgetting issue.</p></li>
</ul>
<p align="center">
<img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/sft/results/plots/instruct_finetune_mmlu_comparison.png" alt="MMLU Subject Comparison">
</p><p align="center">
<em>Looking at individual MMLU subjects, some regressed slightly (college math: 33% -&gt; 26%) while others improved slightly, leading to near-zero net change.</em>
</p>
<p></p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>While writing the SFT code from scratch, I ran into a lot of debugging challenges. It was at times painstaking and frustrating but was also a valuable learning experience. By debugging, I learned a lot about how things work under the hood, and the whole experience prepares you for how to go about debugging code/projects in the future.</p>
<p>I leave you with some of the debugging tips I came across:</p>
<ul>
<li><strong>vLLM OOM</strong>: Tune <code>max_model_len</code>, <code>max_num_seqs</code>, and <code>gpu_memory_utilization</code> and start conservative.</li>
<li><strong>Per-token loss</strong>: Normalize by response token count to prevent long sequences from dominating gradients.</li>
<li><strong>torch.compile + vLLM</strong>: Access weights via <code>model._orig_mod</code> when loading into vLLM.</li>
<li><strong>BPE boundaries</strong>: Drop last prompt token before masking to avoid tokenization edge cases.</li>
<li><strong>Data quality matters</strong>: Filtering incorrect traces gave me a 10% accuracy boost.</li>
<li><strong>vLLM version issues</strong>: Set <code>VLLM_ENABLE_V1_MULTIPROCESSING=0</code> if <code>model_executor</code> is missing.</li>
</ul>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>I have made all the code, datasets, and model checkpoints publicly accessible.</p>
<ul>
<li><strong>Code</strong>: <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/sft">building-from-scratch/sft</a></li>
<li><strong>Datasets</strong>: <a href="https://huggingface.co/datasets/garg-aayush/sft-cs336-assign5-datasets">garg-aayush/sft-cs336-assign5-datasets</a></li>
<li><strong>Checkpoints</strong>:
<ul>
<li>Reasoning:
<ul>
<li>run_all: <a href="https://huggingface.co/garg-aayush/qwen-2.5-math-sft-all">qwen-2.5-math-sft-all-2epoch</a></li>
<li>run_filtered: <a href="https://huggingface.co/garg-aayush/qwen-2.5-math-sft-filtered">qwen-2.5-math-sft-filtered-2epoch</a></li>
<li>run_filtered-res-len: <a href="https://huggingface.co/garg-aayush/qwen-2.5-math-sft-filtered-res-len">qwen-2.5-math-sft-filtered-res-len</a></li>
<li>run_filtered-2epoch: <a href="https://huggingface.co/garg-aayush/qwen-2.5-math-sft-filtered-2epoch">qwen-2.5-math-sft-filtered-2epoch</a></li>
</ul></li>
<li>Instruction d:
<ul>
<li>run_mask: <a href="https://huggingface.co/garg-aayush/llama31-8b-sft-mask">llama31-8b-sft-mask</a></li>
<li>run_nomask: <a href="https://huggingface.co/garg-aayush/llama31-8b-sft-nomask">llama31-8b-sft-nomask</a></li>
</ul></li>
</ul></li>
<li><strong>Training logs</strong>: <a href="https://wandb.ai/garg-aayush/sft">wandb/sft</a> and <a href="https://wandb.ai/garg-aayush/sft_instruct">wandb/sft_instruct</a></li>
</ul>


</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2025-12-28-sft-from-scratch.html</guid>
  <pubDate>Wed, 03 Dec 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>A Guide to Building Custom Nodes in ComfyUI</title>
  <link>https://garg-aayush.github.io/posts/2025-09-10-build-custom-comfyui-node.html</link>
  <description><![CDATA[ 




<p><a href="https://www.comfy.org/">ComfyUI</a> is by far my favorite open-source software right now. Its intuitive node-based interface has transformed the way we build AI image and video generation workflows.</p>
<p>What I really appreciate about ComfyUI is its flexibility. You can easily extend it with your own custom nodes. Here, I’ll show you how to create custom nodes that let you add exactly the tools you need. I’ll use parts from my <a href="https://github.com/garg-aayush/ComfyUI-Svg2Raster">Svg2Raster</a> nodes as the running example for this purpose.</p>
<section id="svg2raster" class="level2">
<h2 class="anchored" data-anchor-id="svg2raster"><a href="https://github.com/garg-aayush/ComfyUI-Svg2Raster">Svg2Raster</a></h2>
<p>ComfyUI does not natively support vector graphics like <a href="https://en.wikipedia.org/wiki/SVG">SVGs</a>. I often work with them and needed lightweight nodes to load (SVG-&gt;JPEGs/PNGs) and manipulate SVGs in ComfyUI.</p>
<p>Thus, I built <a href="https://github.com/garg-aayush/ComfyUI-Svg2Raster">Svg2Raster</a>, a small custom node package that makes it easy to use SVGs with other nodes. <img src="https://garg-aayush.github.io/static/img/blog-2025-09-10/workflow.png" class="img-fluid" alt="SVG2Raster"></p>
</section>
<section id="writing-your-custom-comfyui-node" class="level1">
<h1>Writing your Custom ComfyUI Node</h1>
<p>So here I am assuming that you are fairly comfortable using ComfyUI and you already have ComfyUI installed locally on your system/cloud instance.</p>
<section id="step-1-validate-the-core-logic-first" class="level2">
<h2 class="anchored" data-anchor-id="step-1-validate-the-core-logic-first">Step 1: Validate the core logic first</h2>
<p>I prefer not to start with the ComfyUI node API. First, I like to write a simple Python notebook to test the functionalities I actually need. This validates your core code logic and packages in isolation.</p>
<p>In my case, I needed a way to read, rasterize and manipulate the SVGs. Thus, I tested all the relevant operations using the core packages <a href="https://cairosvg.org/">CairoSVG</a> and <a href="https://python-pillow.org/">Pillow</a>.</p>
<p>For example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simple SVG read and conversion check</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cairosvg</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> PIL <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image, ImageOps</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> io</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Read SVG file</span></span>
<span id="cb1-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logo.svg'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'r'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf-8'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb1-8">    svg_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> f.read()</span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Basic conversion to PNG</span></span>
<span id="cb1-11">img_bytes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cairosvg.svg2png(bytestring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>svg_text.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf-8'</span>), </span>
<span id="cb1-12">                              output_width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">600</span>)</span>
<span id="cb1-13">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Image.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(io.BytesIO(img_bytes)).convert(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RGBA'</span>)</span>
<span id="cb1-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Image size: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>img<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>size<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<p>If you want to see all the code snippets (width/height controls, color and border manipulations etc.), please check out the full <a href="https://github.com/garg-aayush/ComfyUI-Svg2Raster/blob/main/svg2png.ipynb">notebook</a>.</p>
<p><strong>Once you have a working standalone script or code, wrapping it as a ComfyUI node is mostly boilerplate.</strong></p>
</section>
<section id="step-2-understand-the-anatomy-of-a-custom-node" class="level2">
<h2 class="anchored" data-anchor-id="step-2-understand-the-anatomy-of-a-custom-node">Step 2: Understand the Anatomy of a Custom Node</h2>
<p>Every ComfyUI node is a Python class with specific methods that ComfyUI expects. Here are the essential components:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>INPUT_TYPES</code></td>
<td>What inputs your node accepts</td>
</tr>
<tr class="even">
<td><code>RETURN_TYPES</code></td>
<td>What it outputs to other nodes</td>
</tr>
<tr class="odd">
<td><code>RETURN_NAMES</code></td>
<td>Optional labels for outputs</td>
</tr>
<tr class="even">
<td><code>FUNCTION</code></td>
<td>The method name that runs your logic</td>
</tr>
<tr class="odd">
<td><code>CATEGORY</code></td>
<td>Where it appears in ComfyUI’s node menu</td>
</tr>
</tbody>
</table>
<p>ComfyUI handles the rest of UI, connections and execution order. For example, this is how a simple custom node class will looks like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> LoadSVG:</span>
<span id="cb2-2">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@classmethod</span></span>
<span id="cb2-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> INPUT_TYPES(cls):</span>
<span id="cb2-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {</span>
<span id="cb2-5">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"required"</span>: {</span>
<span id="cb2-6">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"svg_file"</span>: (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"STRING"</span>, {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"default"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"file.svg"</span>}),</span>
<span id="cb2-7">            }</span>
<span id="cb2-8">        }</span>
<span id="cb2-9">    </span>
<span id="cb2-10">    RETURN_TYPES <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IMAGE"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"STRING"</span>)</span>
<span id="cb2-11">    RETURN_NAMES <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"image"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"svg_text"</span>)</span>
<span id="cb2-12">    FUNCTION <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"load_svg"</span></span>
<span id="cb2-13">    CATEGORY <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Svg2Raster"</span></span>
<span id="cb2-14">    </span>
<span id="cb2-15">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> load_svg(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, svg_file):</span>
<span id="cb2-16">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Your actual logic here</span></span>
<span id="cb2-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (image_tensor, svg_text)</span></code></pre></div></div>
<p>This is a minimal ComfyUI node class explanation that I believe is good enough to start writing your own nodes. If you want more details, check the official ComfyUI custom node <a href="https://docs.comfy.org/custom-nodes/overview">documentation</a>.</p>
</section>
<section id="step-3-implementing-the-loadsvgimage-node" class="level2">
<h2 class="anchored" data-anchor-id="step-3-implementing-the-loadsvgimage-node">Step 3: Implementing the <strong>LoadSVGImage</strong> Node</h2>
<p>First, I set up the file structure in ComfyUI’s <code>custom_nodes</code> folder for my nodes package:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> ComfyUI/custom_nodes</span>
<span id="cb3-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mkdir</span> svg2raster</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> svg2raster</span></code></pre></div></div>
<p>Then I create two essential files:</p>
<p><code>__init__.py</code>: it allows ComfyUI to import your custom nodes.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> .svg2raster_node <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">__all__</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [ <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NODE_CLASS_MAPPINGS"</span>,</span>
<span id="cb4-4">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NODE_DISPLAY_NAME_MAPPINGS"</span>]</span></code></pre></div></div>
<p><code>svg2raster_node.py</code>: this is where the actual nodes code is written.</p>
<p>You can find the <strong>complete code</strong> for these nodes here: <a href="https://github.com/garg-aayush/ComfyUI-Svg2Raster/blob/main/svg2raster_node.py">svg2raster_node.py</a>. Here’s the boilerplate structure of <strong>LoadSVGImage</strong> node:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> LoadSVGImage:</span>
<span id="cb5-2">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@classmethod</span></span>
<span id="cb5-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> INPUT_TYPES(cls):</span>
<span id="cb5-4">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Define what inputs this node accepts"""</span></span>
<span id="cb5-5">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use `folder_paths` to access ComfyUI's input directory</span></span>
<span id="cb5-6">        input_dir <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> folder_paths.get_input_directory()</span>
<span id="cb5-7">        files <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [f <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> f <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> os.listdir(input_dir) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> os.path.isfile(os.path.join(input_dir, f)) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> f.lower().endswith(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.svg'</span>)]</span>
<span id="cb5-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {</span>
<span id="cb5-9">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"required"</span>: {</span>
<span id="cb5-10">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"svg"</span>: (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>(files), {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"image_upload"</span>: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>}),</span>
<span id="cb5-11">            }</span>
<span id="cb5-12">        }</span>
<span id="cb5-13">    </span>
<span id="cb5-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Output configuration</span></span>
<span id="cb5-15">    RETURN_TYPES <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"STRING"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IMAGE"</span>)</span>
<span id="cb5-16">    RETURN_NAMES <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"svg_text"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"preview_image"</span>)</span>
<span id="cb5-17">    FUNCTION <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"load_svg"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Method name to execute</span></span>
<span id="cb5-18">    CATEGORY <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"FromSVG/Tools"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Menu location</span></span>
<span id="cb5-19">    </span>
<span id="cb5-20">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> load_svg(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, svg, background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#FFFFFF"</span>):</span>
<span id="cb5-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Main execution method - does the actual work"""</span></span>
<span id="cb5-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Your logic here</span></span>
<span id="cb5-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (svg_text, image_tensor)</span>
<span id="cb5-24">    </span>
<span id="cb5-25">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@classmethod</span></span>
<span id="cb5-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> IS_CHANGED(cls, svg):</span>
<span id="cb5-27">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Returns file hash or modification time</span></span>
<span id="cb5-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">pass</span></span>
<span id="cb5-29">    </span>
<span id="cb5-30">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@classmethod</span></span>
<span id="cb5-31">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> VALIDATE_INPUTS(cls, svg):</span>
<span id="cb5-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Check if file exists, return error string if invalid</span></span>
<span id="cb5-33">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span></code></pre></div></div>
<p>Here, the helper methods serve crucial purposes: - <strong><code>IS_CHANGED</code></strong>: Tells ComfyUI when to re-execute the node - <strong><code>VALIDATE_INPUTS</code></strong>: Prevents crashes by validating inputs before execution</p>
<p>ComfyUI expects images as tensors in BHWC format (<code>batch</code>, <code>height</code>, <code>width</code>, <code>channels</code>) with values normalized to 0-1. Thus, you need to have a pil to tensor function.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _pil_to_tensor(pil_img: Image.Image):</span>
<span id="cb6-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Convert PIL image to ComfyUI IMAGE tensor: (B, H, W, C) in [0,1]"""</span></span>
<span id="cb6-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># conversion logic</span></span></code></pre></div></div>
<p>Finally, you need the mappings for ComfyUI to discover your nodes:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">NODE_CLASS_MAPPINGS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LoadSVGImage"</span>: LoadSVGImage,</span>
<span id="cb7-3">}</span>
<span id="cb7-4">NODE_DISPLAY_NAME_MAPPINGS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LoadSVGImage"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Load SVG Image"</span>,</span>
<span id="cb7-6">}</span></code></pre></div></div>
<p>Without these mappings, ComfyUI won’t find your nodes even if the code is correct.</p>
<p>Similarly, I also wrote a <code>RasterizeSVG</code> class for manipulating the loaded SVG. It takes the SVG text from <code>LoadSVGImage</code> and lets you adjust scale, dimensions, borders, and more. You will see the pattern is identical: define inputs, process with CairoSVG/PIL, convert to tensor, return.</p>
<p>That’s how you implement a node. Write a standalone functionality script, wrap it in the ComfyUI class structure, handle the tensor conversions, add the helper methods, and register it with the mappings.</p>
</section>
<section id="step-4-testing-your-custom-node-in-comfyui" class="level2">
<h2 class="anchored" data-anchor-id="step-4-testing-your-custom-node-in-comfyui">Step 4: Testing Your Custom Node in ComfyUI</h2>
<p>Once you are done implementing, testing is straightforward.</p>
<ol type="1">
<li>Ensure all your dependencies are installed, including <code>CairoSVG</code>:</li>
<li>Restart ComfyUI for it to detect the new node.</li>
<li>Find your nodes under the defined category</li>
</ol>
<p><strong>Note</strong>: I have also added the installation steps <a href="https://github.com/garg-aayush/ComfyUI-Svg2Raster?tab=readme-ov-file#installation">here</a>.</p>
</section>
<section id="step-5-sharing-and-publishing-your-node" class="level2">
<h2 class="anchored" data-anchor-id="step-5-sharing-and-publishing-your-node">Step 5: Sharing and Publishing Your Node</h2>
<p>Once everything worked, I created a GitHub repo for the nodes. You can see how I structured mine in the <a href="github.com/garg-aayush/ComfyUI-Svg2Raster">Svg2Raster</a> repo.</p>
<p>Some Essential files in the repo are:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>README.md</td>
<td>Clear installation instructions and usage examples</td>
</tr>
<tr class="even">
<td>requirements.txt</td>
<td>Python dependencies (cairosvg in my case)</td>
</tr>
<tr class="odd">
<td>pyproject.toml</td>
<td>Required if you plan to publish to ComfyUI Registry</td>
</tr>
<tr class="even">
<td>examples/</td>
<td>Optional Sample SVG files and workflow JSON files</td>
</tr>
</tbody>
</table>
<p><strong>Note</strong>: Having a good Readme and examples makes a huge difference for users trying to understand and use your nodes.</p>
<p>Once your repo is ready, you can even publish it to the <a href="https://registry.comfy.org/nodes/svg2raster">ComfyUI Registry</a>. There’s an excellent guide on <a href="https://docs.comfy.org/registry/publishing">publishing to ComfyUI Registry</a> - just follow those steps.</p>
<p>I also set up a GitHub Actions workflow that automatically publishes updates to the ComfyUI Registry whenever I push changes to my repo. This ensures the registry always has the latest version. You can check out my <a href="https://github.com/garg-aayush/ComfyUI-Svg2Raster/blob/main/.github/workflows/publish.yaml">workflow file</a> to see how I did it.</p>


</section>
</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2025-09-10-build-custom-comfyui-node.html</guid>
  <pubDate>Wed, 10 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Building GPT from Scratch: Following Karpathy’s Tutorial</title>
  <link>https://garg-aayush.github.io/posts/2025-09-08-building-gpt-from-scratch.html</link>
  <description><![CDATA[ 




<p>The Transformer architecture has become the workhorse behind modern LLMs. GPT-2/3/4/5, Llama, Claude, Gemini: they all are built on top of the same core architecture or its variants from the 2017 “Attention Is All You Need” paper. I wanted to understand this architecture properly, so I followed Andrej Karpathy’s <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">“Let’s Build GPT from Scratch”</a> video. It’s a 2-hour walkthrough where you start from an empty file and end up with a working Transformer.</p>
<p>I followed Karpathy’s video and captured each architectural addition as a separate commit. This let me see exactly how each component pulled down the validation loss. In this walkthrough, the training data is ~1M characters of Shakespeare and the goal is to generate Shakespeare-like text.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Component</th>
<th>Val Loss</th>
<th>Commit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td>Bigram Model</td>
<td>~2.49</td>
<td><a href="https://github.com/garg-aayush/building-from-scratch/commit/e0b5864"><code>e0b5864</code></a></td>
</tr>
<tr class="even">
<td>Update 1</td>
<td>Single Head Self-Attention</td>
<td>~2.4</td>
<td><a href="https://github.com/garg-aayush/building-from-scratch/commit/7b0e03a"><code>7b0e03a</code></a></td>
</tr>
<tr class="odd">
<td>Update 2</td>
<td>Multi-Head Attention</td>
<td>~2.28</td>
<td><a href="https://github.com/garg-aayush/building-from-scratch/commit/9d2a7b5"><code>9d2a7b5</code></a></td>
</tr>
<tr class="even">
<td>Update 3</td>
<td>Feed-Forward Network</td>
<td>~2.27</td>
<td><a href="https://github.com/garg-aayush/building-from-scratch/commit/c4c46ff"><code>c4c46ff</code></a></td>
</tr>
<tr class="odd">
<td>Update 4</td>
<td>Residual Connections</td>
<td>~2.09</td>
<td><a href="https://github.com/garg-aayush/building-from-scratch/commit/0239c07"><code>0239c07</code></a></td>
</tr>
<tr class="even">
<td>Update 5</td>
<td>Layer Normalization</td>
<td>~2.076</td>
<td><a href="https://github.com/garg-aayush/building-from-scratch/commit/63ef5f8"><code>63ef5f8</code></a></td>
</tr>
<tr class="odd">
<td>Update 6</td>
<td>Pre-LayerNorm (modern)</td>
<td>~2.076</td>
<td><a href="https://github.com/garg-aayush/building-from-scratch/commit/4f5bef8"><code>4f5bef8</code></a></td>
</tr>
<tr class="even">
<td>Update 7</td>
<td>Scaling Up + Dropout</td>
<td>~1.48</td>
<td><a href="https://github.com/garg-aayush/building-from-scratch/commit/d4141d7"><code>d4141d7</code></a></td>
</tr>
</tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/basic-gpt/images/loss_curves.png" class="img-fluid figure-img"></p>
<figcaption>Loss Curves</figcaption>
</figure>
</div>
<p>You can find all the code and notebooks in the repo: <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/basic-gpt">building-from-scratch/basic-gpt</a></p>
<section id="baseline-bigram-model-e0b5864" class="level2">
<h2 class="anchored" data-anchor-id="baseline-bigram-model-e0b5864">Baseline: Bigram Model (<a href="https://github.com/garg-aayush/building-from-scratch/commit/e0b5864"><code>e0b5864</code></a>)</h2>
<p>Karpathy starts with the simplest possible language model: a bigram model. It predicts the next character based only on the current character. No context at all. The tokens aren’t talking to each other.</p>
<p>This still works somewhat because some characters naturally follow others (the letter ‘q’ is almost always followed by ‘u’). But the output is complete gibberish because the model has no way to look at what came before.</p>
<p><strong>Result</strong>: ~2.49 validation loss.</p>
</section>
<section id="update-1-self-attention-7b0e03a" class="level2">
<h2 class="anchored" data-anchor-id="update-1-self-attention-7b0e03a">Update 1: Self-Attention (<a href="https://github.com/garg-aayush/building-from-scratch/commit/7b0e03a"><code>7b0e03a</code></a>)</h2>
<p>We want tokens to communicate with each other and predictions to consider context from previous tokens, not just the current one. A token at position 5 should be able to look at tokens 1-4 and gather information from them. But at the same time, it can’t look at tokens 6, 7, 8 because those are the future we’re trying to predict.</p>
<p>Self-attention solves this. Every token is represented by 3 vectors: - <strong>Query</strong>: “What am I looking for?” - <strong>Key</strong>: “What do I contain?”<br>
- <strong>Value</strong>: “If you find me interesting, here’s what I’ll tell you.”</p>
<p>The query dot-products with all the keys. High dot product means high affinity: “I find you interesting.” The values of interesting tokens get aggregated via weighted sum.</p>
<blockquote class="blockquote">
<p><strong>Note</strong>: Attention is really a communication mechanism. You can think of it as nodes in a directed graph where every node aggregates information from nodes that point to it. In our case, token 5 can receive information from tokens 1-4 (and itself), but not from tokens 6-8. The triangular mask creates this directed structure and is what makes this a “decoder” block.</p>
</blockquote>
<p>One subtle but important point: attention has no notion of space. The tokens don’t inherently know where they are in the sequence. That’s why we add <strong>positional embeddings</strong>. Each position gets its own learned embedding that’s added to the token embedding, giving the model spatial information.</p>
<p><strong>Result</strong>: ~2.4 validation loss. Tokens can now see context.</p>
</section>
<section id="update-2-multi-head-attention-9d2a7b5" class="level2">
<h2 class="anchored" data-anchor-id="update-2-multi-head-attention-9d2a7b5">Update 2: Multi-Head Attention (<a href="https://github.com/garg-aayush/building-from-scratch/commit/9d2a7b5"><code>9d2a7b5</code></a>)</h2>
<p>Tokens have a lot to talk about. One head might look for consonants, another for vowels, another for word boundaries, another for patterns at specific positions. Having multiple independent communication channels lets the model gather diverse types of data in parallel.</p>
<blockquote class="blockquote">
<p><strong>Note</strong>: This is similar to grouped convolutions. Instead of one large convolution, you do it in groups. With 4 heads of 8 dimensions each, we get the same total dimensionality (32) but with 4 separate communication channels. Each head can specialize in different patterns.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/basic-gpt/images/MHA.png" class="img-fluid figure-img"></p>
<figcaption>Multi-Head Attention</figcaption>
</figure>
</div>
<p><strong>Result</strong>: ~2.28 validation loss.</p>
</section>
<section id="update-3-feed-forward-network-c4c46ff" class="level2">
<h2 class="anchored" data-anchor-id="update-3-feed-forward-network-c4c46ff">Update 3: Feed-Forward Network (<a href="https://github.com/garg-aayush/building-from-scratch/commit/c4c46ff"><code>c4c46ff</code></a>)</h2>
<p>The FFN layer addresses a key problem. Until now, “the tokens looked at each other but didn’t have enough time to think about what they found.”</p>
<p>Self-attention is the <strong>communication</strong> phase. Tokens gather data from each other. But then they need to <strong>compute</strong> on that data individually. That’s what the feed-forward network does. It operates on a per-token level. All the tokens process their gathered information independently.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/garg-aayush/building-from-scratch/main/basic-gpt/images/FFN.png" class="img-fluid figure-img"></p>
<figcaption>Feed-Forward Network</figcaption>
</figure>
</div>
<p>So the Transformer block becomes: <strong>communicate</strong> (attention) → <strong>compute</strong> (feed-forward). This pattern repeats for every layer.</p>
<p><strong>Result</strong>: ~2.27 validation loss. The architecture now has both communication and computation.</p>
</section>
<section id="update-4-residual-connections-0239c07" class="level2">
<h2 class="anchored" data-anchor-id="update-4-residual-connections-0239c07">Update 4: Residual Connections (<a href="https://github.com/garg-aayush/building-from-scratch/commit/0239c07"><code>0239c07</code></a>)</h2>
<p>This is one of two optimizations that make deep networks actually trainable. Without it, stacking many layers leads to vanishing gradients and optimization difficulties.</p>
<p>Karpathy visualizes it nicely: imagine a residual pathway running from top to bottom. You can “fork off” from this pathway, do some computation, and project back via addition. The path from inputs to outputs is just a series of additions.</p>
<blockquote class="blockquote">
<p><strong>Note</strong>: Why does this help? During backpropagation, addition distributes gradients equally to both branches. The gradients “hop” through every addition node directly to the input. This creates a <strong>“gradient superhighway”</strong> from supervision to input, unimpeded. The residual blocks are initialized to contribute very little at first, then “come online” over time during optimization.</p>
</blockquote>
<p><strong>Result</strong>: ~2.09 validation loss. Now we can stack layers without vanishing gradients.</p>
</section>
<section id="update-5-6-layer-normalization-63ef5f8-4f5bef8" class="level2">
<h2 class="anchored" data-anchor-id="update-5-6-layer-normalization-63ef5f8-4f5bef8">Update 5 &amp; 6: Layer Normalization (<a href="https://github.com/garg-aayush/building-from-scratch/commit/63ef5f8"><code>63ef5f8</code></a>, <a href="https://github.com/garg-aayush/building-from-scratch/commit/4f5bef8"><code>4f5bef8</code></a>)</h2>
<p>Batch normalization normalizes columns (across examples in a batch). Layer normalization normalizes rows (across features for each example). The implementation is almost identical, you just change which dimension you normalize over.</p>
<p>Layer norm has advantages for Transformers: - No dependency on batch size (works even with batch size 1) - No running buffers to maintain - No distinction between training and test time</p>
<p>The original Transformer paper used <strong>post-layer norm</strong> (normalize after attention/FFN). Modern implementations use <strong>pre-layer norm</strong> (normalize before). Pre-layer norm creates a cleaner residual pathway since the transformation happens on normalized inputs, leading to more stable training.</p>
<p><strong>Result</strong>: ~2.076 validation loss.</p>
</section>
<section id="update-7-scaling-up-d4141d7" class="level2">
<h2 class="anchored" data-anchor-id="update-7-scaling-up-d4141d7">Update 7: Scaling Up (<a href="https://github.com/garg-aayush/building-from-scratch/commit/d4141d7"><code>d4141d7</code></a>)</h2>
<p>With all the architectural pieces in place, Karpathy scales up the architecture:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Parameter</th>
<th>Before</th>
<th>After</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Block size (context)</td>
<td>8</td>
<td>256</td>
</tr>
<tr class="even">
<td>Embedding dim</td>
<td>32</td>
<td>384</td>
</tr>
<tr class="odd">
<td>Heads</td>
<td>4</td>
<td>6</td>
</tr>
<tr class="even">
<td>Layers</td>
<td>3</td>
<td>6</td>
</tr>
<tr class="odd">
<td>Dropout</td>
<td>0</td>
<td>0.2</td>
</tr>
</tbody>
</table>
<p>Dropout is added for regularization. It randomly shuts off neurons during training, effectively training an ensemble of sub-networks. At test time, everything is enabled and the sub-networks merge.</p>
<p><strong>Result</strong>: ~1.48 validation loss. The generated text now looks like Shakespeare (structure, dialogue formatting, character names) even though it’s nonsensical when you actually read it.</p>
</section>
<section id="how-this-compares-to-gpt-3" class="level2">
<h2 class="anchored" data-anchor-id="how-this-compares-to-gpt-3">How This Compares to GPT-3</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>My Model</th>
<th>GPT-3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Parameters</td>
<td>~10M</td>
<td>175B</td>
</tr>
<tr class="even">
<td>Dataset</td>
<td>~300K tokens</td>
<td>300B tokens</td>
</tr>
<tr class="odd">
<td>Architecture</td>
<td>Nearly identical</td>
<td>Nearly identical</td>
</tr>
</tbody>
</table>
<p>The architecture we built is essentially the same as GPT-3. The difference is pure scale: 17,500x more parameters trained on 1 million times more data. By today’s standards, even GPT-3’s 300B tokens is considered modest. Current models train on 1T+ tokens.</p>
<p>This is what makes the Transformer architecture so remarkable. The same fundamental design (attention for communication, feed-forward for computation, residual connections, layer norm) scales from a 10M parameter Shakespeare generator to a 175B parameter model!</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><strong>Code</strong>: <a href="https://github.com/garg-aayush/building-from-scratch/tree/main/basic-gpt">building-from-scratch/basic-gpt</a></li>
<li><strong>Video</strong>: <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s Build GPT from Scratch</a> by Andrej Karpathy</li>
</ul>


</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2025-09-08-building-gpt-from-scratch.html</guid>
  <pubDate>Mon, 08 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Key Takeaways from Lecture 1: LLM Evaluation Lifecycle</title>
  <link>https://garg-aayush.github.io/posts/2025-09-02-llm-evaluation-lifecycle.html</link>
  <description><![CDATA[ 




<p>A couple of months back, I enrolled in <a href="https://maven.com/parlance-labs/evals">AI Evals for Engineers and PMs</a>, a course by <a href="https://hamel.dev/">Hamel</a> and <a href="https://www.sh-reya.com/">Shreya</a>. The live cohort for ot ran from July to mid-August, but due to work commitments I couldn’t follow along in real time.</p>
<p>I have now started following it as a self-paced course and plans to write a blog for each lesson as I progress. This will be my way to capture what I learn and to reflect on the material. In this first blog 🤞, I’ll walk through my key takeaways from introductory <strong>Lecture 1</strong>.</p>
<section id="key-takeaways" class="level1">
<h1>Key Takeaways</h1>
<section id="evaluation-isnt-optional-but-fundamental" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-isnt-optional-but-fundamental">1. Evaluation isn’t Optional but Fundamental</h2>
<p>Anyone who has built or worked with LLM pipelines knows that their outputs are open-ended, subjective, and unstructured (unless you enforce it). If you rely on ad-hoc checks which I have been guilty of, it often leads to knee-jerk fixes. Moreover, it completely miss the long-term need of continuous tracking which is essential for improving your pipeline reliability and usefulness. This is why <strong>Evaluation—the systematic measurement of an LLM pipeline quality—is critical!</strong></p>
</section>
<section id="the-three-gulfs" class="level2">
<h2 class="anchored" data-anchor-id="the-three-gulfs">2. The Three Gulfs</h2>
<p>The below image beautifully captures and categorizes the challenges associated with any LLM application: <img src="https://garg-aayush.github.io/static/img/blog-2025-09-02/three-gulfs.png" class="img-fluid" alt="Three gulfs"></p>
<ul>
<li><p><strong>Gulf of Comprehension</strong>: This is a result of limited understanding of the input data (user queries) and the pipeline’s outputs (behavior). Bridging it requires examining examples to identify common failure modes. This brings it own challenge: <strong>“How to manually review every input or output to identify failure modes?”</strong></p></li>
<li><p><strong>Gulf of Specification</strong>: It refers to the difficulty of translating a user’s high-level intent into unambiguous precise instructions for the LLM. Bridging it requires writing detailed prompts that captures “true intent” which in itself is challenging due to <strong>ambiguous nature of natural language.</strong></p></li>
<li><p><strong>Gulf of Generalizaton</strong>: This is due to LLMs unexpected and inconsistent behavior on new or unusual (out of distribution) inputs. Bridging it requires a good understanding of your LLM model capabilities. This leads to the question: <strong>“How to improve LLM model?”</strong></p></li>
</ul>
</section>
<section id="analyze-measure-improve-lifecycle" class="level2">
<h2 class="anchored" data-anchor-id="analyze-measure-improve-lifecycle">3. Analyze → Measure → Improve Lifecycle</h2>
<p>Hamel and Shreya introduced a structured way to bridge the above gulfs: <strong>Analyze → Measure → Improve</strong> lifecycle.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://garg-aayush.github.io/static/img/blog-2025-09-02/pitfalls.png" class="img-fluid figure-img"></p>
<figcaption>Analyze → Measure → Improve Lifecycle</figcaption>
</figure>
</div>
<p>However, the most important takeaways for me was not what each phase means but the <strong>pitfalls</strong> that often derail them:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 48%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Phase</th>
<th>Pitfalls</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Analyze</strong></td>
<td>Outsourcing annotation; looking at too few examples and forming shaky hypotheses</td>
<td>This is where you learn the most. Spend <strong>~75–80%</strong> of your time here—good analysis sets up everything else.</td>
</tr>
<tr class="even">
<td><strong>Measure</strong></td>
<td>Misaligned or poorly designed LLM judges; “overfitting” by testing judges on the same examples used in the judge prompt</td>
<td>In this phase, you need the rigor of data science. <strong>NEVER</strong> leak test data into judge prompts.</td>
</tr>
<tr class="odd">
<td><strong>Improve</strong></td>
<td>Prematurely jumping to fixes; defaulting to the most complex solution first (fine-tuning, bigger models)</td>
<td><strong>Start simple</strong>. Prompt tweaks and improvements often go a long way before heavier changes are needed.</td>
</tr>
</tbody>
</table>
</section>
<section id="llms-are-imperfectprompt-iteratively" class="level2">
<h2 class="anchored" data-anchor-id="llms-are-imperfectprompt-iteratively">4. LLMs are Imperfect—Prompt Iteratively</h2>
<p>When we write prompts it’s easy to ignore that LLMs are non-deterministic, prompt-sensitive and can confidently hallucinate. Thus, always remember: <strong><em>“LLMs are powerful but imperfect components. Leverage strengths, anticipate weaknesses.”</em></strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://garg-aayush.github.io/static/img/blog-2025-09-02/llm-strengths-weaknesses.jpg" class="img-fluid figure-img"></p>
<figcaption>LLM Strengths vs.&nbsp;Weaknesses</figcaption>
</figure>
</div>
<p><strong>Effective prompting starts with you.</strong> You should not delegate the prompting to an LLM or you will miss important failure modes. Instead, write your own draft prompt and if needed, use an LLM only to polish clarity.</p>
<p>From there on, treat prompting as an iterative process where the first draft is a starting point which you refine based on observed outputs.</p>
</section>
<section id="reference-based-vs-reference-free-metrics" class="level2">
<h2 class="anchored" data-anchor-id="reference-based-vs-reference-free-metrics">5. Reference-based vs Reference-free Metrics</h2>
<p>The evaluation metrics broadly fall into two categories: <strong>reference-free</strong> and <strong>reference-based</strong>. Both of them are useful but in different contexts.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 44%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Reference-Free</strong></th>
<th><strong>Reference-Based</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>What it means</strong></td>
<td>Evaluates properties of the output itself (no golden answer required)</td>
<td>Compares output against a golden reference or ground truth</td>
</tr>
<tr class="even">
<td><strong>When to use</strong></td>
<td>Creative or open-ended tasks, formatting/structure checks, validity tests</td>
<td>Tasks with clearly defined correct answers (e.g., factual QA, deterministic outputs)</td>
</tr>
<tr class="odd">
<td><strong>Examples</strong></td>
<td>- Does the output follow the JSON format?<br>- Does generated code/SQL run without errors?</td>
<td>- Exact match against a gold SQL query<br>- ROUGE/BLEU score for text generation</td>
</tr>
</tbody>
</table>


</section>
</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2025-09-02-llm-evaluation-lifecycle.html</guid>
  <pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Part III: Fine-tuning Llama-3-8B for Structured Functional Representation Extraction</title>
  <link>https://garg-aayush.github.io/posts/2024-07-15-finetune-llama3-8B-predibase.html</link>
  <description><![CDATA[ 




<p>Last week, I published the <a href="https://aayushgarg.dev/2024-07-09-compare-models-structured-data/">second blog</a> in my LLM fine-tuning series, comparing various models performance in functional representation extraction.</p>
<p>In this third part of the series, I discuss the <strong>first steps toward fine-tuning an (open-source)-LLM for functional representation extraction</strong>. My aim is to give you all a sneak peek at the kind of performance you can expect from fine-tuning an LLM for a custom task. To streamline this step (and to satisfy my own curiosity 😊), I will use <a href="https://predibase.com/">Predibase</a>. It is a fast, cheap, and efficient open-source LLM fine-tuning and deployment platform.</p>
<blockquote class="blockquote">
<p>FYI: I have some free Predibase credits through Dan’s and Hamel’s LLM course. Therefore, it is a perfect opportunity to put those credits to good use! 😬</p>
</blockquote>
<blockquote class="blockquote">
<p>Note: Whenever I mention “finetuning LLM,” I am specifically referring to LoRA (Low-Rank Adaptation) finetuning of a Large Language Model. For overview of LoRA, please read Sebastian Raschka’s blogs (<a href="https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html">LORA Blog 1</a>, <a href="https://magazine.sebastianraschka.com/p/llm-research-insights-instruction">LORA Blog 2</a>).</p>
</blockquote>
<section id="task-and-dataset" class="level2">
<h2 class="anchored" data-anchor-id="task-and-dataset">Task and Dataset</h2>
<p>Similar to my previous blogs, the custom task is to predict the structured functional representation from the given text video game opinions of the <a href="https://huggingface.co/datasets/GEM/viggo">ViGGO validation dataset</a>.</p>
<p><strong>To make this exercise interesting and challenging for future experiments, I will use a maximum of 1000 examples for fine-tuning any LLM model, instead of the full ~5K train dataset</strong>.</p>
<p>Below is an example from the randomly selected <code>1K</code> train dataset:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">Text                      : I remember you saying that you loved The Room. Do you tend to enjoy PC games <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2012</span>?</span>
<span id="cb1-2">functional_representation : verify_attribute(name[The Room], release_year[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2012</span>], rating[excellent], platforms[PC])</span></code></pre></div></div>
<p>As shown in the graph below, the selected <code>1K</code> dataset is a fairly representative sample of the full ViGGO train dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://garg-aayush.github.io/static/img/blog-2024-07-15/viggo_function_name_distribution_1K.png" class="img-fluid figure-img"></p>
<figcaption>Understanding Data Distribution</figcaption>
</figure>
</div>
</section>
<section id="upload-the-dataset-to-predibase" class="level2">
<h2 class="anchored" data-anchor-id="upload-the-dataset-to-predibase">Upload the dataset to Predibase</h2>
<p>Predibase requires you to upload the instruction fine-tuning dataset in particular format. This is from <a href="https://docs.predibase.com/user-guide/fine-tuning/prepare-data#how-to-structure-your-dataset">Predibase docs</a>:</p>
<blockquote class="blockquote">
<p>For instruction fine-tuning, your dataset must contain two columns named prompt and completion: - prompt: Your input prompt. It serves as the starting point or the guiding information for the model. - completion: The expected response that corresponds to the input provided in the “prompt” column. - split (optional): Should be either train or evaluation. To learn more, check out this section.</p>
</blockquote>
<p><strong>Make sure to add the prompt template to the examples and convert them to the correct format.</strong> For this exercise, I use the following prompt template:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">prompt_template <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""Given a target sentence convert it structured functional representation.</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">### Target sentence: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{text}</span></span>
<span id="cb2-4"></span>
<span id="cb2-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">### Output Functional representation:</span></span>
<span id="cb2-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div></div>
<p>You can connect your dataset to Predibase via the UI or Python SDK. Here, I will upload the dataset using SDK.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize Predibase client</span></span>
<span id="cb3-2">pb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Predibase(api_token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PREDIBASE_API_TOKEN"</span>])</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Upload the dataset</span></span>
<span id="cb3-5">dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pb.datasets.from_file(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"viggo_train_val_dataset_1K.csv"</span>, </span>
<span id="cb3-6">                                name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"viggo_train_val_dataset_1K"</span>)</span></code></pre></div></div>
<p>Once uploade, you can check the uploaded dataset on the Predibase UI. <img src="https://garg-aayush.github.io/static/img/blog-2024-07-15/predibase_ui_dataset.png" class="img-fluid" alt="Dataset"> <img src="https://garg-aayush.github.io/static/img/blog-2024-07-15/predibase_ui_dataset2.png" class="img-fluid" alt="Dataset"></p>
<p>For detailed steps on uploading the dataset to Predibase, please refer to the companion <a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Finetune_llama-3-8b_Predibase.ipynb">blog notebook</a>.</p>
</section>
<section id="setup-and-finetune" class="level2">
<h2 class="anchored" data-anchor-id="setup-and-finetune">Setup and Finetune</h2>
<p>Once you have uploaded the dataset, running the fine-tuning process is refreshingly simple. For this example, I fine-tune the base <code>llama-3-8b</code> model with the following parameters: <code>epochs=3</code>, <code>rank=16</code>, and <code>learning_rate=2e-4</code>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create an adapter repository</span></span>
<span id="cb4-2">repo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pb.repos.create(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"viggo-finetune-1K"</span>, </span>
<span id="cb4-3">                description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Llama-3-8b adapter repository for viggo 1K examples"</span></span>
<span id="cb4-4">                )</span>
<span id="cb4-5"></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create and run the fine-tuning job</span></span>
<span id="cb4-7">adapter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pb.adapters.create(</span>
<span id="cb4-8">   config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>FinetuningConfig(</span>
<span id="cb4-9">       base_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"llama-3-8b"</span>,</span>
<span id="cb4-10">       epochs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,</span>
<span id="cb4-11">       rank<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>,</span>
<span id="cb4-12">       learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0002</span>,</span>
<span id="cb4-13">   ),</span>
<span id="cb4-14">   dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dataset,</span>
<span id="cb4-15">   repo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>repo,</span>
<span id="cb4-16">   description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"baseline-llama-3-8b"</span>,</span>
<span id="cb4-17">)</span></code></pre></div></div>
<p>That’s all you need to do to submit a job! Once completed, it will be available on the Predibase platform.</p>
<p><img src="https://garg-aayush.github.io/static/img/blog-2024-07-15/predibase_ui_train1.png" class="img-fluid" alt="train-1"> <img src="https://garg-aayush.github.io/static/img/blog-2024-07-15/predibase_ui_train2.png" class="img-fluid" alt="train-2"></p>
<p>You can always tweak multiple hyperparameters (see <a href="https://docs.predibase.com/sdk-guide/SDKv2/ConfigClasses/FineTuningConfig">Finetuning Config</a>) and run the fine-tune job again. All your fine-tune jobs will be available on the Predibase platform.</p>
</section>
<section id="evaluate-the-fine-tuned-model" class="level2">
<h2 class="anchored" data-anchor-id="evaluate-the-fine-tuned-model">Evaluate the Fine-tuned Model</h2>
<p>Predibase provides both popular <code>Serverless endpoints</code> and <code>Dedicated deployments</code> options for opens-source LLMs and their fine-tuned LORA checkpoints. I will create serverless endpoint for this case.</p>
<blockquote class="blockquote">
<p>Note, atleast for now, <a href="https://docs.predibase.com/user-guide/inference/serverless_endpoints">serverless deployments</a> are available for free.</p>
</blockquote>
<section id="generate-the-responses-for-validation-dataset" class="level3">
<h3 class="anchored" data-anchor-id="generate-the-responses-for-validation-dataset">Generate the responses for validation dataset</h3>
<p>Similar to my previous blogs, I will evaluate the finetuned model on ViGGO <code>validation</code> dataset and calculate custom <a href="https://aayushgarg.dev/2024-07-09-compare-models-structured-data/">performance metrics</a> metrics for a better understanding of finetuned model performance.</p>
<p>First, I generate the responses for the validation dataset:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize the Predibase deployment client</span></span>
<span id="cb5-2">lorax_client <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pb.deployments.client(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"llama-3-8b"</span>)</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load the validation dataset</span></span>
<span id="cb5-5">viggo_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"GEM/viggo"</span>)</span>
<span id="cb5-6">val_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> viggo_dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation'</span>]</span>
<span id="cb5-7"></span>
<span id="cb5-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># finetuned adapter id</span></span>
<span id="cb5-9">adapter_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"viggo-finetune-1K/2"</span> </span>
<span id="cb5-10"></span>
<span id="cb5-11">responses_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb5-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(val_dataset)):</span>
<span id="cb5-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Processing </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>idx<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(val_dataset)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-14">    output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lorax_client.generate(prompt_template.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>val_dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"target"</span>][idx]), adapter_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"viggo-finetune-1K/2"</span>, max_new_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">150</span>).generated_text</span>
<span id="cb5-15">    ground_truth <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val_dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"meaning_representation"</span>][idx]</span>
<span id="cb5-16">    text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val_dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"target"</span>][idx]</span>
<span id="cb5-17">    responses_dict[idx] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"output"</span>: output, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ground_truth"</span>: ground_truth, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>: text}</span></code></pre></div></div>
<p><strong>Note: Remember to replace “viggo-finetune-1K/2” with the correct adapter ID. You can find the adapter ID in the Predibase dashboard.</strong></p>
<p>Now, I can generate the evaluation scores using custom evaluation metrics and compare them with previously calculated GPT-4 and Claude 3.5 Sonnet scores:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://garg-aayush.github.io/static/img/blog-2024-07-15/finetuned_llama-3-8B_baseline.png" class="img-fluid figure-img"></p>
<figcaption>plot-finetuned-model</figcaption>
</figure>
</div>
<p>The initial finetuning of LLaMA-3-8B using 1,000 random examples from the ViGGO dataset, while not surpassing GPT-4 and Claude 3.5 Sonnet, shows promising results and outperforms several models from our previous blog. Notably, the exact_match score is even better than that of the two best-performing models.</p>
</section>
</section>
<section id="improved-performance-with-updated-prompt-template" class="level2">
<h2 class="anchored" data-anchor-id="improved-performance-with-updated-prompt-template">Improved Performance with Updated Prompt Template</h2>
<p>A simple yet effective way to enhance the model’s performance is by refining the prompt template. By providing clearer instructions that convey the structure of the functional representation, we can guide the model to produce more accurate outputs.</p>
<p>I updated the prompt template as follows:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">prompt_template <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""Given a target sentence construct the underlying meaningful functional representation of the input sentence as a single function with attributes and attribute values.</span></span>
<span id="cb6-2"></span>
<span id="cb6-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">### Target sentence: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{text}</span></span>
<span id="cb6-4"></span>
<span id="cb6-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">### Output Functional representation:</span></span>
<span id="cb6-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div></div>
<p>After uploading this new dataset, finetuning the model, and evaluating it with the new adapter <code>viggo-finetune-1K/3</code>, there is significantly improved evaluation metrics. Notably, the model now surpasses GPT-4o’s scores for <code>exact_match</code> and <code>function_name_match</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://garg-aayush.github.io/static/img/blog-2024-07-15/finetuned_llama-3-8B_update1.png" class="img-fluid figure-img"></p>
<figcaption>plot-finetuned-model</figcaption>
</figure>
</div>
<p><strong>This improvement highlights the importance of clear and specific instructions in prompt engineering, even when working with finetuned models.</strong></p>
</section>
<section id="conclusions.." class="level2">
<h2 class="anchored" data-anchor-id="conclusions..">Conclusions..</h2>
<ul>
<li><p>First of all, <strong>My overall experience with Predibase has been positive, particularly in terms of rapid finetuning of models</strong>. While there are some limitations such as restricted hyperparameter tuning, standardized dataset format, and inability to download adapters in the developer tier, it offers a user-friendly platform for fine-tuning (LORA) large language models. I was able to quickly upload, setut, finetune and infer the llm models.</p></li>
<li><p>I achieve out-of-the-box performance using only random <code>1K</code> examples. Although the fine-tuned llama-3-8b model doesn’t match the performance of GPT-4 and Sonnet 3.5 on all metrics. <strong>This demonstrates the potential of fine-tuning with limited data, highlighting the efficiency of the approach for task-specific model adaptation.</strong></p></li>
</ul>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next steps…</h2>
<ul>
<li>My next goal is to further enhance the model’s performance on evaluation metrics while maintaining a limit of 1,000 training examples. <em><strong><a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment</a></strong> paper has demonstrated in the past that even 1,000 well-curated examples can lead to strong finetuning performance.</em></li>
<li>Careful curated selection of examples and hyerparameters will definitely improve the performance benchmarks on evaluation metrics.</li>
<li>In addition to it, I will deep dive into one of my favorite LLM fine-tuning tools, <a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a>.</li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://aayushgarg.dev/2024-07-03-baseline-gpt4o-structured-data/">Part II blog post of series</a></li>
<li><a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Analyze_Viggo_Dataset.ipynb">Notebook I: Analyze_Viggo_Dataset.ipynb</a></li>
<li><a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Finetune_llama-3-8b_Predibase.ipynb">Notebook II: Finetune_llama-3-8b_Predibase.ipynb</a></li>
<li><a href="https://predibase.com/">Predibase Platform</a></li>
<li><a href="https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html">LORA blog I: Finetuning Large Language Models (LLMs)</a></li>
<li><a href="https://magazine.sebastianraschka.com/p/llm-research-insights-instruction">LORA blog II: LLM Research Insights: Instruction Tuning &amp; Training Paradigms</a></li>
<li><a href="https://llama.meta.com/llama3/">Llama 3</a></li>
<li><a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment</a></li>
<li><a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl: A Framework for Fine-tuning LLMs</a></li>
</ul>
<hr>
<p>Thanks for reading! If you have any questions or feedback, please let me know on <a href="https://twitter.com/Aayush_ander">Twitter</a> or <a href="https://www.linkedin.com/in/aayush-garg-8b26a734/">LinkedIn</a>.</p>


</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2024-07-15-finetune-llama3-8B-predibase.html</guid>
  <pubDate>Mon, 15 Jul 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Part II: Comparison of Model Performances on Structured Functional Representation Extraction</title>
  <link>https://garg-aayush.github.io/posts/2024-07-09-compare-models-structured-data.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>In the <a href="https://aayushgarg.dev/2024-07-03-baseline-gpt4o-structured-data/">previous blog post</a>, I established a performance baseline using GPT-4o for generating structured data, particularly functional representations, from text using the <a href="https://huggingface.co/datasets/GEM/viggo">ViGGO Dataset</a>.</p>
<p>Building on that foundation, I expand the experiment to include a broader range of models, both open-source and proprietary. This comparison aims to provide insights on how well these models perform out of the box in structured data extraction tasks, which is quite crucial for RAG applications, knowledge base construction, and reasoning systems.</p>
<p>I evaluate and compare the performance of these six LLM models:</p>
<ol type="1">
<li><p><a href="https://openai.com/index/hello-gpt-4o/">GPT-4o</a>: OpenAI’s latest iteration of the GPT-4 model, known for its faster generation, advanced natural language understanding and generation capabilities. Currently one of the most popular and capable models.</p></li>
<li><p><a href="https://www.anthropic.com/news/claude-3-5-sonnet">Claude Sonnet-3.5</a>: Anthropic’s refined language model with enhanced reasoning abilities. It aims to provide more context-aware outputs compared to earlier versions and has recently outperformed GPT-4o on many benchmarks.</p></li>
<li><p><a href="https://deepmind.google/technologies/gemini/flash/">Gemini-1.5-Flash</a>: Google DeepMind’s streamlined version of the Gemini model, optimized for faster inference and reduced computational requirements.</p></li>
<li><p><a href="https://replicate.com/meta/meta-llama-3-70b-instruct">llama-3-70b-instruct</a>: Meta’s large-scale instruction-tuned language model, part of the latest LLaMA 3 family, with 70 billion parameters. It’s designed to follow complex instructions and generate high-quality text across diverse domains.</p></li>
<li><p><a href="https://replicate.com/mistralai/mixtral-8x7b-instruct-v0.1">mixtral-8x7b-instruct-v0.1</a>: Mistral AI’s instruction-tuned variant of the Mixtral 8x7B model, known for its mixture-of-experts architecture.</p></li>
<li><p><a href="https://replicate.com/meta/meta-llama-3-8b-instruct">llama-3-8b-instruct</a>: A more compact version of Meta’s LLaMA 3 family, with 8 billion parameters, optimized for instruction following.</p></li>
</ol>
<blockquote class="blockquote">
<p><strong>Note</strong>: I’ve included the smaller <code>Llama-3-8B</code> model as I plan to finetune it’s base 8B model in coming days. It would help me compare the general instruction finetuned 8B model performance.</p>
</blockquote>
</section>
<section id="dataset-and-prompt-template" class="level3">
<h3 class="anchored" data-anchor-id="dataset-and-prompt-template">Dataset and Prompt Template</h3>
<p>For consistency and fair comparison, I used the same ViGGO validation dataset and the prompt template as in the previous blog post. The prompt template and the few-shot examples are designed to guide the models in generating structured functional representations from the given text input:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">PROMPT_TEMPLATE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb1-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. </span></span>
<span id="cb1-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].</span></span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']. The order your list the attributes within the function must follow the order listed above. For example the 'name' attribute must always come before the 'exp_release_date' attribute, and so forth.</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">For each attribute, fill in the corresponding value of the attribute within brackets. A couple of examples are below. Note: you are to output the string after "Output: ". Do not include "Output: " in your answer.</span></span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 1)</span></span>
<span id="cb1-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.</span></span>
<span id="cb1-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])</span></span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 2) </span></span>
<span id="cb1-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Were there even any terrible games in 2014?</span></span>
<span id="cb1-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: request(release_year[2014], specifier[terrible])</span></span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 3)</span></span>
<span id="cb1-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Adventure games that combine platforming and puzzles  can be frustrating to play, but the side view perspective is perfect for them. That's why I enjoyed playing Little Nightmares.</span></span>
<span id="cb1-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])</span></span>
<span id="cb1-20"></span>
<span id="cb1-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 4)</span></span>
<span id="cb1-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Since we're on the subject of games developed by Telltale Games, I'm wondering, have you played The Wolf Among Us?</span></span>
<span id="cb1-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: recommend(name[The Wolf Among Us], developer[Telltale Games])</span></span>
<span id="cb1-24"></span>
<span id="cb1-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 5) </span></span>
<span id="cb1-26"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Layers of Fear, the indie first person point-and-click adventure game?</span></span>
<span id="cb1-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: confirm(name[Layers of Fear], genres[adventure, indie, point-and-click], player_perspective[first person])  </span></span>
<span id="cb1-28"></span>
<span id="cb1-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 6) </span></span>
<span id="cb1-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: I bet you like it when you can play games on Steam, like Worms: Reloaded, right?  </span></span>
<span id="cb1-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: suggest(name[Worms: Reloaded], available_on_steam[yes])</span></span>
<span id="cb1-32"></span>
<span id="cb1-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 7)</span></span>
<span id="cb1-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: I recall you saying that you really enjoyed The Legend of Zelda: Ocarina of Time. Are you typically a big fan of games on Nintendo rated E (for Everyone)?    </span></span>
<span id="cb1-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: verify_attribute(name[The Legend of Zelda: Ocarina of Time], esrb[E (for Everyone)], rating[excellent], platforms[Nintendo])</span></span>
<span id="cb1-36"></span>
<span id="cb1-37"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 8)</span></span>
<span id="cb1-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: So what is it about the games that were released in 2005 that you find so excellent?  </span></span>
<span id="cb1-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: request_explanation(release_year[2005], rating[excellent])</span></span>
<span id="cb1-40"></span>
<span id="cb1-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 9)</span></span>
<span id="cb1-42"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Do you think Mac is a better gaming platform than others?</span></span>
<span id="cb1-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: request_attribute(has_mac_release[])</span></span>
<span id="cb1-44"></span>
<span id="cb1-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Give the output for the following sentence:</span></span>
<span id="cb1-46"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{input}</span></span>
<span id="cb1-47"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div></div>
</section>
<section id="generating-the-responses" class="level3">
<h3 class="anchored" data-anchor-id="generating-the-responses">Generating the responses</h3>
<p>I used the respective official API calls for the closed models (<code>GPT-4o</code>, <code>Gemini-1.5-flash</code>, <code>Claude-1.5-Sonnet</code>) and the <a href="https://replicate.com/">Replicate</a> API client for open-source models (<code>Llama-3-70B</code>, <code>Llama-3-8B</code>, <code>Mistral-8x7B</code>).</p>
<p><strong>For detailed information on API endpoints and the process of generating responses for all models, please refer to the <a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Generate_responses_all_llms.ipynb">Generate_responses_all_llms.ipynb</a> notebook.</strong></p>
<p>Generating responses for this experiment had the following associated costs:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model/API</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-4o API</td>
<td>~ $2.5</td>
</tr>
<tr class="even">
<td>Claude-1.5-Sonnet API</td>
<td>~ $3.5</td>
</tr>
<tr class="odd">
<td>Replicate API</td>
<td>~ $3</td>
</tr>
<tr class="even">
<td>Gemini-1.5-flash API</td>
<td>Free*</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>~$9</strong></td>
</tr>
</tbody>
</table>
<p>_*for limited usage_</p>
</section>
<section id="evaluation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-strategy">Evaluation Strategy</h3>
<p>To assess the models’ performance, I used the same evaluation criteria as in the <a href="https://aayushgarg.dev/2024-07-03-baseline-gpt4o-structured-data/">previous post</a>:</p>
<ol type="1">
<li><strong>Function Name Match</strong>: The function name must match the ground truth function name.</li>
<li><strong>Function and Attributes Match</strong>: The generated function name and attributes must match the ground truth function attributes. However, the order of the attributes does not matter.</li>
<li><strong>Function, Attributes, and Values Match</strong>: The generated function name, attributes, and values must match the ground truth function attributes and values. The order of the attributes and values does not matter.</li>
<li><strong>Exact Match</strong>: The generated function must exactly match the ground truth function.</li>
</ol>
<p><strong>Note</strong>: I implemented custom Python functions using regex and string manipulation to calculate these metrics, rather than relying on another LLM for evaluation. This approach helps avoid potential biases that might be introduced by using an LLM in the evaluation process.</p>
<p><strong>For the complete evaluation code and functions used, please refer to the <a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Compare_model_performances.ipynb">Compare_models_performances.ipynb</a> notebook.</strong></p>
</section>
<section id="comparing-the-models-performance" class="level3">
<h3 class="anchored" data-anchor-id="comparing-the-models-performance">Comparing the models performance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://garg-aayush.github.io/static/img/blog-2024-07-09/all_metrics_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Compare Models Performance</figcaption>
</figure>
</div>
<p><strong>Based on the evaluation metric plot</strong>:</p>
<ol type="1">
<li><code>Claude Sonnet-3.5</code> and <code>GPT-4o</code> consistently outperform other models across all metrics.</li>
<li><code>Claude Sonnet-3.5</code> performs better than <code>GPT-4o</code>, aligning with all the twitter chats about its superior performance.</li>
<li><code>Gemini-1.5-Flash</code>, despite its optimization for speed, maintains competitive performance for less stringent metrics but struggles significantly with exact matches.</li>
<li>The performance gap between the top-performing models and others widens sharply for more stringent metrics.</li>
<li>As expected, the smaller <code>Llama-3-8B</code> model shows the lowest performance, highlighting the evident size advantage of larger models.</li>
<li><code>Mistral-8x7B's</code> performance is lower than anticipated, suggesting lower instruction-following capabilities.</li>
</ol>
<p><strong>Some key observations</strong></p>
<blockquote class="blockquote">
<p>Based on the quickly eyeballing the generated responses:</p>
</blockquote>
<ol type="1">
<li>Almost all models consistently captures straightforward attributes such as player perspective and multiplayer status.</li>
<li>For queries involving multiple attributes or conditions, the model sometimes misses or misinterprets parts of the input.</li>
<li>The models struggles with capturing subtle distinctions in opinions and inferring information that is implied but not explicitly stated in the text. For example, models struggled to differentiate between <code>inform</code>, <code>give_opinion</code> and <code>suggest</code>.</li>
</ol>
</section>
<section id="conclusions" class="level3">
<h3 class="anchored" data-anchor-id="conclusions">Conclusions</h3>
<p>This comparison provides valuable insights into the capabilities of various LLMs in functional representation extraction. As expected, the proprietary large models like <code>Claude Sonnet-3.5</code> and <code>GPT-4o</code> perform best out of the box, with <code>Claude Sonnet-3.5</code> being the best.</p>
<p>One can argue that these results may not fully represent the models’ maximum capabilities. Different model-specific prompt engineering approaches or dynamic few-shot examples could potentially improve performance further. However, my aim is just to assess how well a model perform without fancy RAG/function calling/complicated prompt engineering approaches.</p>
</section>
<section id="next-steps." class="level3">
<h3 class="anchored" data-anchor-id="next-steps.">Next steps….</h3>
<ul>
<li><p>I plan to fine-tune the base <code>Llama-3-8B</code> and other smaller models (like <code>Phi</code> and <code>Gemma</code>) on the ViGGO dataset to assess whether a fine-tuned smaller model can compete with or surpass the performance of <code>Claude Sonnet-3.5</code> and <code>GPT-4o</code>.</p></li>
<li><p>At the same time, investigate the trade offs like inference speed, accuracy, and latency associated with fine-tuned smaller models and propreitory models api calls.</p></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://aayushgarg.dev/2024-07-03-baseline-gpt4o-structured-data/">Part I blog post of series</a></li>
<li><a href="https://huggingface.co/datasets/GEM/viggo">ViGGO Dataset</a></li>
<li><a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Generate_responses_all_llms.ipynb">Notebook I: Generate_responses_all_llms.ipynb</a></li>
<li><a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Compare_model_performances.ipynb">Notebook II: Compare_models_performances.ipynb</a></li>
</ul>
<hr>
<p>Thanks for reading! If you have any questions or feedback, please let me know on <a href="https://twitter.com/Aayush_ander">Twitter</a> or <a href="https://www.linkedin.com/in/aayush-garg-8b26a734/">LinkedIn</a>.</p>


</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2024-07-09-compare-models-structured-data.html</guid>
  <pubDate>Tue, 09 Jul 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Part I: Baseline Evaluation of GPT-4o for Functional Representation Extraction</title>
  <link>https://garg-aayush.github.io/posts/2024-07-03-baseline-gpt4o-structured-data.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Extracting structured data from unstructured texts allow us to condense the information present in the text. This representation then can be used for efficient indexing and other downstream RAG applications.&nbsp;</p>
<p>I want to evaluate <a href="https://openai.com/index/hello-gpt-4o/">GPT-4o’s</a> performance in extracting structural data, specifically, functional representation, from the unstructured domain-specific text. I will use <a href="https://huggingface.co/datasets/GEM/viggo">ViGGO dataset</a> to evaluate it on custom evaluation criteria and will set it as baseline performance for that can be used for comparison in future work with other models such as <strong>Claude</strong>, <strong>Gemini</strong>, and <strong>custom</strong> fine-tuned open-source models.</p>
</section>
<section id="viggo-dataset" class="level3">
<h3 class="anchored" data-anchor-id="viggo-dataset">ViGGO Dataset</h3>
<p>This is a dataset for generating text opinions in the video game domain. Strictly speaking, it is intended to generate coherent conversational responses based on input functional representations (set of attributes and values).</p>
<p>However, I use the <strong>reverse task</strong>, where I <strong>generate structured functional representations from the given text input</strong>. A typical ViGGO dataset example has the output structured functional representation consisting of a single function with attributes and attribute values.</p>
<pre><code>Text:
You said that you liked Crysis. Do you often play first person games from Crytek Frankfurt?

Functional Representation:
verify_attribute(name[Crysis], developer[Crytek Frankfurt], rating[good], player_perspective[first person])</code></pre>
<p>The function and attributes must be one of the following, respectively:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">Function:</span>
<span id="cb2-2">[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'inform'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'request'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'give_opinion'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'confirm'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'verify_attribute'</span>, </span>
<span id="cb2-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'suggest'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'request_explanation'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'recommend'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'request_attribute'</span>]</span>
<span id="cb2-4"></span>
<span id="cb2-5">Attributes:</span>
<span id="cb2-6">[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'name'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'release_year'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'esrb'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'genres'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'platforms'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'available_on_steam'</span>,</span>
<span id="cb2-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'has_linux_release'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'has_mac_release'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'specifier'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rating'</span>, </span>
<span id="cb2-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'player_perspective'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'has_multiplayer'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'developer'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'exp_release_date'</span>]</span></code></pre></div></div>
<p><strong>Note</strong>: Since I am not training/fine-tuning any model, I will only consider the ViGGO validation datasetfor this exercise.</p>
</section>
<section id="prompt-for-generating-the-functional-representation" class="level3">
<h3 class="anchored" data-anchor-id="prompt-for-generating-the-functional-representation">Prompt for generating the functional representation</h3>
<p>I use modified version of prompt template used in <a href="https://medium.com/r?url=https%3A%2F%2Fwww.anyscale.com%2Fblog%2Ffine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications">Anyscale’s blog</a> for the ViGGO dataset. The prompt template is a few-shot prompt with examples from each function category to assist the model in understanding the intended output response representation.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">PROMPT_TEMPLATE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. </span></span>
<span id="cb3-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].</span></span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']. The order your list the attributes within the function must follow the order listed above. For example the 'name' attribute must always come before the 'exp_release_date' attribute, and so forth.</span></span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">For each attribute, fill in the corresponding value of the attribute within brackets. A couple of examples are below. Note: you are to output the string after "Output: ". Do not include "Output: " in your answer.</span></span>
<span id="cb3-8"></span>
<span id="cb3-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 1)</span></span>
<span id="cb3-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.</span></span>
<span id="cb3-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])</span></span>
<span id="cb3-12"></span>
<span id="cb3-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 2) </span></span>
<span id="cb3-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Were there even any terrible games in 2014?</span></span>
<span id="cb3-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: request(release_year[2014], specifier[terrible])</span></span>
<span id="cb3-16"></span>
<span id="cb3-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 3)</span></span>
<span id="cb3-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Adventure games that combine platforming and puzzles  can be frustrating to play, but the side view perspective is perfect for them. That's why I enjoyed playing Little Nightmares.</span></span>
<span id="cb3-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])</span></span>
<span id="cb3-20"></span>
<span id="cb3-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 4)</span></span>
<span id="cb3-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Since we're on the subject of games developed by Telltale Games, I'm wondering, have you played The Wolf Among Us?</span></span>
<span id="cb3-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: recommend(name[The Wolf Among Us], developer[Telltale Games])</span></span>
<span id="cb3-24"></span>
<span id="cb3-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 5) </span></span>
<span id="cb3-26"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Layers of Fear, the indie first person point-and-click adventure game?</span></span>
<span id="cb3-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: confirm(name[Layers of Fear], genres[adventure, indie, point-and-click], player_perspective[first person])  </span></span>
<span id="cb3-28"></span>
<span id="cb3-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 6) </span></span>
<span id="cb3-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: I bet you like it when you can play games on Steam, like Worms: Reloaded, right?  </span></span>
<span id="cb3-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: suggest(name[Worms: Reloaded], available_on_steam[yes])</span></span>
<span id="cb3-32"></span>
<span id="cb3-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 7)</span></span>
<span id="cb3-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: I recall you saying that you really enjoyed The Legend of Zelda: Ocarina of Time. Are you typically a big fan of games on Nintendo rated E (for Everyone)?    </span></span>
<span id="cb3-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: verify_attribute(name[The Legend of Zelda: Ocarina of Time], esrb[E (for Everyone)], rating[excellent], platforms[Nintendo])</span></span>
<span id="cb3-36"></span>
<span id="cb3-37"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 8)</span></span>
<span id="cb3-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: So what is it about the games that were released in 2005 that you find so excellent?  </span></span>
<span id="cb3-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: request_explanation(release_year[2005], rating[excellent])</span></span>
<span id="cb3-40"></span>
<span id="cb3-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Example 9)</span></span>
<span id="cb3-42"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sentence: Do you think Mac is a better gaming platform than others?</span></span>
<span id="cb3-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Output: request_attribute(has_mac_release[])</span></span>
<span id="cb3-44"></span>
<span id="cb3-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Give the output for the following sentence:</span></span>
<span id="cb3-46"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{input}</span></span>
<span id="cb3-47"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div></div>
<p>The typical responses for the above template are not perfect but it can be used for generating structured output for the full dataset.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">Ground Truth: inform(name[FIFA <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>], release_year[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2011</span>], esrb[E (<span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> Everyone)], rating[average], genres[simulation, sport])</span>
<span id="cb4-2">GPT Response: inform(name[FIFA <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>], release_year[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2011</span>], esrb[E (<span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> Everyone)], rating[average], genres[sports, simulation])</span>
<span id="cb4-3"></span>
<span id="cb4-4"></span>
<span id="cb4-5">Ground Truth: request(player_perspective[side view], specifier[easy])</span>
<span id="cb4-6">GPT Response: Output: request(genres[side view], rating[top], specifier[easy])</span>
<span id="cb4-7"></span>
<span id="cb4-8"></span>
<span id="cb4-9">Ground Truth: recommend(name[Need <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> Speed: The Run], platforms[Xbox])</span>
<span id="cb4-10">GPT Response: confirm(name[Need <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> Speed: The Run], platforms[Xbox])</span></code></pre></div></div>
</section>
<section id="evaluation-criteria" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-criteria">Evaluation criteria</h3>
<p>Often, you require custom evaluation criteria for custom tasks. For structured functional representation extraction, I define the following binary criteria:</p>
<ol type="1">
<li><p><strong>Function Name Match</strong>: The function name must match the ground truth function name.</p></li>
<li><p><strong>Function and Attributes Match</strong>: The generated function name and attributes must match the ground truth function attributes. However, the order of the attributes does not matter.</p></li>
<li><p><strong>Function, Attributes, and Values Match</strong>: The generated function name, attributes, and values must match the ground truth function attributes and values. The order of the attributes and values does not matter.</p></li>
<li><p><strong>Exact Match</strong>: The generated function must exactly match the ground truth function.</p></li>
</ol>
<p>The above criteria are in order of increasing strictness. The first criterion is the least strict, and the last criterion is the most strict. These criteria will help me evaluate the model’s performance.</p>
</section>
<section id="evaluation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-strategy">Evaluation Strategy</h3>
<p>Although not ideal, I ask the model to evaluate its own performance on the given task. This approach has limitations, as it could potentially introduce bias in the evaluation process. However, it serves as a starting point for our analysis. I ask the model to compare the generated function with the ground truth function and provide a boolean score based on the above evaluation criteria.</p>
<p>Since, I need the evaluation scores in a more structured format to analyze the model’s performance effectively. I use <a href="https://docs.pydantic.dev/latest/">pydantic</a> and <a href="https://python.useinstructor.com/">instructor</a> packages to obtain the evaluation scores in a structured format. I use the following prompt and pydantic model to evaluate the GPT-4o performance:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> EvaluateFunctionRepresentation(BaseModel):</span>
<span id="cb5-2">    function_match: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The function name is the same but the attributes and values can be different."</span>)</span>
<span id="cb5-3">    function_attribute_match: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The function and the attributes are the same but the values can be different."</span>)</span>
<span id="cb5-4">    function_attributes_values_match: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The generated representation has same function and attributes and corresponding values without the same attributes and values order."</span>)</span>
<span id="cb5-5">    exact_match: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The generated representation is exactly the same as the ground truth representation."</span>)</span>
<span id="cb5-6"></span>
<span id="cb5-7">PROMPT_TEMPLATE_SCORE_EVAL <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""I will provide you with two functional representations strings. One will be the ground truth representation (ground_truth) and the other will be a generated representation (generated). You need to compare the generated representation with the ground truth representation and provide the following similarity match in true or false:</span></span>
<span id="cb5-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">1) function_match</span></span>
<span id="cb5-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">2) function_attributes_match</span></span>
<span id="cb5-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">3) function_attributes_values_match</span></span>
<span id="cb5-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">4) exact_match</span></span>
<span id="cb5-12"></span>
<span id="cb5-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">A typical functional representation is of this form: function(attribute1[values], attribute2[values], attribute3[values], ...). </span></span>
<span id="cb5-14"></span>
<span id="cb5-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Given the following two functional representation, provide the similarity scores for the following:</span></span>
<span id="cb5-16"></span>
<span id="cb5-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">ground_truth: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{ground_truth}</span></span>
<span id="cb5-18"></span>
<span id="cb5-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">generated: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{generated}</span></span>
<span id="cb5-20"></span>
<span id="cb5-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Let's think step by step.</span></span>
<span id="cb5-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div></div>
<p><strong>Please go through the <a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Evaluate_GPT4_Viggo_dataset.ipynb">GPT-4o baseline evaluation notebook</a> for more details on prompt template and evaluation process and responses.</strong></p>
</section>
<section id="evaluating-the-performance" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-performance">Evaluating the performance</h3>
<p>Using the above evaluation strategy, I generate the average evaluation scores for the full dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://garg-aayush.github.io/static/img/blog-2024-07-03/gpt-4o-evaluation-metrics.png" class="img-fluid figure-img"></p>
<figcaption>GPT-4o Evaluation Metrics</figcaption>
</figure>
</div>
<p>The task of generating functional representations from natural language sentences is challenging as seen in the above scores. The best the model can do is to provide the exact match only for <strong>30%</strong> of the examples. Even the correct function name evaluation score is only about <strong>80%</strong>. These results indicate that while GPT-4o shows some capability in extracting functional representations, there’s significant room for improvement.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>The above evaluation exercise provides a good baseline and useful evaluation criteria/metrics that can be used to assess the performance of other models on the same task in the future. Generating functional representations is a complex task and is not easily accomplished using prompt-engineering techniques like few-shot learning as seen in the results.</p>
</section>
<section id="next-steps" class="level3">
<h3 class="anchored" data-anchor-id="next-steps">Next steps…</h3>
<ul>
<li><p>Evaluate other large models like <strong>Claude</strong>, <strong>Gemini</strong>, and <strong>Llama-3–70B</strong> on the same task to compare their performance. This will provide a broader perspective on how different LLMs handle structured data extraction.</p></li>
<li><p>Explore alternative evaluation methods that don’t rely on the model evaluating itself. Most importantly, to eliminate potential biases in the evaluation process.</p></li>
<li><p>Fine-tune smaller models like <strong>Llama-3–8B</strong> or <strong>Mistral-7B</strong> for this domain-specific structured representation task. Fine-tuning will not only improve the model’s performance but also enhance latency and reduce the number of input tokens required to generate the output.</p></li>
<li><p>Investigate the impact of different prompt engineering techniques on the model’s performance.</p></li>
</ul>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ul>
<li><a href="https://github.com/garg-aayush/llm-warehouse/blob/main/tutorials/Evaluate_GPT4_Viggo_dataset.ipynb">GPT-4o baseline evaluation notebook</a></li>
<li><a href="https://openai.com/index/hello-gpt-4o/">OpenAI GPT-4o</a></li>
<li><a href="https://huggingface.co/datasets/GEM/viggo">ViGGO Dataset</a></li>
<li><a href="https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications">Anyscale Blog on Fine-tuning Llama 2</a></li>
<li><a href="https://docs.pydantic.dev/latest/">Pydantic Documentation</a></li>
<li><a href="https://python.useinstructor.com/">Instructor Package</a></li>
</ul>
<hr>
<p>Thanks for reading! If you have any questions or feedback, please let me know on <a href="https://twitter.com/Aayush_ander">Twitter</a> or <a href="https://www.linkedin.com/in/aayush-garg-8b26a734/">LinkedIn</a>.</p>


</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2024-07-03-baseline-gpt4o-structured-data.html</guid>
  <pubDate>Wed, 03 Jul 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Step-by-Step Guide to Setup Your Personal GPU Server</title>
  <link>https://garg-aayush.github.io/posts/2024-06-30-remote-gpu-server.html</link>
  <description><![CDATA[ 




<ul>
<li><a href="https://gist.github.com/garg-aayush/d377557b20ef9f206c1d6381e174d8a7">Github Gist Link</a></li>
</ul>
<p>I’ve been using a GPU workstation with an RTX 4090 for almost a year now, and it’s been one of the best decisions I’ve made. With a personal GPU server, you no longer need to rely on cloud-based GPU instances from services like <code>RunPod</code> or <code>Vast.ai</code> every time you want to run a job or try new models. The best part? No stress about recurring GPU instance costs! :-)</p>
<p>However, I rarely work directly on my workstation. Instead, I prefer the flexibility of accessing the GPU remotely using my MacBook, whether I’m working from different locations within my home, from a co-working space, or a cozy cafe in another part of town.</p>
<p><strong>In this blog, I will walk you through the steps to configure a personal GPU Ubuntu server.</strong></p>
<p>For this guide, I assume you already have a workstation running Ubuntu with a GPU and it is connected to your local network</p>
<section id="setting-up-local-remote-access" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-local-remote-access">Setting Up Local Remote Access</h2>
<p>Let’s start by setting up local access, which will allow you to <code>ssh</code> into your GPU server when you’re on the same home Wi-Fi network. This is ideal for a work-from-home (WFH) setup where your workstation is running in a corner of your living space.</p>
<ol type="1">
<li><p><strong>Install the SSH server</strong></p>
<p>First, we need to install an SSH (Secure Shell) server. This will allow you to securely access your GPU machine remotely. Open a terminal on your Ubuntu machine and run the following commands: <code>bash  sudo apt update &amp;&amp;  sudo apt install openssh-server</code> This command updates your package lists and installs the OpenSSH server.</p></li>
<li><p><strong>Start and Enable SSH Service</strong></p>
<p>Next, enable the SSH service using this command: <code>bash  sudo systemctl enable --now ssh</code> You can verify if the service is enabled by running: <code>bash  sudo systemctl status ssh</code></p>
<p>Look for a line starting with <code>Active: active (running)</code> for <code>ssh.service</code>. This indicates that the SSH service is up and running.</p>
<blockquote class="blockquote">
<p>Note: The OpenSSH server starts running on boot by default.</p>
</blockquote></li>
<li><p><strong>Configure the firewall</strong></p>
<p>To allow SSH connections through the system firewall, you need to open the appropriate port. Ubuntu’s default firewall, UFW (Uncomplicated Firewall), makes this process straightforward: <code>bash  sudo ufw allow ssh</code> This command adds an exception to your firewall rules, permitting incoming SSH connections. You can check the SSH status with: <code>bash  sudo ufw status</code> You should see the output similar to: <code>bash  To                         Action      From  --                         ------      ----  22/tcp                     ALLOW       Anywhere  22/tcp(v6)                 ALLOW       Anywhere (v6)</code></p></li>
<li><p><strong>Connect to the local server</strong></p>
<p>Now that your GPU server is set up, it’s time to test the connection. From your laptop (which should be on the same local network as your GPU machine), open a terminal and use the following command: <code>bash  ssh user@local-ip-address</code> Replace user with your Ubuntu <code>user</code> and <code>local-ip-address</code> with the IP address of your GPU machine on the local network.</p>
<ul>
<li>To find your username on the workstation, you can use the <code>whoami</code> command.</li>
<li>To find your local IP address, use one of these methods on your workstation:
<ul>
<li>Run <code>hostname -I</code> and use the first address listed.</li>
<li>Use <code>ip addr show | grep -w</code> inet for more detailed network information.</li>
<li><a href="https://linuxconfig.org/how-to-find-my-ip-address-on-ubuntu-20-04-focal-fossa-linux">How to find my IP address on Ubuntu Linux</a> is a great blog on it. It explains multiple commands like <code>ip addr show | grep -w inet</code> or <code>networkctl status</code> to get the local IP address.</li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p>Your local IP address typically starts with 192.168.</p>
<p>Note: If your router dynamically changes the local IP address of your workstation, it’s best to log into your router and assign a fixed local IP address to ensure consistent access.</p>
</blockquote>
<p>If everything is configured correctly, you’ll be prompted to enter your password, after which you’ll gain remote access to your GPU server.</p></li>
<li><p><strong>Set Up SSH Keys for Passwordless Login</strong></p>
<p>It is recommended to set up key-based authentication for better security and convenience purposes. This allows you to connect to your remote server without entering a password each time.</p>
<ul>
<li>It is quite common to setup ssh key-based authentication.</li>
<li>For detailed instructions on setting up SSH keys, refer to the DigitalOcean guide on <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-ubuntu-20-04#step-4-disabling-password-authentication-on-your-server">Setting up SSH keys on Ubuntu 20.04</a>.</li>
</ul></li>
</ol>
</section>
<section id="setting-up-external-remote-access" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-external-remote-access">Setting Up External Remote Access</h2>
<p>While local access is great for working within your home network, sometimes you need to access your GPU workstation from outside your local network, such as from co-working spaces or a cozy cafe.</p>
<p>One simple and secure way to achieve this is by using <a href="https://ngrok.com/">ngrok</a>.</p>
<blockquote class="blockquote">
<p>ngrok helps creates secure tunnels from public endpoints to locally running services. It allows you to expose your personal server to the internet, enabling remote access from anywhere without complex network configurations.</p>
</blockquote>
<p>Here’s how to set it up:</p>
<ol type="1">
<li><p><strong>Install ngrok</strong></p>
<p>First, you need to install ngrok on your GPU workstation. Open a terminal and run this command: <code>bash  snap install ngrok</code></p>
<ul>
<li>For more installation options, see https://dashboard.ngrok.com/get-started/setup/linux.</li>
</ul></li>
<li><p><strong>Create and connect to ngrok Account</strong></p>
<p>Visit <a href="https://ngrok.com/">ngrok’s website</a> and sign up for a free account if you haven’t already. After signing up, you’ll receive an auth token. On your GPU workstation, run: <code>bash  ngrok config add-authtoken YOUR_AUTH_TOKEN</code> You can get the config file path and edit using <code>ngrok config check</code> and <code>vim &lt;path&gt;</code>, respectively.</p></li>
<li><p><strong>Start the ngrok Tunnel</strong></p>
<p>Now, you can create a secure tunnel to your SSH service: <code>bash  ngrok tcp 22</code> This command will display a URL that looks like <code>tcp://X.tcp.ngrok.io:PORT</code>. Note down this URL.</p></li>
<li><p><strong>Connect to Your Workstation</strong></p>
<p>From any external laptop, you can now SSH into your GPU workstation using: <code>bash  ssh -p YYYY user@X.tcp.ngrok.io</code> Replace <code>PORT</code> with the port number and <code>X</code> with the subdomain from the ngrok URL. Replace <code>user</code> with your Ubuntu username.</p>
<p><em>The above steps ensure that you can remotely access the workstation from external network. However, no one is going to manually start the ngrok every time before heading out.</em></p></li>
<li><p><strong>Make ngrok start automatically on boot</strong></p>
<p>To ensure ngrok starts automatically when your workstation boots:</p>
<ul>
<li>Create a new service file:</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> vim /etc/systemd/system/ngrok.service</span></code></pre></div></div>
<ul>
<li>Add the following content:</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[Unit]</span></span>
<span id="cb2-2"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">Description</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>start <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">ngrok</span> tunnel on startup</span>
<span id="cb2-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">After</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>network.target</span>
<span id="cb2-4"></span>
<span id="cb2-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[Service]</span></span>
<span id="cb2-6"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">ExecStart</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>/snap/bin/ngrok <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">tcp</span> 22</span>
<span id="cb2-7"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">Restart</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>on-failure</span>
<span id="cb2-8"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">User</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=&lt;</span>your_username<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span></span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[Install]</span></span>
<span id="cb2-11"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">WantedBy</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>multi-user.target</span></code></pre></div></div>
<p>Replace <code>&lt;your_username&gt;</code> with your Ubuntu username. Save the file and exit the editor.</p>
<ul>
<li>Enable and start the service:</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> systemctl enable ngrok.service</span>
<span id="cb3-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> systemctl start ngrok.service</span></code></pre></div></div>
<p>Now ngrok will automatically start and create a tunnel when your workstation boots.</p>
<blockquote class="blockquote">
<p>Note: With a free account, ngrok assigns a new port (YYYY) each time your workstation boots. You can get the new port from the <a href="https://dashboard.ngrok.com/tunnels/agents">ngrok dashboard</a>.</p>
</blockquote></li>
<li><p><strong>Paid ngrok account for dedicated port</strong></p>
<p>For a dedicated TCP endpoint port that doesn’t change on reboot, you need a paid ngrok personal account (<code>$10/month</code>).</p>
<ol type="a">
<li>Reserve a tcp endpoint</li>
</ol>
<p>Once you have a paid account, reserve a TCP endpoint at <code>https://dashboard.ngrok.com/cloud-edge/tcp-addresses</code>.</p>
<ol start="2" type="a">
<li>Update the ngrok service file</li>
</ol>
<p>Add the following content:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[Unit]</span></span>
<span id="cb4-2"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">Description</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>start <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">ngrok</span> tunnel on startup</span>
<span id="cb4-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">After</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>network.target</span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[Service]</span></span>
<span id="cb4-6"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">ExecStart</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>/snap/bin/ngrok <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">tcp</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--region</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=&lt;</span>region<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> --remote-addr=<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>remote-address<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> 22</span>
<span id="cb4-7"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">Restart</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>on-failure</span>
<span id="cb4-8"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">User</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=&lt;</span>your_username<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span></span>
<span id="cb4-9"></span>
<span id="cb4-10"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[Install]</span></span>
<span id="cb4-11"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">WantedBy</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>multi-user.target</span></code></pre></div></div>
<p>Replace <code>&lt;region&gt;</code>, <code>&lt;remote-address&gt;</code>, and <code>&lt;your_username&gt;</code> with the appropriate values from your reserved TCP endpoint config.</p></li>
</ol>
<p><strong>With this setup, your SSH remote endpoint will remain the same even if the system reboots.</strong></p>
<section id="reference-links" class="level3">
<h3 class="anchored" data-anchor-id="reference-links">Reference Links:</h3>
<ol type="1">
<li><a href="https://ubuntu.com/server/docs/service-openssh">Ubuntu SSH Documentation</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-ubuntu-20-04">DigitalOcean Guide on Setting Up SSH Keys</a></li>
<li><a href="https://ngrok.com/docs">ngrok Documentation</a></li>
<li><a href="https://linuxconfig.org/how-to-find-my-ip-address-on-ubuntu-20-04-focal-fossa-linux">IP Address Information for Ubuntu</a></li>
<li><a href="https://help.ubuntu.com/community/UFW">UFW (Uncomplicated Firewall) Guide</a></li>
</ol>
<hr>
<p>Thanks for reading! If you have any questions or feedback, please let me know on <a href="https://twitter.com/Aayush_ander">Twitter</a> or <a href="https://www.linkedin.com/in/aayush-garg-8b26a734/">LinkedIn</a>.</p>


</section>
</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2024-06-30-remote-gpu-server.html</guid>
  <pubDate>Sun, 30 Jun 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Managing multiple CUDA versions using environment modules in Ubuntu</title>
  <link>https://garg-aayush.github.io/posts/2024-05-19-manage-multiple-cuda.html</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p>Latest Update: May 19th, 2024</p>
</blockquote>
<ul>
<li><a href="https://gist.github.com/garg-aayush/156ec6ddda3d62e2c0ddad00b7e66956">Github Gist Link</a></li>
</ul>
<p>This blog contains all the steps required to: - Install multiple CUDA versions (e.g., <code>CUDA 11.8 and</code>CUDA 12.1 - Manage multiple CUDA environments on Ubuntu using the utility called <a href="https://modules.readthedocs.io/en/latest/">environment modules</a>. - Use this approach to avoid CUDA environment conflicts.</p>
<blockquote class="blockquote">
<p>Environment Modules is a package that provides for the dynamic modification of a user’s environment via modulefiles. You can find more on it at <a href="https://modules.readthedocs.io/en/latest/">https://modules.readthedocs.io/en/latest/</a></p>
</blockquote>
<section id="install-the-compatible-nvidia-drivers-if-required" class="level3">
<h3 class="anchored" data-anchor-id="install-the-compatible-nvidia-drivers-if-required">Install the Compatible NVIDIA Drivers (if required)</h3>
<ul>
<li><p>Add PPA GPU Drivers Repository to the System</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> add-apt-repository ppa:graphics-drivers/ppa</span></code></pre></div></div></li>
<li><p>Check GPU and available drives</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">ubuntu-drivers</span> devices</span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># install it using: sudo ubuntu-drivers</span></span></code></pre></div></div></li>
<li><p>Install the compatible driver</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># best to allow Ubuntu to autodetect and install the compatible nvidia-driver</span></span>
<span id="cb3-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> ubuntu-drivers install</span></code></pre></div></div>
<blockquote class="blockquote">
<p>For example, I tried to install <code>nvidia-driver-545</code> using <code>sudo ubuntu-drivers install nvidia:545</code> command. However, I was unable to install it. There was always some or the other issue.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Note</strong>: Please <strong>restart</strong> your system after installing the nvidia driver. Ideally, you should be able to get GPU state and stats using <code>nvidia-smi</code></p>
</blockquote></li>
<li><p>Check the installed NVIDIA driver</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">nvidia-detector</span> </span></code></pre></div></div></li>
<li><p>Additionally, you can also install NVIDIA drivers using the <strong>Software &amp; Updates</strong> Ubuntu app. Just go to the <strong>Additional Drivers</strong> tab, choose a driver, and click <strong>Apply Changes</strong>.</p></li>
</ul>
</section>
<section id="install-cuda-11.8-and-cuda-12.1" class="level3">
<h3 class="anchored" data-anchor-id="install-cuda-11.8-and-cuda-12.1">Install <code>CUDA 11.8</code> and <code>CUDA 12.1</code></h3>
<ul>
<li><p>Go to the <a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a> and select <code>CUDA Toolkit 11.8</code> from the available options.</p></li>
<li><p>Choose your OS, architecture, distribution, version, and installer type. For example, in my case:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Option</th>
<th style="text-align: center;">value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">OS</td>
<td style="text-align: center;">Linux</td>
</tr>
<tr class="even">
<td style="text-align: center;">Architecture</td>
<td style="text-align: center;">x86_64</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Distribution</td>
<td style="text-align: center;">Linux</td>
</tr>
<tr class="even">
<td style="text-align: center;">Version</td>
<td style="text-align: center;">22.04</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Installer type</td>
<td style="text-align: center;">deb(local)</td>
</tr>
</tbody>
</table></li>
<li><p>Follow the provided installation instructions by copying and pasting the commands into your terminal. This will install <code>CUDA 11.8</code>. Use the following commands:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</span>
<span id="cb5-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600  </span>
<span id="cb5-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">wget</span> https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb</span>
<span id="cb5-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> dpkg <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-i</span> cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb</span>
<span id="cb5-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> cp /var/cuda-repo-ubuntu2204-11-8-local/cuda-<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span>-keyring.gpg /usr/share/keyrings/</span>
<span id="cb5-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get update</span>
<span id="cb5-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-y</span> install cuda</span></code></pre></div></div></li>
<li><p>Similarly, install <code>CUDA 12.1</code> using the following commands:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</span>
<span id="cb6-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600</span>
<span id="cb6-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">wget</span> https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb</span>
<span id="cb6-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> dpkg <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-i</span> cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb</span>
<span id="cb6-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span>-keyring.gpg /usr/share/keyrings/</span>
<span id="cb6-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get update</span>
<span id="cb6-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-y</span> install cuda</span></code></pre></div></div></li>
<li><p>Make sure to copy and execute the commands above in your terminal to install <code>CUDA 11.8</code> and <code>CUDA 12.1</code> on your system.</p></li>
</ul>
</section>
<section id="install-cudnn-library" class="level3">
<h3 class="anchored" data-anchor-id="install-cudnn-library">Install <code>cuDNN</code> library</h3>
<ul>
<li><p>Go to <a href="https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/">https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/</a> and download the <code>cuDNN</code> tar for <code>CUDA 11.x</code>. Note that you might need to create a developer’s account first.</p></li>
<li><p>Untar the downloaded file using the following command:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tar</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-xvf</span> cudnn-linux-x86_64-9.1.0.70_cuda11-archive.tar.xz <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CUDA 11.x</span></span>
<span id="cb7-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tar</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-xvf</span> cudnn-linux-x86_64-9.1.0.70_cuda12-archive.tar.xz <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CUDA 12.x</span></span></code></pre></div></div></li>
<li><p>Copy the <code>cuDNN</code> files to the <code>CUDA</code> toolkit files:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># for CUDA 11.8</span></span>
<span id="cb8-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> cudnn-linux-x86_64-9.1.0.70_cuda11-archive/</span>
<span id="cb8-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> cp include/cudnn<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span>.h /usr/local/cuda-11.8/include</span>
<span id="cb8-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> cp lib64/libcudnn<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span> /usr/local/cuda-11.8/lib64</span>
<span id="cb8-5"></span>
<span id="cb8-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># for CUDA 12.1</span></span>
<span id="cb8-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> cudnn-linux-x86_64-9.1.0.70_cuda12-archive/</span>
<span id="cb8-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> cp include/cudnn<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span>.h /usr/local/cuda-12.1/include</span>
<span id="cb8-9"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> cp lib64/libcudnn<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span> /usr/local/cuda-12.1/lib64</span></code></pre></div></div></li>
<li><p>Make the files executable:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> chmod a+r /usr/local/cuda-11.8/include/cudnn<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span>.h /usr/local/cuda-11.8/lib64/libcudnn<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb9-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> chmod a+r /usr/local/cuda-12.1/include/cudnn<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span>.h /usr/local/cuda-12.1/lib64/libcudnn<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span></span></code></pre></div></div></li>
<li><p><strong>Note</strong>: Strictly speaking, you are done with the CUDA setup. You can use it by adding the CUDA bin and library path to the <code>PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables. For example, you can set up CUDA 11.8 by adding the following lines in the <code>~/.bashrc</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">PATH</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>/usr/local/cuda-11.8/bin:<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$PATH</span></span>
<span id="cb10-2"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">LD_LIBRARY_PATH</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>/usr/local/cuda-11.8/extras/CUPTI/lib64:<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$LD_LIBRARY_PATH</span></span>
<span id="cb10-3"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">LD_LIBRARY_PATH</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>/usr/local/cuda-11.8/lib64:<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$LD_LIBRARY_PATH</span></span></code></pre></div></div></li>
</ul>
<p>Similarly, you can set up CUDA 12.1. However, manually changing the paths every time can be cumbersome!</p>
<p><strong>Note</strong>: In case, you only want to install either of the one, CUDNN 11.x or CUDNN 12.x. The simpler way is to go to <a href="https://developer.nvidia.com/cudnn-downloads">https://developer.nvidia.com/cudnn-downloads</a> and install the CUDNN 11.x or CUDNN 12.x similar to CUDA installation.</p>
</section>
<section id="manage-multiple-cuda-versions-using-environment-modules" class="level3">
<h3 class="anchored" data-anchor-id="manage-multiple-cuda-versions-using-environment-modules">Manage multiple CUDA versions using <code>environment modules</code></h3>
<ol type="1">
<li><p><strong>Install the environment modules utility</strong></p>
<p>Run the following commands:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get update</span>
<span id="cb11-2">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get install environment-modules</span></code></pre></div></div>
<p>Check the installation:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Check the installation by running</span></span>
<span id="cb12-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module</span> list</span></code></pre></div></div>
<blockquote class="blockquote">
<p>You should see a list of default installed modules like git and maybe their versions displayed when you run the command <code>module list</code>. This confirms that the environment modules utility has been successfully installed on your system.</p>
</blockquote></li>
<li><p><strong>Create modulefiles for CUDA distributions</strong></p>
<blockquote class="blockquote">
<p><strong>Note</strong>: You might need root permissions to create directories and files. Use sudo in that case.</p>
</blockquote>
<p>Create a directory <code>/usr/share/modules/modulefiles/cuda</code> to hold modulefiles for cuda distributions</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> mkdir <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-p</span> /usr/share/modules/modulefiles/cuda</span></code></pre></div></div>
<p>Create a modulefile <code>/usr/share/modules/modulefiles/cuda/11.8</code> for <code>CUDA 11.8</code> and add the following lines:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#%Module1.0</span></span>
<span id="cb14-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">##</span></span>
<span id="cb14-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## cuda 11.8 modulefile</span></span>
<span id="cb14-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">##</span></span>
<span id="cb14-5"></span>
<span id="cb14-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">proc</span> ModulesHelp { } {</span>
<span id="cb14-7">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">global</span> version</span>
<span id="cb14-8"></span>
<span id="cb14-9">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">puts</span> stderr <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"\tSets up environment for CUDA </span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$version</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">\n"</span></span>
<span id="cb14-10"><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb14-11"></span>
<span id="cb14-12"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module-whatis</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sets up environment for CUDA 11.8"</span></span>
<span id="cb14-13"></span>
<span id="cb14-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">{</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">[</span> is-loaded cuda/12.1 <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">]</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">}</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">{</span></span>
<span id="cb14-15"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module</span> unload cuda/12.1</span>
<span id="cb14-16"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">}</span></span>
<span id="cb14-17"></span>
<span id="cb14-18"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span> version 11.8</span>
<span id="cb14-19"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span> root /usr/local/cuda-11.8</span>
<span id="cb14-20"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">setenv</span> CUDA_HOME    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$root</span></span>
<span id="cb14-21"></span>
<span id="cb14-22"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">prepend-path</span> PATH <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$root</span>/bin</span>
<span id="cb14-23"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">prepend-path</span> LD_LIBRARY_PATH <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$root</span>/extras/CUPTI/lib64</span>
<span id="cb14-24"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">prepend-path</span> LD_LIBRARY_PATH <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$root</span>/lib64</span>
<span id="cb14-25"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conflict</span> cuda</span></code></pre></div></div>
<p>Similarly, create a modulefile <code>/usr/share/modules/modulefiles/cuda/12.1</code> for <code>CUDA 12.1</code> and add the following lines:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#%Module1.0</span></span>
<span id="cb15-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">##</span></span>
<span id="cb15-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## cuda 12.1 modulefile</span></span>
<span id="cb15-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">##</span></span>
<span id="cb15-5"></span>
<span id="cb15-6"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">proc</span> ModulesHelp { } {</span>
<span id="cb15-7">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">global</span> version</span>
<span id="cb15-8"></span>
<span id="cb15-9">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">puts</span> stderr <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"\tSets up environment for CUDA </span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$version</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">\n"</span></span>
<span id="cb15-10"><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb15-11"></span>
<span id="cb15-12"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module-whatis</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sets up environment for CUDA 12.1"</span></span>
<span id="cb15-13"></span>
<span id="cb15-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">{</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">[</span> is-loaded cuda/11.8 <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">]</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">}</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">{</span></span>
<span id="cb15-15"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module</span> unload cuda/11.8</span>
<span id="cb15-16"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">}</span></span>
<span id="cb15-17"></span>
<span id="cb15-18"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span> version 12.1</span>
<span id="cb15-19"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span> root /usr/local/cuda-12.1</span>
<span id="cb15-20"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">setenv</span> CUDA_HOME    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$root</span></span>
<span id="cb15-21"></span>
<span id="cb15-22"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">prepend-path</span> PATH <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$root</span>/bin</span>
<span id="cb15-23"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">prepend-path</span> LD_LIBRARY_PATH <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$root</span>/extras/CUPTI/lib64</span>
<span id="cb15-24"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">prepend-path</span> LD_LIBRARY_PATH <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$root</span>/lib64</span>
<span id="cb15-25"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conflict</span> cuda</span></code></pre></div></div></li>
<li><p><strong>Make <code>CUDA 11.8</code> the default cuda version</strong></p>
<p>Create a file <code>/usr/share/modules/modulefiles/cuda.version</code> to make <code>CUDA 11.8</code> the default cuda module:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#%Module</span></span>
<span id="cb16-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span> ModulesVersion 11.8</span></code></pre></div></div>
<blockquote class="blockquote">
<p><strong>Note</strong>: make sure to reload your terminal.</p>
</blockquote></li>
<li><p><strong>Changing and Viewing the CUDA Module</strong></p>
<p>To change and view the loaded CUDA module, you can use the following commands:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Check the currently loaded module</span></span>
<span id="cb17-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module</span> list</span>
<span id="cb17-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Check the available modules</span></span>
<span id="cb17-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module</span> avail</span>
<span id="cb17-5"></span>
<span id="cb17-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load a specific cuda version</span></span>
<span id="cb17-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module</span> load cuda/12.1</span>
<span id="cb17-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Unload the currently loaded CUDA module</span></span>
<span id="cb17-9"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module</span> unload cuda</span>
<span id="cb17-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load CUDA 11.8</span></span>
<span id="cb17-11"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">module</span> load cuda/11.8</span>
<span id="cb17-12"></span>
<span id="cb17-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># verify the paths of the loaded CUDA</span></span>
<span id="cb17-14"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">nvcc</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--version</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># should give the loaded CUDA version</span></span>
<span id="cb17-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$CUDA_HOME</span></span>
<span id="cb17-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$PATH</span></span>
<span id="cb17-17"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">$LD_LIBRARY_PATH</span></span></code></pre></div></div>
<blockquote class="blockquote">
<p><strong>Note</strong>: You can add additional <code>CUDA</code> versions or other packages by creating corresponding modulefiles and following the steps outlined in this gist.</p>
</blockquote></li>
</ol>
</section>
<section id="some-useful-tips" class="level3">
<h3 class="anchored" data-anchor-id="some-useful-tips">Some Useful Tips</h3>
<ol type="1">
<li><p><strong>What to do if <code>nvidia-smi</code> does not works</strong></p>
<p>Sometime, after Ubuntu update or some other weird issue. The system might not be able to detect drivers. For example, you get erros such as <code>nvidia-smi has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</code> The best solution is to remove the current drivers and reinstall the compatible nvidia-driver.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># removes all the nvidia drivers</span></span>
<span id="cb18-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--purge</span> remove <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*nvidia*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"libxnvctrl*"</span></span>
<span id="cb18-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># reinstall the compatible driver and restart</span></span>
<span id="cb18-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> ubuntu-drivers install</span></code></pre></div></div></li>
<li><p><strong>How to purge CUDA from your computer</strong></p>
<p><strong>&gt; DO IT AT YOUR OWN RISK</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># removes all the nvidia drivers</span></span>
<span id="cb19-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--purge</span> remove <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*nvidia*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"libxnvctrl*"</span></span>
<span id="cb19-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># remove all cuda versions</span></span>
<span id="cb19-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--purge</span> remove <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*cuda*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*cublas*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*cufft*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*cufile*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*curand*"</span>  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*cusolver*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*cusparse*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*gds-tools*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*npp*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*nvjpeg*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nsight*"</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"*nvvm*"</span></span>
<span id="cb19-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># remove all cuda folders</span></span>
<span id="cb19-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> rm <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-rf</span> /usr/loca/cuda<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span></span></code></pre></div></div></li>
</ol>
</section>
<section id="resources-and-helpful-links" class="level2">
<h2 class="anchored" data-anchor-id="resources-and-helpful-links">Resources and helpful links</h2>
<ul>
<li><a href="https://ubuntu.com/server/docs/nvidia-drivers-installation">https://ubuntu.com/server/docs/nvidia-drivers-installation</a></li>
<li><a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a></li>
<li><a href="https://developer.nvidia.com/cudnn-downloads">https://developer.nvidia.com/cudnn-downloads</a></li>
</ul>
<hr>
<p>Thanks for reading! If you have any questions or feedback, please let me know on <a href="https://twitter.com/Aayush_ander">Twitter</a> or <a href="https://www.linkedin.com/in/aayush-garg-8b26a734/">LinkedIn</a>.</p>


</section>

 ]]></description>
  <guid>https://garg-aayush.github.io/posts/2024-05-19-manage-multiple-cuda.html</guid>
  <pubDate>Sun, 19 May 2024 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
